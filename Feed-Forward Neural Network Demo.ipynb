{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Exercise - Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey there, hi there, ho there, dear reader! \n",
    "Today, Andrew Ng's machine learning class (https://www.coursera.org/learn/machine-learning) has tasked us with implementing a neural network that accurately classifies hand-written digits, basically pictures of 0-9 written by hand which were then labeled with the text representation of that number. This is the same data set used in the logistic regression exercise, but this time we will create a basic feed-forward neural network with backpropagation in python 3.0. It's important to remember that for neural networks we need to randomly initialize our weights (parameters (numbers we multiple by to guess what the correct answer is)), where as with models like linear and logistic regression, it's okay to use zero-intialization. Additionally, we will completely vectorize our code. What does that mean? It means using linear algebra instead of explicit for-loops when doing computations. Why? Because linear algebra libraries are optimized, so the code runs faster and the code is easier to read. After coding up a neural network from scratch, we'll implement the same neural network in Keras, a highly abstracted neural network python library.\n",
    "\n",
    "Enough talk! Let's load up the data!\n",
    "\n",
    "This notebook was inspired by http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1/. Check out his webpage if you're interesting in machine learning or data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__header__': b'MATLAB 5.0 MAT-file, Platform: GLNXA64, Created on: Sun Oct 16 13:09:09 2011',\n",
       " '__version__': '1.0',\n",
       " '__globals__': [],\n",
       " 'X': array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]),\n",
       " 'y': array([[10],\n",
       "        [10],\n",
       "        [10],\n",
       "        ...,\n",
       "        [ 9],\n",
       "        [ 9],\n",
       "        [ 9]], dtype=uint8)}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this code assumes the data file is in the same directory as the code file\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "data = loadmat('ex4data1.mat')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, X is a matrix with the grayscale values of pixels that equate to the picture in question. y is a vector where each element corresponds to a row from our X matrix.\n",
    "Let's change X and y into something a bit more comfortable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (5000, 401) (+ bias term)\n",
      "y: (5000, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data['y']\n",
    "# Add intercept or bias term\n",
    "X = np.c_[np.ones((data['X'].shape[0],1)), data['X']]\n",
    "\n",
    "print('X:',X.shape, '(+ bias term)')\n",
    "print('y:',y.shape)\n",
    "type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So X is a 5000 by 401 matrix, meaning 5000 data examples or rows (the value 'm') and 401 columns; in machine learning columns are often called features. The 400 comes from the fact that pictures from our data are 20x20 pixels and the number in each cell is the grayscale value of the pixel in question. 1 comes from adding the bias or intercept term, which allows us to fit the data more accurately because we don't force our model to pass through the origin. y is a 5000 by 1 column vector will 5000 elements that correspond to whatever picture X's grayscale pixel values are supposed to represent.\n",
    "\n",
    "... What? A picture is worth a thousand words so let's take a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2674fd17320>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1152 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ran_samp = np.random.choice(X.shape[0], 20)\n",
    "plt.figure(figsize = (16, 16))\n",
    "plt.imshow(X[ran_samp, 1:].reshape(-1, 20).T, cmap = cm.Greys_r) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So one row of X is the grayscale values corresponding to one of the numbers above. Meanwhile, one element of y is the text representation or the label for one of the above pictures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the nature of neural networks, we need to convert the elements of y from scalar or real numbers into vectors. This is called one-hot encoding. Basically we will create a vector with nine 0s and one 1 for each unique label we have in our data. Our data only take on the values 0-9, so that's 10 one-hot encoded vectors. In other words, one-hot encoding converts a class label n (out of k classes) into a vector of length k, where the index n is \"hot\" (1) while the rest are zero. We will then fill a \"y matrix\" with these vectors. Luckily for us, Scikit-learn, a machine learning library, is already set up to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        [0., 0., 0., ..., 0., 0., 1.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.],\n",
       "        [0., 0., 0., ..., 0., 1., 0.]]), (5000, 10))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_onehot = encoder.fit_transform(y)\n",
    "y_onehot, y_onehot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also use pandas for this! I prefer scikit-learn, though, so this notebook will use the above variables. But, for your reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 0, ..., 0, 0, 1],\n",
       "        [0, 0, 0, ..., 0, 0, 1],\n",
       "        [0, 0, 0, ..., 0, 0, 1],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 1, 0],\n",
       "        [0, 0, 0, ..., 0, 1, 0],\n",
       "        [0, 0, 0, ..., 0, 1, 0]], dtype=uint8), (5000, 10))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "y_matrix = pd.get_dummies(y.ravel()).as_matrix() \n",
    "y_matrix, y_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data is all set up, it's time for the main event. The neural network Andrew Ng has asked us to build has an input layer corresponding to the features of our data (400 + the bias or intercept term), a hidden layer with 25 units (26 if you include the bias unit), and an output layer with 10 units for each of our class labels. If you're feeling frisky and want more info, please check out the PDF in this repo, which also contains a picture of our network architecture.\n",
    "\n",
    "Let's first code up the sigmoid or logistic function, which converts real numbers into probabilities. This function, g(z), is defined as:\n",
    "\n",
    "#### Sigmoid or logistic function\n",
    "#### $$ g(z) = \\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad, right? Now let's draw a plot and see if it will give us that beautiful 'S' line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Sigmoid S')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "digits = np.arange(-15, 15, step=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot(digits, sigmoid(digits), 'b')\n",
    "plt.xlabel('z',fontsize=16)\n",
    "plt.ylabel('g(z)',fontsize=16)\n",
    "plt.title('Sigmoid S',fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the sigmoid function asymptotes at 0 and 1? That means it's smashed all our values into probabilities. This type of function is called the activation function and is repeated over and over again in neural networks. If you're familiar with logistic regression, neural networks are often called \"stacked logistic regression.\"\n",
    "\n",
    "Because the sigmoid function asymptotes at 0 and 1, the derivative at those values is close to 0, which slows learning down. In modern machine learning or deep learning, people typically use tanh, ReLu (rectified linear unit), or leaky ReLu as their activation function, because those don't suffer from the near-zero derivative problem. But who am I to argue with Andrew Ng? At any rate, I think you'll find ol' sigmoid does just fine for this task.\n",
    "\n",
    "Now let's do random intialization. We need to set up a matrix to store and update our parameter values for each layer of our network, excluding the input layer. In this case, that's two layers: one hidden layer and one output layer. We will then fill the matrix with small random values. We use random-intialization to let each unit in our network learn its own unique and interesting function. If we did zero-intialization, each node would be learning the same function and be redundant.\n",
    "\n",
    "First, we need to set up the parameters of our neural network. These are also given in the exercise PDF, but for your reference, dear reader:\n",
    "#### Neural Network\n",
    "Input layer size = 400 (20x20 pixels) <br>\n",
    "Hidden layer size = 25 <br>\n",
    "Number of labels = 10 <br>\n",
    "Regularization parameter = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial setup\n",
    "input_size = 400\n",
    "hidden_size = 25\n",
    "num_labels = 10\n",
    "reg = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do random initialization, we need to set up the value $\\epsilon$ to multiply our theta matrices full of random values by."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\epsilon = \\frac{\\sqrt{6}} {\\sqrt{L_{in} + L_{out}}}$$\n",
    "where $$ L_{in} = s_l $$  and  $$ L_{out} = s_{l+1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_epi1 = np.sqrt(6)/np.sqrt((input_size + 1) + (hidden_size))\n",
    "init_epi2 = np.sqrt(6)/np.sqrt((hidden_size + 1) + (num_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's randomly generate the values of our two matrices, multiply them by our epsilons *2, subtract the resulting matrices by the inital epsilons, and unroll them into a looooooooooooooong parameter vector, theta_vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25, 401), (10, 26), (10285, 1))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta1 = np.random.rand((hidden_size), (input_size + 1)) * (2 * init_epi1) - init_epi1\n",
    "theta2 = np.random.rand((num_labels), (hidden_size + 1)) * (2 * init_epi2) - init_epi2\n",
    "theta_vec = np.concatenate((np.ravel(theta1), np.ravel(theta2)))\n",
    "theta_vec = theta_vec.reshape((len(theta_vec), 1))\n",
    "theta1.shape, theta2.shape, theta_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use theta_vec to avoid crashing the code later when using scipy's optimization functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to write some code for forward propagation.\n",
    "\n",
    "#### Forward Propagation\n",
    "$$ a^{(1)} = X $$\n",
    "$$ z^{(2)} = a^{(1)}\\Theta^{(1)T} $$\n",
    "$$ a^{(2)} = g(z^{(2)}) \\quad (add \\quad a_0^{(2)} = 1) $$\n",
    "$$ z^{(3)} = a^{(2)} \\Theta^{(2)T}$$\n",
    "$$ a^{(3)} = h_\\Theta(x) = g(z^{(3)}) $$\n",
    "\n",
    "Forward propagation essentially computes our hypothesis or our network's guess based on our current parameter values. We denote our hypothesis by the letter 'h', which is dependent on computations from the previous layers. As we update our parameters (our theta matrixes) our hypthothesis will also change. We will return the values for each layer in order to do backpropagation later. Remember: for-loops should be avoided at all costs so we need to vectorize our approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(X, theta1, theta2):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    \n",
    "    a1 = X\n",
    "    z2 = a1.dot(theta1.T) # 5000x401 * 401x25 = 5000x25\n",
    "    #ones for the bias unit\n",
    "    a2 = np.insert(sigmoid(z2), 0, values = np.ones(m), axis = 1)\n",
    "    z3 = a2.dot(theta2.T) # 5000x26 * 26x10 = 5000x10\n",
    "    h = sigmoid(z3) # = a3 = g(z3)\n",
    "    \n",
    "    return a1, z2, a2, z3, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create our cost function, which is a beast! The equations for both the cost function with and without regularization are given below. \n",
    "\n",
    "#### Cost Function \n",
    "#### $$ J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}\\big[-y^{(i)}_{k}\\, log\\,(( h_\\theta\\,(x^{(i)}))_k)-(1-y^{(i)}_k)\\,log\\,(1-h_\\theta(x^{(i)}))_k)\\big]$$\n",
    "\n",
    "#### Regularized Cost Function\n",
    "#### $$ J(\\theta) = \\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=1}^{K}\\bigg[-y^{(i)}_{k}\\, log\\,(( h_\\theta\\,(x^{(i)}))_k)-(1-y^{(i)}_k)\\,log\\,(1-h_\\theta(x^{(i)}))_k)\\bigg] + \\frac{\\lambda}{2m}\\bigg[\\sum_{j=1}^{25}\\sum_{k=1}^{400}(\\Theta_{j,k}^{(1)})^2+\\sum_{j=1}^{10}\\sum_{k=1}^{25}(\\Theta_{j,k}^{(2)})^2\\bigg]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost/loss/error function basically computes how bad our network's guess is. Our goal is to minimize our cost function, which we can do by manipulating our parameter values.\n",
    "\n",
    "If you're intimidated by the above equations, you're not alone. The first term of the regularized cost function just computes our network's error summed up over all of our data. The second term is called regularization and it tries to keep the values of both theta matrices as small as possible. Why do we want them small? Because we're trying to avoid overfitting to our data. If we can avoid overfitting, then hopefully our model will perform well on data it has never seen before.\n",
    "\n",
    "Now we just need to code it up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(theta_vec, input_size, hidden_size, num_labels, X, y, reg):\n",
    "    m = X.shape[0]\n",
    "\n",
    "    # reshape the our theta vectors into theta matrices for the hidden layer and the output layer   \n",
    "    theta1 = theta_vec[0:(hidden_size * (input_size + 1))].reshape(hidden_size, (input_size + 1))\n",
    "    theta2 = theta_vec[(hidden_size * (input_size + 1)):].reshape(num_labels, (hidden_size + 1))\n",
    "     \n",
    "    #We need h\n",
    "    a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
    "    \n",
    "    J = -1 * (1 / m) * np.sum((np.log(h) * (y) + np.log(1 - h) *(1 - y))) +\\\n",
    "    (float(reg) / (2 * m)) * (np.sum(np.square(theta1[:,1:])) + np.sum(np.square(theta2[:,1:])))\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the lack of for-loops? Vectorization is a beautiful thing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The worst part about all this is making sure your matrix dimensions make sense. Let's take a look and convince ourselves that we're doing something sensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 401), (5000, 25), (5000, 26), (5000, 10), (5000, 10))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
    "a1.shape, z2.shape, a2.shape, z3.shape, h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... Looks reasonable I guess...\n",
    "\n",
    "\n",
    "Now, let's take our cost function for a spin. This function computes how different our model's guess is from the true label. In other words, what's the difference between our hypothesis (h) and our label (y)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.229675726704277"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(theta_vec, input_size, hidden_size, num_labels, X, y_onehot, reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not good... typically you want to see a cost below 0.6.\n",
    "\n",
    "What we need is an algorithm to reduce our cost. We're in luck because this algorithm is called backpropagation. Backpropagation computes the partial derivatives of our parameters for each layer, excluding the input layer. We can then use the derivatives as a guide to find a local (or if we're lucky a global) minimum of our cost function. If we find a minimum, we can use its values to update the values of our parameters and minimize our cost. Because we used the sigmoid activation function, the first thing we need is a function that computes the derivative (also called the gradient) of the sigmoid function.\n",
    "\n",
    "\n",
    "#### Sigmoid gradient\n",
    "#### $$ \\frac{d} {{dz}} g(z)  = g(z)(1 - g(z))$$\n",
    "where $$ g(z) = \\frac{1}{1+e^{-z}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_gradient(z):\n",
    "    \n",
    "    return(sigmoid(z) * (1 - sigmoid(z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for backpropagation, which may be the hardest thing in all of machine learning. Because backprop is dependent the values of everything we've talked about so far, we'll bring it all together below. We will use the variable $\\delta$ is represent the partial derivatives of J of theta with respect to z ($\\frac{\\partial}{{\\partial z^{(l)}}} J(\\Theta)$) which is just a vector. This capital delta ($\\Delta$) is a matrix of all theses partial derivatives. Then by accumlating all these derivatives and following the equations below, we can put them together into D which becomes the partial derivative of J of theta with respect to a given theta, in other words, a Jacobian of our cost function, J. ($\\frac{\\partial}{{\\partial \\Theta^{(l)}}} J(\\Theta)$)  \n",
    "\n",
    "Remember, daddio, for-loops are for squares!\n",
    "\n",
    "#### Backpropagation\n",
    "$$ \\delta_{j}^{(l)}= \\frac{\\partial}{{\\partial z_{j}^{(l)}}} J(\\Theta)$$\n",
    "$$ \\delta^{(3)} = a^{(3)} - y = h_\\Theta(x) - y $$ \n",
    "$$ \\delta^{(2)} = \\Theta^{(2)T}\\delta^{(3)T}(\\frac{\\partial} {{\\partial z}}g(z^{(2)}))^T $$\n",
    "$$ \\Delta^{(l)} := \\Delta^{(l)} + \\delta^{(l+1)}(a^{(l)})^T $$\n",
    "$$ D^{(l)} := \\frac{1} {{m}} \\Delta^{(l)}+ \\lambda \\Theta^{(l)} \\quad if \\quad j \\neq 0 $$\n",
    "$$ D^{(l)} := \\frac{1} {{m}} \\Delta^{(l)} \\quad if \\quad j = 0 $$\n",
    "\n",
    "$$ D^{(l)} = \\frac{\\partial}{{\\partial \\Theta^{(l)}}} J(\\Theta) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(theta_vec, input_size, hidden_size, num_labels, X, y, reg):\n",
    "    m = X.shape[0]\n",
    "    # reshape the our theta vectors into theta matrices for the hidden layer and the output layer\n",
    "    theta1 = theta_vec[0:(hidden_size * (input_size + 1))].reshape(hidden_size, (input_size +1))\n",
    "    theta2 = theta_vec[(hidden_size * (input_size + 1)):].reshape(num_labels, (hidden_size + 1))\n",
    "   \n",
    "    \n",
    "    # propagate the network with a guess\n",
    "    a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
    "    \n",
    "    # compute the cost of that guess\n",
    "    J = cost(theta_vec, input_size, hidden_size, num_labels, X, y, reg)\n",
    "     \n",
    "    \n",
    "   # perform backprop so we can try to make a better guess next time\n",
    "    d3 = h - y\n",
    "\n",
    "    d2 = theta2[:,1:].T.dot(d3.T) * sigmoid_gradient(z2).T # 25x10 * 10x5000 * 25x5000 = 25x5000\n",
    "     \n",
    "  \n",
    "    delta1 = np.zeros(theta1.shape)  # (25, 401)\n",
    "    delta2 = np.zeros(theta2.shape) # (10, 26)\n",
    "    \n",
    "    delta1 = delta1 + d2.dot(a1) # 25x5000 * 5000x401 = 25x401\n",
    "    delta2 = delta2 + d3.T.dot(a2) # 10x5000 * 5000x26 = 10x26\n",
    "       \n",
    "    delta1 = delta1 / m\n",
    "    delta2 = delta2 / m\n",
    "    \n",
    "    #regularization\n",
    "    delta1[:,1:] = delta1[:,1:] + ((theta1[:, 1:] * reg) / m)\n",
    "    delta2[:,1:] = delta2[:,1:] + ((theta2[:, 1:] * reg) / m)\n",
    "    \n",
    "    \n",
    "  \n",
    "    #combine our two theta matrices into a row vector\n",
    "    grad = np.concatenate((np.ravel(delta1), np.ravel(delta2)))\n",
    "    grad = grad.reshape(len(grad), 1)\n",
    "    \n",
    "    \n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of backpropagation is to calculate the gradient of our theta weight matrices. In order to do that for each theta, we need the \"error\" associated with each layer, which is represented by our delta variable. Through a mathematical proof you can find in this repo, you can show that the partial derivative of a given theta is the activation layer that theta maps from multiplied by the delta of the layer that theta maps to, so we need all deltas and all activations to calculate the complete gradient of our cost function. With the graident calculated, we can minimize the cost function by finding the optimal values of each theta matrix that yield the best results.\n",
    "\n",
    "Once you understand the purpose of backprop-no small feat-, the only hard part is getting your matrix dimensions right, which is the hardest part of any machine learning task. I recommend liberal use of \".shape\". \n",
    "\n",
    "\n",
    "Let's take a look and hope it outputs something meaningful and doesn't crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.229675726704277, (10285, 1))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J, grad = backprop(theta_vec, input_size, hidden_size, num_labels, X, y_onehot, reg)\n",
    "J, grad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we can use the output of backprop to minimize our cost function. One way to do that is gradient descent, which is an iterative algorithm that just repeats backprop until convergence or our cost is small enough that we don't care to minimize it anymore. Instead, let's use scipy's more intelligent minimization functions that do that same thing as gradient descent only faster. For more info, please see https://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html.\n",
    "\n",
    "We just need to pass it backprop, our initial guess (random initializations of our parameters(theta_vec)), and the same variables we gave backprop. Then scipy will do the rest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 0.30091139816510926\n",
       "     jac: array([[-1.75422525e-07],\n",
       "       [ 7.82452436e-15],\n",
       "       [ 3.07925713e-14],\n",
       "       ...,\n",
       "       [ 3.21230467e-08],\n",
       "       [-2.06457268e-08],\n",
       "       [ 4.27670652e-08]])\n",
       " message: 'Converged (|f_n-f_(n-1)| ~= 0)'\n",
       "    nfev: 4707\n",
       "     nit: 180\n",
       "  status: 1\n",
       " success: True\n",
       "       x: array([-1.01360679e+00,  3.91226218e-11,  1.53962856e-10, ...,\n",
       "       -1.01351265e+00, -7.65894673e-01,  1.37742992e+00])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# minimize our cost function\n",
    "res = minimize(fun=backprop, x0=theta_vec, args=(input_size, hidden_size, num_labels, X, y_onehot, reg), \n",
    "                method='TNC', jac=True, options={'maxiter': 5000})\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We converged~ yay~ \n",
    "\n",
    "As you can see, this function outputs a solution array containing the optimized values of our parameters. Our total cost is below 0.5, so at that's an encouraging sign that we've done something right. Now we can use our optimized parameter values from the solution arrary, forward propagate them through our network, and make some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the our theta vector into theta matrices for the hidden layer and the output layer \n",
    "# this time using the solution array\n",
    "theta1 = res.x[0:(hidden_size * (input_size + 1))].reshape(hidden_size, (input_size + 1))\n",
    "theta2 = res.x[(hidden_size * (input_size + 1)):].reshape(num_labels, (hidden_size + 1))\n",
    "\n",
    "a1, z2, a2, z3, h = forward_propagate(X, theta1, theta2)\n",
    "\n",
    "#because of python indexing add one to get true value predictions\n",
    "predictions = np.array(np.argmax(h, axis=1) + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At long last we can finally see if our neural network makes decent predictions. The way to do this is by comparing our network's output with the true label. Then, we can divide our total number of successes by the number of guesses we made, in our case, 5,000. This metric is called accuracy.\n",
    "\n",
    "To my great shame, dear reader, there is a for-loop below. I hope you will find it in your heart to forgive me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 99.74%\n"
     ]
    }
   ],
   "source": [
    "correct = [1 if a == b else 0 for (a, b) in zip(predictions, y)]\n",
    "accuracy = (sum(map(int, correct)) / len(correct))\n",
    "print('accuracy = {0}%'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too legit! Due to random intialization, the accuracy might jump around by 2% or so, but this is a nice result, very similar to what Andrew Ng's exercise indicated we should expect.\n",
    "\n",
    "Should we take a deeper look at the artificial brain we just cooked up? Of course we should! Remember our theta1? The parameter matrix of our network's hidden layer? Well, it turns out we can use those parameter values to visualize the picture features our network was learning at that point in training. Let's pry open this neural blackbox and take a look under the hood!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x267551ca400>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6IAAABPCAYAAADxynhGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJztnXmstVWV5p8tzooDiohg+TnghAgOIJMyCIKowdaqjpi0xiFookl30mnLav6o7qQ7qY7pNnbSXQmVVuhOK/VpW5a2qF0iKsgsooiIfiropwgOOCuKvv3HPb+7n7Pevc+dz73nYz3JzT3Dft937bXXXnufNe0yDIMSiUQikUgkEolEIpGYF+6z3QQkEolEIpFIJBKJROLehfwhmkgkEolEIpFIJBKJuSJ/iCYSiUQikUgkEolEYq7IH6KJRCKRSCQSiUQikZgr8odoIpFIJBKJRCKRSCTmivwhmkgkEolEIpFIJBKJuSJ/iCYSiUQikUgkEolEYq7Y0A/RUsqZpZRbSil7Sinv3CyiEolEIpFIJBKJRCKx76IMw7C+C0vZT9I3JJ0uaa+kayWdMwzD1zaPvEQikUgkEolEIpFI7Gu47wauPUbSnmEYvi1JpZSLJJ0tqftDdP/99x8e9ahHbeCR+x4wBJRStpmSRGJjWDRZdiPcotC8iFg0udipaBmNk6fzQ+qL+WDR9IXTuV7HzrwR6VwEXi+aXCwiNpvHt91224+HYThwpXYb+SF6iKTv2fu9kl4QG5VSzpV0riQdcMABOu+88zbwyJUBAxdFIfzxj3+UJO23337bTMnqsWg8lhaHZui8z32Woub/9Kc/LX+302lftIWCuSctzvxbFDl2LKKO24mY9UMUfdFqs1NlZdFkeRF/iC4aj6Wdv45Euu5737qNvueeeyTtfH7nD9GtR8496dxzz71tNe028kO0RemI48MwnC/pfEnatWvXmkfEGRIXWzbpvsGhTWzLZsiFwjf5W4E//OEPo894Jt9Bz/3ud7/lNrye58att6FxHkFPj8fSmM9bxeNFlovIa6kuZsgFdPmPJWikX/NQyJFW6PTF9wEPeIAk6Wc/+9kUnS7TcYGOPN6KvsSFgP/0xXnLZ/SF77bDEOC84HXkj9Peu34e9EYd5/zq6bjt0G8tRB5CT4u3s2iNY7NZfI/j+KAHPWj5O+bf7bffLkl64AMfOLqeORdlCBmfhycnPrs1z7dTllu6GBmGf8jrb3/729F1ke9OZ+qLPuAt8+r+97//6Lvt3MC35BS5iJt1l4Hf//73kqRf//rXksZ6YyvX7CjLrb2N87lHz3bu4eL+Al0l9fcXyMssmjeL76uJSmHM+e9rZJT7eWBeezh/1lqxkWJFeyU93t4fKukHG7hfIpFIJBKJRCKRSCTuBdiIR/RaSYeVUp4o6fuSXiPptZtCleqvbrfgPPzhD5c0tvT+/Oc/X26DNed3v/udpGqtwhqAxUqS7r77bkmbZ6GPljLuL01bs53mhz70oZKkX/ziF8vf7b///pKqJeURj3iEpGkLxGZYMVuWsh6PneYej6Uxn7eKx4sgF9CB5SlapFrWo4c97GGSpDvvvFOStGfPnuXvnvjEJ061werq1m74shYrJnRBj1vB3AsgSb/5zW+m/ktLIffeFt76feAv/6N3fb3geu7n1lFeM+Y/+tGPJFUeOd/oA5bBhzzkIZKm53D0Vm+WlTV6MZxv0PXoRz9aknTrrbdKqjLv+NWvfiVpPGbeh42E3rRCE7l31G9SX8dF/SaNddxWeWm838zDBz/4wZKq3MJHSfrBD5Zsq9Q2oI3Tjjwgb25pXi1a/XzkIx859Z3LxeMe97ip/qALfBy+972lzBn0BXIfvSJSnSMbRU+WoxxLfVl2/vdkeb1hsowVeogxc72B3MJjxsFp+bM/+7Op+3z/+9+fei9VmYa+zfIsLYq+aEWVIHvQx329dgjzEX3Bf5cL1h/u3ZLptSBGG6GzHIxtXLN8vjMWeJbgLfPS9xK8Xo9ctPQFz4I3zGl/5oknniipyuZPfvKT0X3oD3znfnfdddeI9tXIRW9/EeXOn8V/5Nnbx/2Fz7nN2F+0IkXi2u/855kHH3zwVFs+P+SQQ5bb/vCHP5zqH/T5Hno9nujIY6nP5522h5M28EN0GIZ7Silvl/QpSftJeu8wDDdtmKJEIpFIJBKJRCKRSOzT2IhHVMMwXCzp4k2iJZFIJBKJRCKRSCQS9wJs6IfoZiJWVpxVOILQGcLnCEuSakgE4SWEfBBq4WEnuKZjWOpqEvs9NCsmpkMXIT2SdNBBB0mq4VK43wmRcPc54YG4xQ888MCpfnt7aF1NOFiPx/4diDz2fvZ4LI353ONx65lroXmnyQXw0AjCcQhziAVGCFGUpL17907RA571rGctv6ZfyBAy6KERXL8aeSAcLcrST3/60+U2yOnjH7+UDk5Yxs0337zcJoYtPuc5z5FUw9Uk6clPfrKk2mf6wJi5LMaiELPAHKGtzyP6Q4jRNddcI6nOOefbL3/5S0nScccdJ0k65phjRnQR6kQ4k4cErQfMc/QE8ubPJKwS+eC/y0XkJWPPONJfaVy4aVbxBeSrVXgj6rio3/zeUcdF/SaNdVyUSe/XLMTwd+hjLhL2J1V5veOOO6bo8vBRrkPe6bfrRdq0itOtFq05DH9aYbNPeMITJFXeXnvttZKm9Q+hpei2GHLn+meWXl0JXlCkJ8tRjv01shzl2GmOstwqCtSrBNoqOMbYE67v/SYdAprh11Of+tTlNsx91gZ0gofYMScIaWRdWW+qx2boC5+fm60v+I51BbnzeQEveRbX/vjHP15u88xnPlNSHTf46OG33DuGxTsvVgob9TkXC8uwdrEuSHUNhF/IgNMFrew3vva1pdMM4anPuagHZ613UZZ9zvH8r3zlK5LGqUW+TrH3OOmkkyRJt9xyi6QqN5L09Kc/feo+0O684DunI2Kl/YX3t7e/8L1lb3/B3kJaeX+xmuKDLhexTes+yGtv3864SJX/yDv9Q278Gciir5MRPR5LYz5v9R7OaV9ruPnGg3sTiUQikUgkEolEIpFYA7bVI9pK+I1J0G4RwfLDZ1gzW4iFHq6++mpJ04nDz3jGMyRViwPP9KR495Q53MpHeywiWLTdO4Ml5NBDD5VULQ2PfexjJU0nZV955ZVTfYAGt/ZFiyKW2Za1FR70eCxVnm6Ex9KYzz0eS5VvkceLJBd81yo5H48L4RoKEXmyOBb1ww47TFK1pn37299eboPsYPVCzrygETTjsXGanV6pyicJ9Px3j+jhhx8+9Sxw9NFHL7/mWRR2wWLm3oFvfvObkqplHo8OoKiGVD1A0cM0Sy7gsfOUez7lKU+RJD372c+WVC2Dbs3FIol3DL67F47+YCHGut8qXNODW8+hAyCbHkkBT4844ghJlQfuQUC3IF/0AU+MywAFFVYD+hX1mzTWcVG/SX0dF/Wb0854Rv0m9XVcq7gE89GttU6vVOX98ssvl1TnJUXBvJ/0nXnptMeCZ9AXPZEO5ilzxfU/84n7cL1bsHfv3i2pFs1BltzjiNwjB3g0GCvnxWMe8xhJa/OIIstRjqWxLEc5lsayHOVY6svyauQY+rwQEZ4fnsF93AuCHDDmsbCRJN1229LxeKeffvpU/570pCctt4keKfQFPF5NgZ3N1hfwWNo8fREjKOLxNy7/eArR089//vMlTfP/y1/+sqRx4SVvgzeGz+CBRw+0iiVJlaceHUHf4Q/vn/vc5y63oX9EQJx66qmSqldRqnOW/QTr3ne+8x1JdZ5Jdb8C7chLK7IiHjfnXutvfetbU98hJ4yr34/X6ODo9ZSqBw85QEd5ITR0B7zlu7XsL9hbSP39hUee9PYX8FhaeX/h8tHbX3ibGJ3COOIF9T4TlcJ3rIXeT+YTfWntubgf48m6555R2vR47Pec1x5OavN5NUiPaCKRSCQSiUQikUgk5opt9Yi6pQprIVZIrFWeU/j1r39dUrXeYPHBKixViwNWIn6Zky/TKjUcD3x1y2cEVgm32mIFwxqKJQPLqlStVFgEPa5dkr773e8uv8biAD1Yr1oHALfKskfA5x6P/T49HkuVzz0eS30+tw4O7/F5keQiHh7tHgUsitCMRwKrvOeL8BqZwTrncsG9sariuXU5gwexzHgrXyaWGaeN50JhNcPLTL6gyz/jRX4lVkMs2/4MPL9nnnnmFL14GJx20MrlYCzwUHG984I5Bg+xvtM/5+1RRx0lSbr++uslVbnzPGOsheRPxP63EI8FcO8FlnlksZUvxRjBN7yL7mUgDwmvLl5AZNPnMPQwH1pyEXVc1G/SWMdF/Sb1dVzUb9JYx83Sb5Fmlw/mHB5t5IT++ph/4xvfkCR94QtfkCS97nWvk1S9jZJ0/PHHT13PfZ/3vOctt0HPxGiLVjRNzJvlvm7tRs7QF3iE3DuADPMMdIx7VomUcC9MfBZYTX5rT5bda9eT5SjH0liWoxxLfVn2MY9eTeZj9ExLVT7wqEGDexKY3/QPufd1Dpq5N3S6LsCbEKN5Zh13sNX6Ah5Lm6cv6DPyGo/L8rGKXkrmJ3mJUvXc8Cy8k358CHofnsBr1z+uu71/3N/7edVVV0mqPMGj6XMOephPzDWfOxdeeKGkyvczzjhDknTDDTdImo4qYR/A2Kxm3+nzG8Bf9C30wRtokaosX3LJJZLacstnu3btklTniq+t8Nsj0yJW2l+4R663v3C6evsLz4FdaX8xK+KDZ7l3kvvBS7yC3u/owWdfRhuPKkDmoJm50vLkIyuzcp17PJbGfN7qPZy0Oj63kB7RRCKRSCQSiUQikUjMFdviEW1ZBrFSYelqVePC6hst6m5lxTqN5Zu47Ve96lWSpi2fWNiwfrUOywbRQuneLOK08Qpg2XKPY8vyIVVLmcdiY1kkzwNLl1vDsJq3KqeCyOeN8FiqfO7xWBrzucdjqV/lcJHkAsuP5ybG/uFFoQ/Ih1tbGUc+w0KLR0aqsf6XXXaZpHauqVunpGrJ4/7eh5jDhiUVOqVqicWTAB+RW6nyGatay5qJZwuLc8yHcS8NPGUcW4c+IyvwgGdSDVCqckHeAl43cgC9UiB5MYwNMtCqZo2stCrExcPmATk07snE8gxv0A2eFww9VPyFFz4PuB7vPv/h13XXXbfcFqtm9Bp53mXUcVG/OV3wuKffpLGOi/pNGuu4qN+877GCp9Mev8Ob+PGPf1zSdM7XaaedJql6lC699FJJ0lvf+tblNljHkU+XQUCfoweslRsao1OQC7cg47VC/8AL1/9Y1skb5TufRzfeeOPUven7V7/6VUnT3jbGMVbwbFnjoyx7Hm5PlqMcS2NZjnLsr6MsuxcEnsa5h9z6ehJzoVrjij4lX/CLX/yipOmcKHiAVx0PgufRoYujt5r7t/RGvP9m6wuPMFiPvoheZ6cx8rJVA4Lv8MrAP68siucG/kGze6rw7EE7/fZ5FCNV0CXMHffwMbbIPfntzCGprl2vfe1rp57F2Et1D8Iaw/4FufPoAbzArJ+t2hZxHsJrlxc8bvQLucUT6pEpN910k6TKP3Sz7+FY/9mDIF++z0B3xjzntewvXFf19he+h+vtL1ymV9pfuHzEOcf9XBfE/P9WJW5oR6eff/75kqR3vOMdU/eQqk5H9rifyzb8Wk3kRI/H0pjPW72H88/SI5pIJBKJRCKRSCQSiR2N/CGaSCQSiUQikUgkEom5YluLFXlIFW5kwuVw+XtCOWEOJMTinuZYBqmGQBCWgxuaJHYPYyE0CZdzLD8uVdc5tEKXl/gHuLMJ1fCD0QnxIEyHkBTCLSmWIklf+tKXJFX3NmX4PeyNMAlCBjxUOALaezyWKp97PJYqn3s8lsZ87vFY6hd5WQS5ADEEwd8TwhlDTAkF9CNjCLsinIb3hHdI0oknnjj1DEIt/JknnHDCVP9iAQRvCz3wBr4deeSRy20I4SF0iRAQL1zzlre8RVINBzv55JMlTcs/4Rw8g3BZxtNDcGJISpx7Ug2DQS4YMy9QQXtCEpl73M9DcwmNoU0MC5PGRTPon8tr76gAQuN8DkMrYYIUF/AjFhjjj33sY5LquL7xjW9cbkPIH8URuJ6x8nBt+BzDRr0PPR3nIXZRx0X9JvV1XNRv0ljHRf0mjXUcutl5inzRT+5D2BuFzaTKd2STo5O8EBH8Zy5zP9d5hG4Suuxy1QPyy38vrc8RLIRvUWCNtt6/U045Zaqth5uhS2LIHjxyOpHPWSFVPVluFfSKshzlWBrLcpRjvz7KsusLvothjMiOy23kE3qDkEWpFuw45phjJNUxp2iLVPUz4eWESH/yk59cbnPWWWdJquFqhNt6uH/EVusLD+9ej76Ioe9SXWPQoRR7Qjd4CCzziDWVsFLXGzGcmz65bLNmwVuXB0D7ePQJ+tt14Ctf+UpJVT+QwuPHdKDPKGgHbwlhdR4gcxztwjridMZiNPFIOP8MvqEL/EgQ9hM8g7UV2faCM8ggIcTolFaqGbLYGiPkKK4ja9lf+B6it79gbyH19xde3Gyl/YXvLeLxejzb+8S943E+Puegg1BX7ocssbZJVZfAU/aApEv483lm60jGmL4UeSyN+bzVezipzefVID2iiUQikUgkEolEIpGYK7bFI9oqhBAPMifp1gsq8GsbSwEHxrqVj/bHHnusJOlzn/ucpGrB8KMXIh0tixRtYpnkVmI5Fq6WRRxrBNeT7E3hDrcq44nA6kG/vTS5l2yXqrXKrTmxfz0eS5VvPR5Llc89HktjPvd47J/16J1F83bLRaSvZcHGyxE95cgSJdSlapGkUET0nkrS+973PknSi170IknS0572NEnTskM/6bt7Ufy+UrU4I5scb+Jea6xdePqwQLe8KS984Qun3rs3lvtESyr3czqx7DIOrbL28P0FL3iBpOoN90IbkWY8RFjc/cgAeHnFFVdM0eAeTo7D4Dusy24pjoAX9Jd7SNWyjlWU+3oRH4pmvPnNb5ZU+ecFMtAXWFmRGXSNe5h6c87HvKfjWkcPgKjfpL6Oi/pNGuu4qN+kto6Tpot+UWiC/lDM5F3vepek6aIQ0ZqMx9bHCC8Y3qdPf/rTkqb1InOeee48iEBu4TveT9ftfIZOoNiIe9LgV+8YFweWbPqCDMVjXVZCT5aRY6kvy1GOnQ5kOcqx1JflWUcaxIPpvZgScxaLP3x0rwp6Ak9caz7wjA9+8IOSpPe///2Spj3leD/4z9i0it6BrdYXHuWwHn3R+hy5ZL2Dx9zX1xzWX67hqAt47f0CjKPLBbzkPvF/i1b+s9Y435h/RHagN1rH9L361a+eur/rRfQC0RVcg1fS5zl8Qt5ahaBiQTz652siXiv2MHg78W45P1kv8axybUsm+Y4IFtf56KZY9G4t+wv3pPX2F87b3v7C77PS/qJ1HBjPhi5fl2KEFMe3tAq0EYHHGLNW+PE5zE+OxuFZHr2HDCIfkcfS+EjByGPvz7z2cNJsPs9CekQTiUQikUgkEolEIjFXrOgRLaW8V9LLJd05DMOzJp8dIOnvJe2SdKukfz4Mw129e0TEmH2pWqqxfmGtdYsUFnWsTtFjKFXLANYvrApYEbEgS9UqhMWydRxDBJYQtxRAM1Y0nkV+gAPLK7l8n//85yVNW/nhBcdNYFFxrxFWl3iIuiPyucdjqfK5x2Op8rnHY+87fO7xWBrzeZHlgvu5pQwLEtZeLMbQ695T8pHIp8ELgmVVqhZrZA+63CMEjS2PkjRtycbahxeJPvh4Qiv/yXfwsvZYubGMkRvhXmYsuTGvLx467n2IOT7utcODCS/wcuIZlapVDh5Sop7chpb1nDHCWuj0QQfj2irlHq3vyClj5bkgyAWeOJ7p85z8Q6zwWGb9uAM+I6+J9635Ce2zvHZRx0X9JvV1nHuWejou6jdprOOifpP6niTXxcxdvALk9DD/3fOC9wq5xePkeaTki8KTN7zhDZJqvphUZYZnEJkxCzwT3rqnlvFjXtI/19d4c8j/hDee64U80E/kFI+XW62R8+gBa+WrRVl2nvZkOcqx9xNZjnLsn0VZdi9IT5aRN/fwIQ/wFA+FR89cfPHFkmpOPl67D3/4w8tt4Bd5YciA13qgDXyORy045qUvPP9zI/rCPV/x6CDkFhn1PDOeBb+QW8+5gz9xbjjtPIvoAdrO8pSzL2Cf4LmTrAlEG6Gz8AZK0jnnnDNFB+NLnrYkffazn51qg5xBl3u4Wbfxjs3y5qJXWbN9/0MdDZ7tOlia9iSjL5AZ9Lh7HolgQQejd1perpi/uJb9ha+tvf0Fewupv79wvq20v2it1VF3+n6KZ8JT1jt0gtMT180Xv/jFkqpsSXVtgP8xMkCq/ImRjA6+6/HYX2/nHm61WI1H9AJJZ4bP3inpkmEYDpN0yeR9IpFIJBKJRCKRSCQSK2JFj+gwDJ8vpewKH58t6eTJ6wslfVbSX672ofyids8XFjE8OVgXqGDn3/HLHEuZ52VwT6x0fMfnWMylagWKuT1u7YtWk1YuAtYIPC5YqNxSiVUJax+HlkOXx1njFeOZWC7dg4YlBCtVq0pV5HOPx1Llc4/H0piXkcf+HXzu8dhpjl6kRZILwHi41wjavcKdVL12HqNP7g55i3j4zjyz2oDI86HCIx5ft2ZiwcaqRx/cmwjigfTkQHl1SCzsHOINb1228d5yP6x7fmAyVsJ4CDVy7/LfyomQpmWH/sALrI5uQcVDddlll0mqljvmonuukAf6gjfceYuFn3lJf2fl9NBvnuWyffrpp0uq3gG8Fq5bsFhfffXVU/drebP4LubWusWyFZng1/jz+R/1mzTWcVG/SX0dF/WbNNZxUb9JYx0Xq6P6M84444ypZ3M/ZEGq8w+559nuHfvQhz4kqVbPZJ66vDJG0SLesmTHvuAJbrWN1m63YMNvdAI88Bw0PC2MFZ4RxtUPqO/BZbsny8ix1JflKMd+P8Y4yrHUl2WnfRafpem8WV6jX/Gcex4XERToUirh+nqJfj3++OMlVR67/mEsoJ25gqeiFR201frCdcFG9IWvf8gyvGVetXKR0dus2VSD9ar1XM9nPNt1S6ziy3x0+Y9eV9rgofWoIeZWrDZPpJJUPal4i1hbXY/hUWVsoB1++f4AGZpVaZTPkBn67bqYPQNeO7yerF1ekZuxQOchO6x7Ut0LIdPoRao1+71nYaX9BXsLqb+/cLp6+wvXFyvtL1prNWC+O7+QJ2S7VS2YsYG37O/QyeSV+jO4H/9dn9Gf1VSe7fFYGvN5q/dw/norPKItHDQMw+2SNPm/tqoHiUQikUgkEolEIpG412LLixWVUs4tpVxXSrmO2PNEIpFIJBKJRCKRSNx7sd7jW+4opRw8DMPtpZSDJd3ZazgMw/mSzpekXbt2DdI40Vaq4Ri4mCk97eGLuH0JoyBswkOpCB8g5IOQr5Ybmc8IoSQMhlAoaey+xz3tYTqEmxBqQZig0wXoO+56yjb7/XDTEzaBi98TkfkM2jlM3emNfO7x2L/r8dj70+OxNOZzj8dS5XMMX1kkuUAeGBtPdKcfhClwH0JGPaSHkAjkgcIDHuLA9RRWInTJizJRjIMwHQpcIKP+zHgsATLk4R2EGhFSQtiPF2uJh2LzTC9iQmgjfOI7ZNzDzuO8aR3jgjzAY8JpXP5j2AqFSaCTgkdSDfWm7xQJ8ZA9xpb7MB6EMMXnO+C/h4vTZ2SS0F8PDSI0Blk8+uijJU2POSE4FGUhtA7ZcQNgDGlrIeq4qN+kvo7zkL2ejov6TRrruKjfpLGOiweSSzU8iiImhKfGEDephubRL8KRCPf2Z/EdY+5FgRh/aEW20RetAiqE9TFPXW74Dj1E2JQ/MxZIgpe+jjA3kC/GCnnw+YQ+XU24bpRln7s9WY5yLI1lOcqx1Jfl1jFlIIa0OX1f+MIXpr4jxNDlgnBn1gSe2Vq7YqEZPx6FvqIvGBvuOyukeKv0haddbJa+gNYYPsq64HLGuktBKOTN+U+BFD4j1NHvQ59jgSWX30gz/IYuT+8hXPczn/mMpDrnvPgUobnMw49+9KOSpuccxdXQX/C0tYYhB1wTj1rz/nE/9jGeVgLtpCMQitk63oTrGXvk1feCu3fvllT5wxxxfc0Yz0rxWGl/wd7CaYz7C+dJb3/B3kJaeX8xa09OG98Lwif6FdPApKoL2I9xH45S8bWaezMPaet7y7h+tPQE9PR47J/Naw/n/WnxeRbW6xH9qKTXT16/XtI/rvM+iUQikUgkEolEIpG4l2E1x7d8QEuFiR5dStkr6a8l/Y2k3aWUN0n6rqS/WMtDY3Eaqf4Sp6AC1lxPPqfIBcm2JOb7r2+seVjEsQqRgO1FW7h3tOS1LBBYWfn17xY8LB5YHKDLrTlY2bG00U/oorCH3wf+UELcE8SxhOMBcGstiHzu8dh50eOxNLaoRx57f+Bzj8dSvwDCIsgFNHIfrvFCDxz9gEUcK9hLX/rSKbqlaoV3i5Y0bc3HmhyLkCALUuUXljvkAnq9n1i0sLBjZXMrfDyyhP75fbDy4VmijVuK6Rde61jswGWxdbyB0ytVvmNNhmb64m3wkEMz7z/ykY8st8WTQB/gnxdoYCywuntZ/B6QGTxObj2ET1gaKavuuoD+4AXHE+AWVOQc/rSKKIF45Azv3YsUdVzUb9JYx0X9JvV1XNRv0ljHRf0mtXVcBPxBLvBoU6ToE5/4xHJbjrjA20kUggO+c9QDxzL4ES9YzSO/e0W3pMpbeOTzHD123XXXSaqeF/dIcz0ejSOPPFLStEcIueIoCgrzIB8+5rHUf8uL25Nl13k9WY5yLI1lOcqx0xp56/RFWY50+lqNnoGXeAWcF8jnVVddNUX7ySefvNyGaAr0I7LjY423Ag839Mw6YmSr9YWviRvRF44Y/QRP0Aneh3g0EfLrUQisl8grz3ZdQDEgvotHFDkdkWY+d76hd5iHeAjdI0r/mJ+sxz7m8BddBW/pp68ZPBPZm+V15n5xHyPV6B32G8g0RYzcY84e5HOf+9wUfS4XeNEZh3iETOyzt1nL/sL729tfuHeyt7/wPdNK+4vW3iIWzWx5rfEytzx9eB4QXX4EAAAWT0lEQVSRi5e97GWSpJtvvnnUlrWGNZH7udeU++CBBy7H8KfHY2nM563ew0mz+TwLq6mae07nqxev6UmJRCKRSCQSiUQikUho/Tmim/Nw+9WMpQyLFL/W/fgQXmPtxtLjFk+sQzHHi2e5FwqLA9ZH97JFYFXA4uZ5dLzGw4IVzfuH9QCLPPThQfC8jCc96UmSqkUQnrilGK8YfJtlgeC7Ho+lytsej6XK5x6P/VnweS08jveYRfN2ywXygJWUNu7ZwGuC1YvS7TzTrZrQTO4HtLt3MuacIlNuOYu5JDE3tGVhxLKOldrbAHiMZdYtoljj4AVeMj8MOeYy4FGL1roWsAS654X7UMYeHnsOGp5PPFbkcOAtaOVZcl8s4m5hxJoN7eTmtOZetG5Dg8tSlKuLLrpI0rS3E5qZ+3hiXC6gHa8649A6ggbvDFZfxtrpjTou6jdprOOifpP6Oi7qN2ms46J+82dFuLzGeRQ9cqeeeupy2+hVZH66F/zyyy+XVL0fyI63YRyRi5iv3PLS0Iaxcj2Lx4s+XHrppZKkV7ziFcttsE6Tf8u89Pswp5AV9AfzyfnGs3reI0eU5ZZ+jLIc5Vgay3KUY6kvy8ixNJZl3kc97v2LOf3ubceDhwfuhBNOkDSd/8l1yORZZ5019WxpnPPH2LQ8YPPSF54juln6guuhA/5xjXvpIw+QO5db1gp0AtERnovPdTzD110QZZn3zAvvA7r87LPPllTHzvUs/GeNoV+sn1L17MbjOVr5n+S+tvQEcHmSxhFiknTFFVdIqmOF5/7KK6+UNO1J5n70C3nxiAB0+wc+8IGpZzv/4xFmrWMIN2N/4RFOvf2FH9+1nv0FstPyoLNHQD7i0TZSrcsR643gtffcfmSH/4yZR9j0jmR0eY0ezMhjbwO2eg8nzebzLGx51dxEIpFIJBKJRCKRSCQc2+IRxeLg1lF+SWO9wZLklkosPIcffrikailzaxgWH+6DxYC2eM2kGvOO1whLnlsSsPhEi6Bb57Hw812rgicWECz0PBs6nS5A3hRWCedXtGq3LNeRzz0eS5XPPR5Llc89Hnt7+tPjsdMOLxdJLqAVCxLv/WBucjWwJGFFw5Lk8fdYoffs2SOpWrKdX9AIzVi43CIY5XVWbhXyyf2gxw/dpg0eY/rrcoGlmP7hnfTDkPFAQBfj18oXjJZJ3ruMY8GL1ulWX/E804f3vOc9kqqVU6p8wgqJDLSs5m4lXwn0F6uoVwokdw9PL/LqFuxjjz1WUpV7xtp5i5UWb120Urv3qOUZdzr9O+4T9Zs01nFRv0kr6zj0mzTWcS2vXdRxrTbIAXMPuWNcPQoEryJeImTA9RnetJjz5bmO5BNHb8+sQ70ZT9o6b/C8nXTSSZLqQeTu2cMjFK3n7qkF5I8iJ7FColT5FuecoyfLyLHUl+Uox9JYlqMcS31ZbnkHImLkiFTz3uAfnnevQooMHnbYYVPvyRmV6tjQB8bBD6/nubG2wmoqVm+VvoDH0sb0hQN6WI/wmNAXX5/ITUTe6Kd7f5hHeP2gjzw9/w79wxrkUS69daRVBR9dxVjB2+uvv365DfOGiuHoRfcCwSfWCHQMcueeX7zes441bOXwS1XepMp3PNysw/TXo/fQs+g4Ij6OP/745TbocnjLfsW9sNzbZcXp9et7+wvfH/f2Fx4F0NtfeOTbSvsLn3vwlD5wjetZvNbIYKyWLdWoHbySyEfLGx4/iznh0nQubqQZxHz4yGNpzOet3sM5XbN0XAvpEU0kEolEIpFIJBKJxFyRP0QTiUQikUgkEolEIjFXbEtoLi5dD4chjIbQItzHHoJA8REKzRD64uFIhHpQCAN3NCEbHo6B+zge2D7r+JZ4CLFUwydPOeWUqWeSrCyNi1PgfieswstCExoBzfHwZqnyMIYXOCKfezz21z0e+3U9HjvN8LnHY2nM50WSC8YiHpPickGoBrR6IR2nxe9D4QFCcz0EOZbQpw8emkWIRwy3in3zNvSLcCkPx4hhyhRtocS7VMM56AMHJnvpe+ggHIn/MczD+xffuywSJgSfGF8Ph+Q1ISiMMUV0PGSMUuuERVLwgf5KNXwlFhuZFTrDf0JmCNvx18gtRWp8jlCCn/DzSy65RFI9LFuqJeEBz6J/3s8ePPQr6rio36Sxjov6TerruKjf/Hr4FfWbNNZxcZ56G3iIzBCe5qFxF1xwgaQ6Z+i3h0YRdnXEEUdM9cHlNR5eH8PoHPSPcDxk0vnG/KENtHs/kR3mHDS7jmE+v/zlL5dUQwkJH+Vap7039/x1lGWX6Z4sRzmWxrIc5diftRZZRi4Ih2ylg0Az8uGhdsjI7t27JVUd43JBPwh15FmeIgAv0BuEsLXW6nnpi83iscsFfUZu4Td8vOWWW5bbwmfGobVukp5CAReu8cI1yC7hh4TS+tybdUyO0+m0cz+Oa/KiiPAW2lvHfbDWUKAN3jC/WPecZuSjte/ks3h8i6/5rL+EkfKeazy8GJpjeL7rYq5jjUXneeE42rAOtEIyV9pf+Hzq7S+8KFBvf+G6eKX9RSsFKBa3bIWh0xYeeFpJLNxEOPZLXvISSdPF15hrPAs6Xf4J350V3hqfGXksjfm81Xs4b7PS3ItIj2gikUgkEolEIpFIJOaKstak0o1g165dw3nnnbf8vmV5w+qHFca9Y9BKueRYBEaqlgG8HiTSY3HwIkP8ksc6sRpe8IvfrWlYl0j+x/LsHlH6A82xvLdbWOgzVq94kLL3B8wqjBH5HHnsz+zxWKp87vHY6YpFc9bC40WUC6zcrSN2sEJ/6lOfklSttu7hQ4bwftBPTz7ns3hkhieLx2JOs+QCC1m00LsVHm8R3ltk0pPrKQyDBzoW8JCqxRUvMXTFpH2p75XxwgXwBQslXgKfI7H0OnxiXKFbqlZ7rKNYC93CiEWeIiarsfrFNtEr3uqfF2thjPgOfrkFG48XcgEPkPG1Hi4ddVzUb9JYx0X9JvV1XNRv0ljHtYq39WTZ5yceA+Yh12Cldr4h53hB4vEyUp27yAE88WiLOJ9nzTloxYsFD/waxhp6mJ9edITCbNHr5gUooDF6K2JxmdViI7Ic5Vgay3KUY6dxPbLMeLQKG1FUjjannXba6HrWAcaBozn8OuSe/rl8xcI3jPGs9WSr9QU8ljZPXyD/6FfkFC+xF7lxr5pUdbHvp/AgoYtZe5y3eOK4X+tInJXodV0Vo0BaHmnGHLnAQ9q6D/Ob8eCZXiAMDyb3m7WeoJPhretH1mLuhzeX+/k+jz4Q+UMfGCup6mt4EI88ktrRKBEr7S98D9HbX/ga3dtfuFystL+YxWPaeIQB/GIOI8u+50WX+NhKlX/uBYceolJ4phdA5Jmr2V/0eCyN+bzVezjvD7Sfe+65XxyGoYZvdZAe0UQikUgkEolEIpFIzBXbkiMKPC+PX9AxH8M9JVjS+WVOHLNbIrD+YGHAmtkqpYwVbi0WiJgr6rRiEcRK4Zb1aPnjfTyeRKpWUKx+9MktUrOs7hHwucdjqfK5x2Op8rnHY2nM5/XweJHkgu+4xvNlORaCPB2sj3hgWkctHHfccZJqnoyPOZY6LMXR6yNVy10r3yQCqxfWVqyceFmkav3i2ci/W4qZC+RVXnPNNZKkG2+8cXSfyK9Zxw9FtA6Ah3bG3POI8Apgkbz66qsl1dwqzwvDokiOBPLhcyTmBq0GMY/UPQJ4MpB35NVlkTypE044QVLlm+faQSvWWnRK60ib1SDquKjfpLGOa3k2ejou6jdprOPWot98PKCLvmOtZt77fGLuM2fwmrrOw7uALsAb7pEK3Mf11kpgfnJf96ogr9DBff1YJdqgA7CwuwUbr0k86iha7leLniy7R64ny1GOpbEsRzmWNibL7tkA8AtvAHz3Y7fi0TqXXXaZpGlvLrlTfBa9z1KV4bj+zsJW6wtfqzdLX3A9/MZbhG5mzkh1rkEXfXDvPJ+hk8lpc68PR4rA45ZHqAf0ms8VeEitAOSDyAOpeiPRh3gRocX7BS/QKS3eriWfDi818973izwT+SCKI+ZBSzUqhe/QQ37UHfsWaEc3+xj1csodK+0v2BNI/f2F77N7+wu/z0b2F7FvUpUR9go8u5Xfin5lD8EY+VoRczFbtRXWIss9HktjPm/1Hi4+fy1Ij2gikUgkEolEIpFIJOaKbc0RnSJk8isb6xpWSa/IhCUQKySWt0MPPXTUBi8ZbVp5h/Gw4LV4OtzjhKUByxE0O2+xOO3du1dStVZDr1s1Y14BVjC3yK7FYwB6PHaaezyWKp97PJbGfN4Ij2fRvFPlwu/HmPNMr9gpTVu7Gc+Y4+ljFKsSRo+TNPvg8R6wwCF3nvcRK/vBE/eakpdG//AOuMcr5s7EA5lXA59PkT/MEbc+wi+8YdCFldut1DG3Crrce8R3q8n1Wg0Yf57V8vZzAD1jhGXc5z+8jPk6rUqGa6E55sx4rlHUcVG/SX0dF/WbNNZx69Fv0rjqKLkv0Ou8xTuEbEKPzzm8A/SFCt2t6pBrsb5DZ2u+4rXjOzwTbimnf1jW6ZfTjtzDy1h9d7PWftdjPVmOciyNZbknx/7drErVvWtczngmehoeu67Cs4FXDNlueaGQITwdvi6hD10/bwSboS9a0QNgo/oiViiN+dBS5VvMT/YcOfoAffTTZTuOzaxcxR682ioyQmQB8Mr28DRGI7hMw2fWDdboVrQEcrYeXef5fbEeB89GNlu1RdDTseq23xua4a3L9lrkore/cC9sb3/hOe+9/YXr0I3sL4BfE0+5QG94ZA17SfpD/i389+rkXM+eENpblWfXgshjf72de7jMEU0kEolEIpFIJBKJxI5E/hBNJBKJRCKRSCQSicRcsWNCc1eDGCKJi7kV2tAL5WkdaLtZmBU+1AoTivQAkstx0eMu90IDqzl0ej3o8Vga83lWKE/ru63CTpeLeF/ocvpi+FYsxhPbS+sLw10NfR4mAi8jT71NlFfu0wrZ4Pr1hmqD3hEv/jk08j/2wXnOd4SdEM7iYWWEYjH31kO7y1bUBbxvHSUU5cGv5XWv+MVmyXNrjsT5E/vUoifKizTWcevVbzwrFjGJIf7+Gc8iLM9D6AlHjgVsPKyMebjWYy+k1Y1NS4/15pjznzaz9OFGEO/foiOGDbbC66OOcbnYDFleSxivNA7zRD686As00hY90SpitZHQ3M3WF36PzdYXMaSwVQgnHiHEsz3UNB53Q3iky/pmpEe0wutJH2DMXRbicT5HHHGEpPYazXXQzP1aIbpr6UOrbW8cWwWSCO8mRJr7eQpKDLEG612rQdxftNawuL9ozbl57C8izfFYEtel8ZmE8bJf8NQ1dAL83yqdLI35vB17uAzNTSQSiUQikUgkEonEjsRcPaKllB9J+rWkH6/UNrFj8Wjl+C0qcuwWGzl+i4scu8VGjt9iI8dvcZFjt7h4wjAMB67UaK4/RCWplHLdaly1iZ2JHL/FRY7dYiPHb3GRY7fYyPFbbOT4LS5y7PZ9ZGhuIpFIJBKJRCKRSCTmivwhmkgkEolEIpFIJBKJuWI7foievw3PTGwecvwWFzl2i40cv8VFjt1iI8dvsZHjt7jIsdvHMfcc0UQikUgkEolEIpFI3LuRobmJRCKRSCQSiUQikZgr5vZDtJRyZinlllLKnlLKO+f13MT6UUq5tZRyYynlhlLKdZPPDiil/FMp5ZuT/4/cbjoTSyilvLeUcmcp5av2WXO8yhL+62Q+fqWU8tztozzRGbt/V0r5/mT+3VBKOcu++6vJ2N1SSjlje6hOgFLK40spl5ZSbi6l3FRK+ZeTz3P+7XDMGLucfwuAUsoDSynXlFK+PBm/fz/5/ImllKsnc+/vSyn3n3z+gMn7PZPvd20n/fdmzBi7C0op37G5d9Tk89Sb+yDm8kO0lLKfpP8m6aWSninpnFLKM+fx7MSGccowDEdZ+ex3SrpkGIbDJF0yeZ/YGbhA0pnhs954vVTSYZO/cyX97ZxoTLRxgcZjJ0nvnsy/o4ZhuFiSJrrzNZIOn1zz3yc6NrF9uEfSvx6G4RmSjpX0tsk45fzb+eiNnZTzbxFwt6RTh2E4UtJRks4spRwr6T9pafwOk3SXpDdN2r9J0l3DMDxF0rsn7RLbg97YSdK/sbl3w+Sz1Jv7IOblET1G0p5hGL49DMPvJV0k6ew5PTuxuThb0oWT1xdKeuU20pIwDMPweUk/DR/3xutsSf9zWMJVkh5RSjl4PpQmIjpj18PZki4ahuHuYRi+I2mPlnRsYpswDMPtwzBcP3n9S0k3SzpEOf92PGaMXQ85/3YQJnPoV5O395v8DZJOlfShyedx7jEnPyTpxaWUMidyE4YZY9dD6s19EPP6IXqIpO/Z+72aregTOwODpP9XSvliKeXcyWcHDcNwu7S0gEt6zLZRl1gNeuOVc3Ix8PZJCNJ7LQw+x24HYxLq9xxJVyvn30IhjJ2U828hUErZr5Ryg6Q7Jf2TpG9J+tkwDPdMmvgYLY/f5PufS3rUfClOgDh2wzAw9/7jZO69u5TygMlnOff2Qczrh2jL2pTlenc+ThiG4blaCod4WynlRdtNUGLTkHNy5+NvJT1ZSyFLt0v6z5PPc+x2KEopD5X0fyT9q2EYfjGraeOzHMNtRGPscv4tCIZh+OMwDEdJOlRL3ulntJpN/uf47SDEsSulPEvSX0l6uqSjJR0g6S8nzXPs9kHM64foXkmPt/eHSvrBnJ6dWCeGYfjB5P+dkv5BSwr+DkIhJv/v3D4KE6tAb7xyTu5wDMNwx2SR/pOkv1MN/8ux24EopdxPSz9k/vcwDB+efJzzbwHQGrucf4uHYRh+JumzWsr1fUQp5b6Tr3yMlsdv8v3Dtfq0iMQWwcbuzEm4/DAMw92S3qece/s05vVD9FpJh02qmN1fS4n+H53TsxPrQCnlIaWU/Xkt6SWSvqqlcXv9pNnrJf3j9lCYWCV64/VRSa+bVKE7VtLPCSFM7AyE3Jd/pqX5Jy2N3Wsm1R+fqKXCDdfMm75ExSTH7H9IunkYhv9iX+X82+HojV3Ov8VAKeXAUsojJq8fJOk0LeX5XirpzyfN4txjTv65pM8Mw5BetW1AZ+y+bsa7oqXcXp97qTf3Mdx35SYbxzAM95RS3i7pU5L2k/TeYRhumsezE+vGQZL+YZLDf19J7x+G4ZOllGsl7S6lvEnSdyX9xTbSmDCUUj4g6WRJjy6l7JX015L+Ru3xuljSWVoqtPEbSW+YO8GJZXTG7uRJ2fpB0q2S3iJJwzDcVErZLelrWqr4+bZhGP64HXQnlnGCpH8h6cZJvpMk/Vvl/FsE9MbunJx/C4GDJV04qVx8H0m7h2H4v6WUr0m6qJTyHyR9SUvGBk3+/69Syh4teUJfsx1EJyT1x+4zpZQDtRSKe4Okt07ap97cB1HSEJRIJBKJRCKRSCQSiXliXqG5iUQikUgkEolEIpFISMofoolEIpFIJBKJRCKRmDPyh2gikUgkEolEIpFIJOaK/CGaSCQSiUQikUgkEom5In+IJhKJRCKRSCQSiURirsgfoolEIpFIJBKJRCKRmCvyh2gikUgkEolEIpFIJOaK/CGaSCQSiUQikUgkEom54v8Dw1U7irPqTaIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1152x1152 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ran_samp = np.random.choice(theta1.shape[0], 20)\n",
    "plt.figure(figsize = (16, 16))\n",
    "plt.imshow(theta1[ran_samp, 1:].reshape(-1, 20).T, cmap = cm.Greys_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah! You can almost see numbers starting to form, like the network is beginning to understand what a handwritten number might look like.\n",
    "\n",
    "We just coded up a completely vectorized, feed-forward neural network with backpropagation without using any neural network libraries like Tensorflow, Pytorch or Theano. We then used our network to classify images of handwritten digits, which is an important task in computer vision. \n",
    "\n",
    "Now, let's check out Keras, a neural network library in Python, and build our neural network there as well.\n",
    "\n",
    "We've actually been cheating by training our network and testing our network on the same data set. In order to get an idea of how accurately our model will perform in a real world scenario, we need to divide our data into a training set and a test or validation set. In small data settings, like this one, this is typically done by allocating 80% of the data to the training set and 20% to the test set. However, we're now faced with a problem: our data is logically ordered! This is bad because it will limit our model's exposure certain data types. We can overcome this combining the data into a pandas dataframe and randomly shuffling it with scikit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data2 = pd.DataFrame(data['X'])\n",
    "\n",
    "#add y to the new dataframe\n",
    "data2['y'] = data['y']\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "data2 = shuffle(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reconvert to appropriate dimensions\n",
    "y = np.matrix(data2['y']).T\n",
    "X = data2.iloc[:, 0:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can once again use scikit learn to create our one-hot encoded vectors and additionally we can use it to split our data up into 80% for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 400), (1000, 400), (4000, 10), (1000, 10))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_onehot = encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build our network! We'll use Sequential to indicate that the layers of our model are linearly ordered. We then use Dense to indicate that each unit or neuron is connected to each unit or neuron in the next layer.\n",
    "\n",
    "We will add one hidden layer with 25 units or neurons where we will once again use the sigmoid activation function. The input shape the hidden layer matches the number of features in our model, in this case 400 (20x20 pixel images). We don't need to add the bias term, because Keras does that for us automatically. Our output layer will also use the sigmoid activation function, but with only 10 neurons corresponding to the 10 (0-9) possible values we're trying to predict.\n",
    "\n",
    "To be consistent with Andrew Ng, I am using the sigmoid activation function, but there are other activation functions availble, even more useful than our beloved sigmoid function. Check them out and play with them: https://keras.io/activations/\n",
    "\n",
    "Note that, in this case, the final layer should use either softmax or sigmoid activation in order to yield a probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import regularizers\n",
    " \n",
    "model = Sequential()\n",
    "model.add(Dense(25, activation='sigmoid', input_shape=(400,)))\n",
    "model.add(Dense(10, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll configure the model. My choices for the optimizer and cost function are again to be consistent with Andrew Ng, but they are essentially arbitrary, similar to the optimization function chosen above. Feel free to play around with them and check out how the results change!\n",
    "For more info see:\n",
    "\n",
    "\n",
    "https://keras.io/losses/   \n",
    "https://keras.io/optimizers/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch size is the number of training samples we forward propogate into the network at a time. Epochs just mean the number of times the network sees the data (iterations).\n",
    "\n",
    "In general, larger batch sizes result in faster progress in training, but don't always converge as fast. Smaller batch sizes train slower, but can converge faster. Models typically improve with more epochs of training, to a point. They'll start to plateau in accuracy as they converge.\n",
    "\n",
    "We'll save our model as history so we can use the values later for drawing plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "4000/4000 [==============================] - 0s 124us/step - loss: 2.3258 - acc: 0.0880 - val_loss: 2.3140 - val_acc: 0.0990\n",
      "Epoch 2/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 2.3144 - acc: 0.0905 - val_loss: 2.3041 - val_acc: 0.1020\n",
      "Epoch 3/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 2.3047 - acc: 0.0938 - val_loss: 2.2957 - val_acc: 0.1030\n",
      "Epoch 4/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 2.2964 - acc: 0.0980 - val_loss: 2.2882 - val_acc: 0.1100\n",
      "Epoch 5/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 2.2887 - acc: 0.1028 - val_loss: 2.2813 - val_acc: 0.1130\n",
      "Epoch 6/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 2.2815 - acc: 0.1137 - val_loss: 2.2747 - val_acc: 0.1280\n",
      "Epoch 7/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 2.2747 - acc: 0.1275 - val_loss: 2.2685 - val_acc: 0.1450\n",
      "Epoch 8/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 2.2681 - acc: 0.1417 - val_loss: 2.2624 - val_acc: 0.1610\n",
      "Epoch 9/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 2.2616 - acc: 0.1635 - val_loss: 2.2562 - val_acc: 0.1740\n",
      "Epoch 10/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 2.2551 - acc: 0.1732 - val_loss: 2.2500 - val_acc: 0.1900\n",
      "Epoch 11/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 2.2486 - acc: 0.1845 - val_loss: 2.2438 - val_acc: 0.2060\n",
      "Epoch 12/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 2.2421 - acc: 0.2065 - val_loss: 2.2375 - val_acc: 0.2290\n",
      "Epoch 13/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 2.2354 - acc: 0.2265 - val_loss: 2.2310 - val_acc: 0.2560\n",
      "Epoch 14/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 2.2285 - acc: 0.2527 - val_loss: 2.2243 - val_acc: 0.2820\n",
      "Epoch 15/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 2.2215 - acc: 0.2760 - val_loss: 2.2174 - val_acc: 0.3080\n",
      "Epoch 16/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 2.2143 - acc: 0.3090 - val_loss: 2.2103 - val_acc: 0.3500\n",
      "Epoch 17/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 2.2068 - acc: 0.3473 - val_loss: 2.2028 - val_acc: 0.4010\n",
      "Epoch 18/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 2.1991 - acc: 0.4095 - val_loss: 2.1951 - val_acc: 0.4430\n",
      "Epoch 19/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 2.1911 - acc: 0.4400 - val_loss: 2.1870 - val_acc: 0.4890\n",
      "Epoch 20/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 2.1827 - acc: 0.4823 - val_loss: 2.1786 - val_acc: 0.5210\n",
      "Epoch 21/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 2.1741 - acc: 0.5228 - val_loss: 2.1699 - val_acc: 0.5650\n",
      "Epoch 22/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 2.1650 - acc: 0.5653 - val_loss: 2.1607 - val_acc: 0.5910\n",
      "Epoch 23/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 2.1556 - acc: 0.5952 - val_loss: 2.1511 - val_acc: 0.6190\n",
      "Epoch 24/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 2.1457 - acc: 0.6245 - val_loss: 2.1411 - val_acc: 0.6440\n",
      "Epoch 25/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 2.1354 - acc: 0.6405 - val_loss: 2.1306 - val_acc: 0.6580\n",
      "Epoch 26/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 2.1247 - acc: 0.6545 - val_loss: 2.1196 - val_acc: 0.6740\n",
      "Epoch 27/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 2.1134 - acc: 0.6733 - val_loss: 2.1081 - val_acc: 0.6870\n",
      "Epoch 28/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 2.1017 - acc: 0.6855 - val_loss: 2.0961 - val_acc: 0.6990\n",
      "Epoch 29/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 2.0894 - acc: 0.7013 - val_loss: 2.0836 - val_acc: 0.7100\n",
      "Epoch 30/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 2.0766 - acc: 0.7117 - val_loss: 2.0705 - val_acc: 0.7240\n",
      "Epoch 31/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 2.0633 - acc: 0.7173 - val_loss: 2.0567 - val_acc: 0.7290\n",
      "Epoch 32/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 2.0493 - acc: 0.7280 - val_loss: 2.0424 - val_acc: 0.7370\n",
      "Epoch 33/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 2.0348 - acc: 0.7378 - val_loss: 2.0276 - val_acc: 0.7410\n",
      "Epoch 34/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 2.0196 - acc: 0.7392 - val_loss: 2.0121 - val_acc: 0.7420\n",
      "Epoch 35/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 2.0039 - acc: 0.7435 - val_loss: 1.9960 - val_acc: 0.7490\n",
      "Epoch 36/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 1.9875 - acc: 0.7485 - val_loss: 1.9792 - val_acc: 0.7510\n",
      "Epoch 37/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 1.9705 - acc: 0.7520 - val_loss: 1.9619 - val_acc: 0.7600\n",
      "Epoch 38/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 1.9529 - acc: 0.7518 - val_loss: 1.9440 - val_acc: 0.7610\n",
      "Epoch 39/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 1.9348 - acc: 0.7535 - val_loss: 1.9255 - val_acc: 0.7650\n",
      "Epoch 40/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 1.9161 - acc: 0.7582 - val_loss: 1.9064 - val_acc: 0.7660\n",
      "Epoch 41/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 1.8969 - acc: 0.7572 - val_loss: 1.8869 - val_acc: 0.7700\n",
      "Epoch 42/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 1.8772 - acc: 0.7585 - val_loss: 1.8669 - val_acc: 0.7700\n",
      "Epoch 43/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 1.8571 - acc: 0.7595 - val_loss: 1.8466 - val_acc: 0.7740\n",
      "Epoch 44/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 1.8365 - acc: 0.7592 - val_loss: 1.8259 - val_acc: 0.7700\n",
      "Epoch 45/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 1.8155 - acc: 0.7615 - val_loss: 1.8048 - val_acc: 0.7710\n",
      "Epoch 46/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 1.7943 - acc: 0.7635 - val_loss: 1.7834 - val_acc: 0.7660\n",
      "Epoch 47/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 1.7729 - acc: 0.7597 - val_loss: 1.7618 - val_acc: 0.7690\n",
      "Epoch 48/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 1.7511 - acc: 0.7645 - val_loss: 1.7400 - val_acc: 0.7660\n",
      "Epoch 49/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 1.7292 - acc: 0.7620 - val_loss: 1.7180 - val_acc: 0.7650\n",
      "Epoch 50/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 1.7072 - acc: 0.7612 - val_loss: 1.6959 - val_acc: 0.7670\n",
      "Epoch 51/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 1.6852 - acc: 0.7630 - val_loss: 1.6739 - val_acc: 0.7690\n",
      "Epoch 52/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 1.6632 - acc: 0.7620 - val_loss: 1.6518 - val_acc: 0.7710\n",
      "Epoch 53/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 1.6412 - acc: 0.7605 - val_loss: 1.6299 - val_acc: 0.7770\n",
      "Epoch 54/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 1.6192 - acc: 0.7620 - val_loss: 1.6081 - val_acc: 0.7770\n",
      "Epoch 55/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 1.5973 - acc: 0.7642 - val_loss: 1.5864 - val_acc: 0.7750\n",
      "Epoch 56/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 1.5758 - acc: 0.7657 - val_loss: 1.5650 - val_acc: 0.7740\n",
      "Epoch 57/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 1.5542 - acc: 0.7685 - val_loss: 1.5434 - val_acc: 0.7750\n",
      "Epoch 58/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 1.5329 - acc: 0.7692 - val_loss: 1.5222 - val_acc: 0.7770\n",
      "Epoch 59/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 1.5119 - acc: 0.7678 - val_loss: 1.5014 - val_acc: 0.7790\n",
      "Epoch 60/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 23us/step - loss: 1.4911 - acc: 0.7720 - val_loss: 1.4808 - val_acc: 0.7790\n",
      "Epoch 61/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 1.4705 - acc: 0.7747 - val_loss: 1.4603 - val_acc: 0.7790\n",
      "Epoch 62/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 1.4504 - acc: 0.7752 - val_loss: 1.4402 - val_acc: 0.7790\n",
      "Epoch 63/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 1.4305 - acc: 0.7755 - val_loss: 1.4208 - val_acc: 0.7790\n",
      "Epoch 64/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 1.4110 - acc: 0.7785 - val_loss: 1.4013 - val_acc: 0.7800\n",
      "Epoch 65/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 1.3917 - acc: 0.7785 - val_loss: 1.3822 - val_acc: 0.7800\n",
      "Epoch 66/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 1.3727 - acc: 0.7785 - val_loss: 1.3634 - val_acc: 0.7810\n",
      "Epoch 67/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 1.3542 - acc: 0.7758 - val_loss: 1.3449 - val_acc: 0.7820\n",
      "Epoch 68/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 1.3360 - acc: 0.7833 - val_loss: 1.3269 - val_acc: 0.7800\n",
      "Epoch 69/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 1.3182 - acc: 0.7837 - val_loss: 1.3093 - val_acc: 0.7810\n",
      "Epoch 70/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 1.3007 - acc: 0.7865 - val_loss: 1.2919 - val_acc: 0.7810\n",
      "Epoch 71/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 1.2835 - acc: 0.7860 - val_loss: 1.2750 - val_acc: 0.7840\n",
      "Epoch 72/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 1.2667 - acc: 0.7890 - val_loss: 1.2584 - val_acc: 0.7830\n",
      "Epoch 73/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 1.2503 - acc: 0.7895 - val_loss: 1.2422 - val_acc: 0.7860\n",
      "Epoch 74/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 1.2343 - acc: 0.7930 - val_loss: 1.2260 - val_acc: 0.7900\n",
      "Epoch 75/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 1.2185 - acc: 0.7940 - val_loss: 1.2103 - val_acc: 0.7920\n",
      "Epoch 76/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 1.2032 - acc: 0.7933 - val_loss: 1.1952 - val_acc: 0.7940\n",
      "Epoch 77/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 1.1881 - acc: 0.7950 - val_loss: 1.1803 - val_acc: 0.7960\n",
      "Epoch 78/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 1.1735 - acc: 0.7970 - val_loss: 1.1657 - val_acc: 0.8010\n",
      "Epoch 79/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 1.1591 - acc: 0.7980 - val_loss: 1.1515 - val_acc: 0.8040\n",
      "Epoch 80/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 1.1451 - acc: 0.7970 - val_loss: 1.1374 - val_acc: 0.8030\n",
      "Epoch 81/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 1.1314 - acc: 0.7990 - val_loss: 1.1238 - val_acc: 0.8040\n",
      "Epoch 82/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 1.1180 - acc: 0.8013 - val_loss: 1.1104 - val_acc: 0.8060\n",
      "Epoch 83/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 1.1048 - acc: 0.8017 - val_loss: 1.0975 - val_acc: 0.8070\n",
      "Epoch 84/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 1.0921 - acc: 0.8028 - val_loss: 1.0845 - val_acc: 0.8110\n",
      "Epoch 85/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 1.0796 - acc: 0.8040 - val_loss: 1.0722 - val_acc: 0.8160\n",
      "Epoch 86/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 1.0673 - acc: 0.8050 - val_loss: 1.0600 - val_acc: 0.8190\n",
      "Epoch 87/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 1.0554 - acc: 0.8060 - val_loss: 1.0481 - val_acc: 0.8200\n",
      "Epoch 88/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 1.0436 - acc: 0.8070 - val_loss: 1.0365 - val_acc: 0.8200\n",
      "Epoch 89/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 1.0322 - acc: 0.8088 - val_loss: 1.0250 - val_acc: 0.8200\n",
      "Epoch 90/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 1.0210 - acc: 0.8113 - val_loss: 1.0138 - val_acc: 0.8220\n",
      "Epoch 91/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 1.0101 - acc: 0.8115 - val_loss: 1.0028 - val_acc: 0.8220\n",
      "Epoch 92/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.9993 - acc: 0.8120 - val_loss: 0.9922 - val_acc: 0.8210\n",
      "Epoch 93/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.9889 - acc: 0.8145 - val_loss: 0.9817 - val_acc: 0.8230\n",
      "Epoch 94/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.9787 - acc: 0.8135 - val_loss: 0.9716 - val_acc: 0.8250\n",
      "Epoch 95/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.9686 - acc: 0.8165 - val_loss: 0.9615 - val_acc: 0.8260\n",
      "Epoch 96/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.9588 - acc: 0.8185 - val_loss: 0.9516 - val_acc: 0.8260\n",
      "Epoch 97/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.9492 - acc: 0.8200 - val_loss: 0.9420 - val_acc: 0.8270\n",
      "Epoch 98/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.9398 - acc: 0.8207 - val_loss: 0.9325 - val_acc: 0.8250\n",
      "Epoch 99/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.9306 - acc: 0.8213 - val_loss: 0.9233 - val_acc: 0.8270\n",
      "Epoch 100/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.9216 - acc: 0.8220 - val_loss: 0.9143 - val_acc: 0.8290\n",
      "Epoch 101/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.9127 - acc: 0.8255 - val_loss: 0.9057 - val_acc: 0.8300\n",
      "Epoch 102/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.9041 - acc: 0.8273 - val_loss: 0.8970 - val_acc: 0.8310\n",
      "Epoch 103/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.8956 - acc: 0.8275 - val_loss: 0.8885 - val_acc: 0.8310\n",
      "Epoch 104/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.8873 - acc: 0.8275 - val_loss: 0.8801 - val_acc: 0.8350\n",
      "Epoch 105/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.8791 - acc: 0.8290 - val_loss: 0.8718 - val_acc: 0.8380\n",
      "Epoch 106/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.8711 - acc: 0.8293 - val_loss: 0.8639 - val_acc: 0.8390\n",
      "Epoch 107/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.8633 - acc: 0.8310 - val_loss: 0.8560 - val_acc: 0.8390\n",
      "Epoch 108/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.8556 - acc: 0.8325 - val_loss: 0.8485 - val_acc: 0.8390\n",
      "Epoch 109/1000\n",
      "4000/4000 [==============================] - 0s 33us/step - loss: 0.8481 - acc: 0.8327 - val_loss: 0.8410 - val_acc: 0.8410\n",
      "Epoch 110/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.8407 - acc: 0.8340 - val_loss: 0.8337 - val_acc: 0.8430\n",
      "Epoch 111/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.8334 - acc: 0.8350 - val_loss: 0.8264 - val_acc: 0.8420\n",
      "Epoch 112/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.8263 - acc: 0.8353 - val_loss: 0.8193 - val_acc: 0.8440\n",
      "Epoch 113/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.8193 - acc: 0.8363 - val_loss: 0.8123 - val_acc: 0.8440\n",
      "Epoch 114/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.8125 - acc: 0.8375 - val_loss: 0.8055 - val_acc: 0.8470\n",
      "Epoch 115/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.8057 - acc: 0.8395 - val_loss: 0.7987 - val_acc: 0.8480\n",
      "Epoch 116/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.7991 - acc: 0.8390 - val_loss: 0.7923 - val_acc: 0.8450\n",
      "Epoch 117/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.7927 - acc: 0.8387 - val_loss: 0.7858 - val_acc: 0.8470\n",
      "Epoch 118/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.7863 - acc: 0.8393 - val_loss: 0.7794 - val_acc: 0.8480\n",
      "Epoch 119/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.7801 - acc: 0.8405 - val_loss: 0.7732 - val_acc: 0.8510\n",
      "Epoch 120/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.7739 - acc: 0.8420 - val_loss: 0.7671 - val_acc: 0.8500\n",
      "Epoch 121/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.7679 - acc: 0.8415 - val_loss: 0.7612 - val_acc: 0.8530\n",
      "Epoch 122/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.7620 - acc: 0.8435 - val_loss: 0.7552 - val_acc: 0.8520\n",
      "Epoch 123/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.7562 - acc: 0.8440 - val_loss: 0.7494 - val_acc: 0.8530\n",
      "Epoch 124/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.7505 - acc: 0.8445 - val_loss: 0.7437 - val_acc: 0.8520\n",
      "Epoch 125/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.7449 - acc: 0.8452 - val_loss: 0.7381 - val_acc: 0.8520\n",
      "Epoch 126/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.7393 - acc: 0.8465 - val_loss: 0.7326 - val_acc: 0.8530\n",
      "Epoch 127/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.7339 - acc: 0.8470 - val_loss: 0.7272 - val_acc: 0.8520\n",
      "Epoch 128/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.7285 - acc: 0.8482 - val_loss: 0.7221 - val_acc: 0.8550\n",
      "Epoch 129/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.7234 - acc: 0.8490 - val_loss: 0.7169 - val_acc: 0.8550\n",
      "Epoch 130/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.7182 - acc: 0.8498 - val_loss: 0.7118 - val_acc: 0.8550\n",
      "Epoch 131/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.7131 - acc: 0.8505 - val_loss: 0.7067 - val_acc: 0.8540\n",
      "Epoch 132/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.7082 - acc: 0.8512 - val_loss: 0.7018 - val_acc: 0.8540\n",
      "Epoch 133/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.7032 - acc: 0.8525 - val_loss: 0.6970 - val_acc: 0.8560\n",
      "Epoch 134/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.6984 - acc: 0.8533 - val_loss: 0.6923 - val_acc: 0.8580\n",
      "Epoch 135/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.6937 - acc: 0.8538 - val_loss: 0.6876 - val_acc: 0.8570\n",
      "Epoch 136/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.6890 - acc: 0.8530 - val_loss: 0.6830 - val_acc: 0.8570\n",
      "Epoch 137/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.6845 - acc: 0.8552 - val_loss: 0.6784 - val_acc: 0.8580\n",
      "Epoch 138/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.6800 - acc: 0.8555 - val_loss: 0.6741 - val_acc: 0.8610\n",
      "Epoch 139/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.6755 - acc: 0.8563 - val_loss: 0.6697 - val_acc: 0.8620\n",
      "Epoch 140/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.6712 - acc: 0.8568 - val_loss: 0.6654 - val_acc: 0.8620\n",
      "Epoch 141/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.6669 - acc: 0.8580 - val_loss: 0.6613 - val_acc: 0.8630\n",
      "Epoch 142/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.6627 - acc: 0.8593 - val_loss: 0.6571 - val_acc: 0.8630\n",
      "Epoch 143/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.6585 - acc: 0.8590 - val_loss: 0.6530 - val_acc: 0.8660\n",
      "Epoch 144/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.6544 - acc: 0.8605 - val_loss: 0.6490 - val_acc: 0.8660\n",
      "Epoch 145/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.6504 - acc: 0.8608 - val_loss: 0.6451 - val_acc: 0.8670\n",
      "Epoch 146/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.6464 - acc: 0.8625 - val_loss: 0.6411 - val_acc: 0.8680\n",
      "Epoch 147/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.6425 - acc: 0.8620 - val_loss: 0.6373 - val_acc: 0.8680\n",
      "Epoch 148/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.6387 - acc: 0.8628 - val_loss: 0.6336 - val_acc: 0.8680\n",
      "Epoch 149/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.6348 - acc: 0.8650 - val_loss: 0.6300 - val_acc: 0.8690\n",
      "Epoch 150/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.6311 - acc: 0.8658 - val_loss: 0.6264 - val_acc: 0.8690\n",
      "Epoch 151/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.6274 - acc: 0.8660 - val_loss: 0.6227 - val_acc: 0.8690\n",
      "Epoch 152/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.6238 - acc: 0.8662 - val_loss: 0.6193 - val_acc: 0.8700\n",
      "Epoch 153/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.6203 - acc: 0.8662 - val_loss: 0.6158 - val_acc: 0.8700\n",
      "Epoch 154/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.6167 - acc: 0.8680 - val_loss: 0.6125 - val_acc: 0.8720\n",
      "Epoch 155/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.6133 - acc: 0.8680 - val_loss: 0.6091 - val_acc: 0.8720\n",
      "Epoch 156/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.6099 - acc: 0.8692 - val_loss: 0.6058 - val_acc: 0.8740\n",
      "Epoch 157/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.6065 - acc: 0.8695 - val_loss: 0.6026 - val_acc: 0.8740\n",
      "Epoch 158/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.6032 - acc: 0.8708 - val_loss: 0.5995 - val_acc: 0.8750\n",
      "Epoch 159/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.5999 - acc: 0.8720 - val_loss: 0.5963 - val_acc: 0.8760\n",
      "Epoch 160/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.5967 - acc: 0.8725 - val_loss: 0.5931 - val_acc: 0.8760\n",
      "Epoch 161/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.5935 - acc: 0.8732 - val_loss: 0.5901 - val_acc: 0.8760\n",
      "Epoch 162/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.5904 - acc: 0.8720 - val_loss: 0.5871 - val_acc: 0.8770\n",
      "Epoch 163/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.5873 - acc: 0.8732 - val_loss: 0.5841 - val_acc: 0.8770\n",
      "Epoch 164/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.5842 - acc: 0.8735 - val_loss: 0.5812 - val_acc: 0.8780\n",
      "Epoch 165/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.5812 - acc: 0.8745 - val_loss: 0.5783 - val_acc: 0.8770\n",
      "Epoch 166/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.5782 - acc: 0.8748 - val_loss: 0.5756 - val_acc: 0.8770\n",
      "Epoch 167/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.5753 - acc: 0.8757 - val_loss: 0.5728 - val_acc: 0.8770\n",
      "Epoch 168/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.5724 - acc: 0.8757 - val_loss: 0.5701 - val_acc: 0.8760\n",
      "Epoch 169/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.5696 - acc: 0.8768 - val_loss: 0.5674 - val_acc: 0.8760\n",
      "Epoch 170/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.5668 - acc: 0.8790 - val_loss: 0.5646 - val_acc: 0.8770\n",
      "Epoch 171/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.5640 - acc: 0.8790 - val_loss: 0.5620 - val_acc: 0.8770\n",
      "Epoch 172/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.5612 - acc: 0.8798 - val_loss: 0.5594 - val_acc: 0.8780\n",
      "Epoch 173/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.5585 - acc: 0.8780 - val_loss: 0.5568 - val_acc: 0.8790\n",
      "Epoch 174/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.5559 - acc: 0.8792 - val_loss: 0.5543 - val_acc: 0.8790\n",
      "Epoch 175/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.5533 - acc: 0.8808 - val_loss: 0.5517 - val_acc: 0.8790\n",
      "Epoch 176/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.5507 - acc: 0.8810 - val_loss: 0.5493 - val_acc: 0.8780\n",
      "Epoch 177/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.5481 - acc: 0.8812 - val_loss: 0.5470 - val_acc: 0.8790\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.5456 - acc: 0.8815 - val_loss: 0.5447 - val_acc: 0.8800\n",
      "Epoch 179/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.5431 - acc: 0.8812 - val_loss: 0.5423 - val_acc: 0.8800\n",
      "Epoch 180/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.5406 - acc: 0.8822 - val_loss: 0.5400 - val_acc: 0.8800\n",
      "Epoch 181/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.5382 - acc: 0.8817 - val_loss: 0.5377 - val_acc: 0.8800\n",
      "Epoch 182/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.5357 - acc: 0.8838 - val_loss: 0.5355 - val_acc: 0.8820\n",
      "Epoch 183/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.5334 - acc: 0.8822 - val_loss: 0.5333 - val_acc: 0.8820\n",
      "Epoch 184/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.5311 - acc: 0.8832 - val_loss: 0.5311 - val_acc: 0.8820\n",
      "Epoch 185/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.5288 - acc: 0.8847 - val_loss: 0.5288 - val_acc: 0.8830\n",
      "Epoch 186/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.5265 - acc: 0.8852 - val_loss: 0.5267 - val_acc: 0.8830\n",
      "Epoch 187/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.5242 - acc: 0.8850 - val_loss: 0.5247 - val_acc: 0.8830\n",
      "Epoch 188/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.5219 - acc: 0.8847 - val_loss: 0.5225 - val_acc: 0.8830\n",
      "Epoch 189/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.5197 - acc: 0.8858 - val_loss: 0.5205 - val_acc: 0.8840\n",
      "Epoch 190/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.5176 - acc: 0.8850 - val_loss: 0.5185 - val_acc: 0.8840\n",
      "Epoch 191/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.5155 - acc: 0.8868 - val_loss: 0.5164 - val_acc: 0.8840\n",
      "Epoch 192/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.5133 - acc: 0.8860 - val_loss: 0.5145 - val_acc: 0.8840\n",
      "Epoch 193/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.5112 - acc: 0.8875 - val_loss: 0.5125 - val_acc: 0.8850\n",
      "Epoch 194/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.5091 - acc: 0.8875 - val_loss: 0.5107 - val_acc: 0.8850\n",
      "Epoch 195/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.5071 - acc: 0.8877 - val_loss: 0.5087 - val_acc: 0.8850\n",
      "Epoch 196/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.5051 - acc: 0.8880 - val_loss: 0.5068 - val_acc: 0.8850\n",
      "Epoch 197/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.5030 - acc: 0.8885 - val_loss: 0.5050 - val_acc: 0.8850\n",
      "Epoch 198/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.5011 - acc: 0.8895 - val_loss: 0.5032 - val_acc: 0.8850\n",
      "Epoch 199/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4991 - acc: 0.8890 - val_loss: 0.5013 - val_acc: 0.8850\n",
      "Epoch 200/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.4972 - acc: 0.8902 - val_loss: 0.4996 - val_acc: 0.8860\n",
      "Epoch 201/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4952 - acc: 0.8900 - val_loss: 0.4979 - val_acc: 0.8860\n",
      "Epoch 202/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4933 - acc: 0.8902 - val_loss: 0.4961 - val_acc: 0.8860\n",
      "Epoch 203/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4914 - acc: 0.8910 - val_loss: 0.4944 - val_acc: 0.8860\n",
      "Epoch 204/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.4896 - acc: 0.8907 - val_loss: 0.4926 - val_acc: 0.8860\n",
      "Epoch 205/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.4877 - acc: 0.8907 - val_loss: 0.4910 - val_acc: 0.8860\n",
      "Epoch 206/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.4860 - acc: 0.8915 - val_loss: 0.4894 - val_acc: 0.8860\n",
      "Epoch 207/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4841 - acc: 0.8918 - val_loss: 0.4878 - val_acc: 0.8860\n",
      "Epoch 208/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.4824 - acc: 0.8930 - val_loss: 0.4861 - val_acc: 0.8860\n",
      "Epoch 209/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.4806 - acc: 0.8930 - val_loss: 0.4846 - val_acc: 0.8860\n",
      "Epoch 210/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4789 - acc: 0.8945 - val_loss: 0.4829 - val_acc: 0.8860\n",
      "Epoch 211/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.4772 - acc: 0.8940 - val_loss: 0.4815 - val_acc: 0.8860\n",
      "Epoch 212/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.4754 - acc: 0.8945 - val_loss: 0.4800 - val_acc: 0.8860\n",
      "Epoch 213/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4738 - acc: 0.8952 - val_loss: 0.4782 - val_acc: 0.8870\n",
      "Epoch 214/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.4721 - acc: 0.8958 - val_loss: 0.4767 - val_acc: 0.8880\n",
      "Epoch 215/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4705 - acc: 0.8962 - val_loss: 0.4753 - val_acc: 0.8880\n",
      "Epoch 216/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.4688 - acc: 0.8972 - val_loss: 0.4738 - val_acc: 0.8880\n",
      "Epoch 217/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.4672 - acc: 0.8960 - val_loss: 0.4724 - val_acc: 0.8890\n",
      "Epoch 218/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.4656 - acc: 0.8967 - val_loss: 0.4710 - val_acc: 0.8890\n",
      "Epoch 219/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.4641 - acc: 0.8975 - val_loss: 0.4695 - val_acc: 0.8890\n",
      "Epoch 220/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.4625 - acc: 0.8972 - val_loss: 0.4682 - val_acc: 0.8910\n",
      "Epoch 221/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4609 - acc: 0.8975 - val_loss: 0.4668 - val_acc: 0.8910\n",
      "Epoch 222/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.4594 - acc: 0.8980 - val_loss: 0.4655 - val_acc: 0.8920\n",
      "Epoch 223/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.4579 - acc: 0.8982 - val_loss: 0.4641 - val_acc: 0.8910\n",
      "Epoch 224/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4564 - acc: 0.8995 - val_loss: 0.4626 - val_acc: 0.8910\n",
      "Epoch 225/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4549 - acc: 0.8985 - val_loss: 0.4613 - val_acc: 0.8920\n",
      "Epoch 226/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4534 - acc: 0.8992 - val_loss: 0.4600 - val_acc: 0.8920\n",
      "Epoch 227/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.4520 - acc: 0.8990 - val_loss: 0.4588 - val_acc: 0.8930\n",
      "Epoch 228/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4505 - acc: 0.8988 - val_loss: 0.4575 - val_acc: 0.8950\n",
      "Epoch 229/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.4491 - acc: 0.9005 - val_loss: 0.4562 - val_acc: 0.8960\n",
      "Epoch 230/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.4476 - acc: 0.9002 - val_loss: 0.4549 - val_acc: 0.8960\n",
      "Epoch 231/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.4462 - acc: 0.9010 - val_loss: 0.4536 - val_acc: 0.8940\n",
      "Epoch 232/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.4448 - acc: 0.8997 - val_loss: 0.4524 - val_acc: 0.8950\n",
      "Epoch 233/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.4435 - acc: 0.9012 - val_loss: 0.4512 - val_acc: 0.8950\n",
      "Epoch 234/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.4421 - acc: 0.9018 - val_loss: 0.4500 - val_acc: 0.8950\n",
      "Epoch 235/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4408 - acc: 0.9018 - val_loss: 0.4488 - val_acc: 0.8950\n",
      "Epoch 236/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4394 - acc: 0.9010 - val_loss: 0.4477 - val_acc: 0.8950\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4381 - acc: 0.9015 - val_loss: 0.4465 - val_acc: 0.8950\n",
      "Epoch 238/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.4367 - acc: 0.9015 - val_loss: 0.4453 - val_acc: 0.8970\n",
      "Epoch 239/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4355 - acc: 0.9022 - val_loss: 0.4442 - val_acc: 0.8950\n",
      "Epoch 240/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4342 - acc: 0.9027 - val_loss: 0.4431 - val_acc: 0.8960\n",
      "Epoch 241/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4329 - acc: 0.9030 - val_loss: 0.4421 - val_acc: 0.8960\n",
      "Epoch 242/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4317 - acc: 0.9035 - val_loss: 0.4409 - val_acc: 0.8970\n",
      "Epoch 243/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4304 - acc: 0.9030 - val_loss: 0.4398 - val_acc: 0.8980\n",
      "Epoch 244/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.4291 - acc: 0.9022 - val_loss: 0.4388 - val_acc: 0.8980\n",
      "Epoch 245/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.4279 - acc: 0.9032 - val_loss: 0.4377 - val_acc: 0.8980\n",
      "Epoch 246/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.4267 - acc: 0.9032 - val_loss: 0.4366 - val_acc: 0.8980\n",
      "Epoch 247/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4255 - acc: 0.9032 - val_loss: 0.4355 - val_acc: 0.8990\n",
      "Epoch 248/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4243 - acc: 0.9035 - val_loss: 0.4345 - val_acc: 0.8990\n",
      "Epoch 249/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.4231 - acc: 0.9040 - val_loss: 0.4334 - val_acc: 0.8990\n",
      "Epoch 250/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.4219 - acc: 0.9035 - val_loss: 0.4324 - val_acc: 0.8990\n",
      "Epoch 251/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4207 - acc: 0.9030 - val_loss: 0.4314 - val_acc: 0.8990\n",
      "Epoch 252/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.4196 - acc: 0.9040 - val_loss: 0.4304 - val_acc: 0.8990\n",
      "Epoch 253/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.4184 - acc: 0.9035 - val_loss: 0.4293 - val_acc: 0.8990\n",
      "Epoch 254/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4173 - acc: 0.9050 - val_loss: 0.4284 - val_acc: 0.8990\n",
      "Epoch 255/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.4162 - acc: 0.9048 - val_loss: 0.4275 - val_acc: 0.9000\n",
      "Epoch 256/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4150 - acc: 0.9045 - val_loss: 0.4265 - val_acc: 0.9000\n",
      "Epoch 257/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.4139 - acc: 0.9042 - val_loss: 0.4255 - val_acc: 0.8990\n",
      "Epoch 258/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4129 - acc: 0.9050 - val_loss: 0.4245 - val_acc: 0.8990\n",
      "Epoch 259/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4117 - acc: 0.9060 - val_loss: 0.4236 - val_acc: 0.8990\n",
      "Epoch 260/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.4106 - acc: 0.9055 - val_loss: 0.4227 - val_acc: 0.8990\n",
      "Epoch 261/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.4096 - acc: 0.9062 - val_loss: 0.4218 - val_acc: 0.9000\n",
      "Epoch 262/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4085 - acc: 0.9062 - val_loss: 0.4209 - val_acc: 0.9000\n",
      "Epoch 263/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.4074 - acc: 0.9065 - val_loss: 0.4200 - val_acc: 0.9010\n",
      "Epoch 264/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.4063 - acc: 0.9055 - val_loss: 0.4191 - val_acc: 0.9010\n",
      "Epoch 265/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.4053 - acc: 0.9068 - val_loss: 0.4181 - val_acc: 0.9000\n",
      "Epoch 266/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4043 - acc: 0.9073 - val_loss: 0.4172 - val_acc: 0.9000\n",
      "Epoch 267/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.4033 - acc: 0.9073 - val_loss: 0.4163 - val_acc: 0.8990\n",
      "Epoch 268/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.4023 - acc: 0.9070 - val_loss: 0.4155 - val_acc: 0.8990\n",
      "Epoch 269/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.4013 - acc: 0.9073 - val_loss: 0.4146 - val_acc: 0.8990\n",
      "Epoch 270/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4003 - acc: 0.9080 - val_loss: 0.4138 - val_acc: 0.9000\n",
      "Epoch 271/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3993 - acc: 0.9085 - val_loss: 0.4130 - val_acc: 0.9000\n",
      "Epoch 272/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3983 - acc: 0.9087 - val_loss: 0.4122 - val_acc: 0.9000\n",
      "Epoch 273/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3973 - acc: 0.9087 - val_loss: 0.4113 - val_acc: 0.9000\n",
      "Epoch 274/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3964 - acc: 0.9087 - val_loss: 0.4106 - val_acc: 0.9000\n",
      "Epoch 275/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3954 - acc: 0.9093 - val_loss: 0.4097 - val_acc: 0.9010\n",
      "Epoch 276/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3944 - acc: 0.9087 - val_loss: 0.4089 - val_acc: 0.9020\n",
      "Epoch 277/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3935 - acc: 0.9103 - val_loss: 0.4082 - val_acc: 0.9010\n",
      "Epoch 278/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3925 - acc: 0.9103 - val_loss: 0.4074 - val_acc: 0.9020\n",
      "Epoch 279/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3916 - acc: 0.9103 - val_loss: 0.4066 - val_acc: 0.9010\n",
      "Epoch 280/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3907 - acc: 0.9105 - val_loss: 0.4058 - val_acc: 0.9020\n",
      "Epoch 281/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3898 - acc: 0.9113 - val_loss: 0.4051 - val_acc: 0.9010\n",
      "Epoch 282/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3888 - acc: 0.9110 - val_loss: 0.4043 - val_acc: 0.9010\n",
      "Epoch 283/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3880 - acc: 0.9113 - val_loss: 0.4035 - val_acc: 0.9020\n",
      "Epoch 284/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3871 - acc: 0.9117 - val_loss: 0.4029 - val_acc: 0.9010\n",
      "Epoch 285/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3862 - acc: 0.9123 - val_loss: 0.4021 - val_acc: 0.9020\n",
      "Epoch 286/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3853 - acc: 0.9113 - val_loss: 0.4013 - val_acc: 0.9020\n",
      "Epoch 287/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3844 - acc: 0.9120 - val_loss: 0.4006 - val_acc: 0.9020\n",
      "Epoch 288/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3835 - acc: 0.9123 - val_loss: 0.3999 - val_acc: 0.9020\n",
      "Epoch 289/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3827 - acc: 0.9128 - val_loss: 0.3992 - val_acc: 0.9020\n",
      "Epoch 290/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3818 - acc: 0.9125 - val_loss: 0.3984 - val_acc: 0.9030\n",
      "Epoch 291/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3809 - acc: 0.9115 - val_loss: 0.3976 - val_acc: 0.9030\n",
      "Epoch 292/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3801 - acc: 0.9120 - val_loss: 0.3970 - val_acc: 0.9030\n",
      "Epoch 293/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3792 - acc: 0.9130 - val_loss: 0.3963 - val_acc: 0.9020\n",
      "Epoch 294/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3784 - acc: 0.9130 - val_loss: 0.3957 - val_acc: 0.9020\n",
      "Epoch 295/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3776 - acc: 0.9130 - val_loss: 0.3950 - val_acc: 0.9020\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3768 - acc: 0.9130 - val_loss: 0.3943 - val_acc: 0.9020\n",
      "Epoch 297/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3759 - acc: 0.9130 - val_loss: 0.3936 - val_acc: 0.9020\n",
      "Epoch 298/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3751 - acc: 0.9130 - val_loss: 0.3929 - val_acc: 0.9030\n",
      "Epoch 299/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3743 - acc: 0.9137 - val_loss: 0.3922 - val_acc: 0.9030\n",
      "Epoch 300/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3735 - acc: 0.9133 - val_loss: 0.3915 - val_acc: 0.9030\n",
      "Epoch 301/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3728 - acc: 0.9143 - val_loss: 0.3909 - val_acc: 0.9030\n",
      "Epoch 302/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3719 - acc: 0.9145 - val_loss: 0.3903 - val_acc: 0.9030\n",
      "Epoch 303/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3711 - acc: 0.9135 - val_loss: 0.3897 - val_acc: 0.9040\n",
      "Epoch 304/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3703 - acc: 0.9145 - val_loss: 0.3892 - val_acc: 0.9040\n",
      "Epoch 305/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3696 - acc: 0.9140 - val_loss: 0.3885 - val_acc: 0.9030\n",
      "Epoch 306/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3688 - acc: 0.9140 - val_loss: 0.3879 - val_acc: 0.9020\n",
      "Epoch 307/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3681 - acc: 0.9147 - val_loss: 0.3873 - val_acc: 0.9050\n",
      "Epoch 308/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3673 - acc: 0.9145 - val_loss: 0.3866 - val_acc: 0.9060\n",
      "Epoch 309/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3665 - acc: 0.9150 - val_loss: 0.3859 - val_acc: 0.9060\n",
      "Epoch 310/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3658 - acc: 0.9147 - val_loss: 0.3853 - val_acc: 0.9060\n",
      "Epoch 311/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3650 - acc: 0.9153 - val_loss: 0.3848 - val_acc: 0.9070\n",
      "Epoch 312/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3643 - acc: 0.9150 - val_loss: 0.3842 - val_acc: 0.9080\n",
      "Epoch 313/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3635 - acc: 0.9150 - val_loss: 0.3836 - val_acc: 0.9080\n",
      "Epoch 314/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3628 - acc: 0.9153 - val_loss: 0.3830 - val_acc: 0.9090\n",
      "Epoch 315/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3621 - acc: 0.9155 - val_loss: 0.3824 - val_acc: 0.9070\n",
      "Epoch 316/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3613 - acc: 0.9155 - val_loss: 0.3819 - val_acc: 0.9080\n",
      "Epoch 317/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3607 - acc: 0.9150 - val_loss: 0.3814 - val_acc: 0.9070\n",
      "Epoch 318/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3600 - acc: 0.9160 - val_loss: 0.3808 - val_acc: 0.9090\n",
      "Epoch 319/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3593 - acc: 0.9150 - val_loss: 0.3802 - val_acc: 0.9080\n",
      "Epoch 320/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3585 - acc: 0.9167 - val_loss: 0.3797 - val_acc: 0.9090\n",
      "Epoch 321/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3578 - acc: 0.9167 - val_loss: 0.3791 - val_acc: 0.9100\n",
      "Epoch 322/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3571 - acc: 0.9165 - val_loss: 0.3785 - val_acc: 0.9090\n",
      "Epoch 323/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3565 - acc: 0.9160 - val_loss: 0.3780 - val_acc: 0.9100\n",
      "Epoch 324/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3558 - acc: 0.9167 - val_loss: 0.3774 - val_acc: 0.9100\n",
      "Epoch 325/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3551 - acc: 0.9160 - val_loss: 0.3770 - val_acc: 0.9090\n",
      "Epoch 326/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3544 - acc: 0.9170 - val_loss: 0.3764 - val_acc: 0.9100\n",
      "Epoch 327/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3538 - acc: 0.9170 - val_loss: 0.3759 - val_acc: 0.9100\n",
      "Epoch 328/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3531 - acc: 0.9170 - val_loss: 0.3753 - val_acc: 0.9100\n",
      "Epoch 329/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3524 - acc: 0.9175 - val_loss: 0.3747 - val_acc: 0.9100\n",
      "Epoch 330/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3518 - acc: 0.9175 - val_loss: 0.3742 - val_acc: 0.9100\n",
      "Epoch 331/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3511 - acc: 0.9175 - val_loss: 0.3737 - val_acc: 0.9100\n",
      "Epoch 332/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3504 - acc: 0.9173 - val_loss: 0.3731 - val_acc: 0.9100\n",
      "Epoch 333/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3498 - acc: 0.9175 - val_loss: 0.3727 - val_acc: 0.9100\n",
      "Epoch 334/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3491 - acc: 0.9167 - val_loss: 0.3722 - val_acc: 0.9100\n",
      "Epoch 335/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3485 - acc: 0.9173 - val_loss: 0.3717 - val_acc: 0.9100\n",
      "Epoch 336/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3479 - acc: 0.9177 - val_loss: 0.3712 - val_acc: 0.9100\n",
      "Epoch 337/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3472 - acc: 0.9175 - val_loss: 0.3707 - val_acc: 0.9100\n",
      "Epoch 338/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3466 - acc: 0.9183 - val_loss: 0.3702 - val_acc: 0.9100\n",
      "Epoch 339/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3460 - acc: 0.9177 - val_loss: 0.3698 - val_acc: 0.9100\n",
      "Epoch 340/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3454 - acc: 0.9180 - val_loss: 0.3692 - val_acc: 0.9100\n",
      "Epoch 341/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3447 - acc: 0.9183 - val_loss: 0.3687 - val_acc: 0.9100\n",
      "Epoch 342/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3441 - acc: 0.9183 - val_loss: 0.3683 - val_acc: 0.9100\n",
      "Epoch 343/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3435 - acc: 0.9185 - val_loss: 0.3679 - val_acc: 0.9100\n",
      "Epoch 344/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3429 - acc: 0.9183 - val_loss: 0.3674 - val_acc: 0.9110\n",
      "Epoch 345/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3423 - acc: 0.9188 - val_loss: 0.3668 - val_acc: 0.9110\n",
      "Epoch 346/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3417 - acc: 0.9190 - val_loss: 0.3664 - val_acc: 0.9110\n",
      "Epoch 347/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3411 - acc: 0.9188 - val_loss: 0.3660 - val_acc: 0.9120\n",
      "Epoch 348/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3405 - acc: 0.9190 - val_loss: 0.3655 - val_acc: 0.9120\n",
      "Epoch 349/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3399 - acc: 0.9193 - val_loss: 0.3651 - val_acc: 0.9120\n",
      "Epoch 350/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3393 - acc: 0.9190 - val_loss: 0.3645 - val_acc: 0.9120\n",
      "Epoch 351/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3387 - acc: 0.9195 - val_loss: 0.3641 - val_acc: 0.9130\n",
      "Epoch 352/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3382 - acc: 0.9193 - val_loss: 0.3637 - val_acc: 0.9130\n",
      "Epoch 353/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3375 - acc: 0.9197 - val_loss: 0.3633 - val_acc: 0.9130\n",
      "Epoch 354/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3370 - acc: 0.9195 - val_loss: 0.3628 - val_acc: 0.9120\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3364 - acc: 0.9188 - val_loss: 0.3624 - val_acc: 0.9120\n",
      "Epoch 356/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3359 - acc: 0.9195 - val_loss: 0.3619 - val_acc: 0.9120\n",
      "Epoch 357/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3352 - acc: 0.9197 - val_loss: 0.3615 - val_acc: 0.9130\n",
      "Epoch 358/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3347 - acc: 0.9197 - val_loss: 0.3611 - val_acc: 0.9130\n",
      "Epoch 359/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3341 - acc: 0.9193 - val_loss: 0.3607 - val_acc: 0.9130\n",
      "Epoch 360/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3336 - acc: 0.9197 - val_loss: 0.3602 - val_acc: 0.9130\n",
      "Epoch 361/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3330 - acc: 0.9200 - val_loss: 0.3598 - val_acc: 0.9130\n",
      "Epoch 362/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3325 - acc: 0.9200 - val_loss: 0.3594 - val_acc: 0.9140\n",
      "Epoch 363/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3319 - acc: 0.9197 - val_loss: 0.3590 - val_acc: 0.9140\n",
      "Epoch 364/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3314 - acc: 0.9200 - val_loss: 0.3586 - val_acc: 0.9140\n",
      "Epoch 365/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3308 - acc: 0.9197 - val_loss: 0.3582 - val_acc: 0.9140\n",
      "Epoch 366/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3303 - acc: 0.9205 - val_loss: 0.3578 - val_acc: 0.9130\n",
      "Epoch 367/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3297 - acc: 0.9200 - val_loss: 0.3574 - val_acc: 0.9140\n",
      "Epoch 368/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3292 - acc: 0.9197 - val_loss: 0.3569 - val_acc: 0.9140\n",
      "Epoch 369/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3287 - acc: 0.9203 - val_loss: 0.3566 - val_acc: 0.9140\n",
      "Epoch 370/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3281 - acc: 0.9205 - val_loss: 0.3563 - val_acc: 0.9140\n",
      "Epoch 371/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3276 - acc: 0.9205 - val_loss: 0.3559 - val_acc: 0.9140\n",
      "Epoch 372/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3271 - acc: 0.9205 - val_loss: 0.3556 - val_acc: 0.9130\n",
      "Epoch 373/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3266 - acc: 0.9203 - val_loss: 0.3551 - val_acc: 0.9130\n",
      "Epoch 374/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3260 - acc: 0.9207 - val_loss: 0.3547 - val_acc: 0.9130\n",
      "Epoch 375/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3255 - acc: 0.9207 - val_loss: 0.3544 - val_acc: 0.9130\n",
      "Epoch 376/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3250 - acc: 0.9210 - val_loss: 0.3540 - val_acc: 0.9130\n",
      "Epoch 377/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3245 - acc: 0.9205 - val_loss: 0.3536 - val_acc: 0.9130\n",
      "Epoch 378/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3240 - acc: 0.9210 - val_loss: 0.3532 - val_acc: 0.9140\n",
      "Epoch 379/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3235 - acc: 0.9213 - val_loss: 0.3528 - val_acc: 0.9140\n",
      "Epoch 380/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3230 - acc: 0.9215 - val_loss: 0.3524 - val_acc: 0.9140\n",
      "Epoch 381/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3225 - acc: 0.9215 - val_loss: 0.3521 - val_acc: 0.9140\n",
      "Epoch 382/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3220 - acc: 0.9213 - val_loss: 0.3516 - val_acc: 0.9150\n",
      "Epoch 383/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3215 - acc: 0.9218 - val_loss: 0.3513 - val_acc: 0.9140\n",
      "Epoch 384/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3210 - acc: 0.9218 - val_loss: 0.3509 - val_acc: 0.9140\n",
      "Epoch 385/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3205 - acc: 0.9220 - val_loss: 0.3507 - val_acc: 0.9150\n",
      "Epoch 386/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3200 - acc: 0.9220 - val_loss: 0.3502 - val_acc: 0.9150\n",
      "Epoch 387/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3195 - acc: 0.9225 - val_loss: 0.3499 - val_acc: 0.9160\n",
      "Epoch 388/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3190 - acc: 0.9223 - val_loss: 0.3496 - val_acc: 0.9160\n",
      "Epoch 389/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3186 - acc: 0.9220 - val_loss: 0.3493 - val_acc: 0.9160\n",
      "Epoch 390/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3180 - acc: 0.9225 - val_loss: 0.3490 - val_acc: 0.9170\n",
      "Epoch 391/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3176 - acc: 0.9220 - val_loss: 0.3486 - val_acc: 0.9170\n",
      "Epoch 392/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3171 - acc: 0.9220 - val_loss: 0.3483 - val_acc: 0.9170\n",
      "Epoch 393/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3166 - acc: 0.9230 - val_loss: 0.3479 - val_acc: 0.9170\n",
      "Epoch 394/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3162 - acc: 0.9227 - val_loss: 0.3476 - val_acc: 0.9170\n",
      "Epoch 395/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3157 - acc: 0.9223 - val_loss: 0.3472 - val_acc: 0.9170\n",
      "Epoch 396/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3152 - acc: 0.9227 - val_loss: 0.3468 - val_acc: 0.9170\n",
      "Epoch 397/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3148 - acc: 0.9223 - val_loss: 0.3465 - val_acc: 0.9170\n",
      "Epoch 398/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3143 - acc: 0.9220 - val_loss: 0.3461 - val_acc: 0.9170\n",
      "Epoch 399/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3138 - acc: 0.9227 - val_loss: 0.3457 - val_acc: 0.9170\n",
      "Epoch 400/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3134 - acc: 0.9227 - val_loss: 0.3454 - val_acc: 0.9170\n",
      "Epoch 401/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3129 - acc: 0.9225 - val_loss: 0.3452 - val_acc: 0.9170\n",
      "Epoch 402/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3125 - acc: 0.9227 - val_loss: 0.3448 - val_acc: 0.9170\n",
      "Epoch 403/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3120 - acc: 0.9225 - val_loss: 0.3445 - val_acc: 0.9170\n",
      "Epoch 404/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3115 - acc: 0.9230 - val_loss: 0.3442 - val_acc: 0.9170\n",
      "Epoch 405/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3111 - acc: 0.9225 - val_loss: 0.3440 - val_acc: 0.9170\n",
      "Epoch 406/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3106 - acc: 0.9225 - val_loss: 0.3437 - val_acc: 0.9170\n",
      "Epoch 407/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3102 - acc: 0.9230 - val_loss: 0.3434 - val_acc: 0.9170\n",
      "Epoch 408/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3098 - acc: 0.9230 - val_loss: 0.3431 - val_acc: 0.9170\n",
      "Epoch 409/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3093 - acc: 0.9235 - val_loss: 0.3427 - val_acc: 0.9170\n",
      "Epoch 410/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3089 - acc: 0.9233 - val_loss: 0.3424 - val_acc: 0.9170\n",
      "Epoch 411/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3085 - acc: 0.9237 - val_loss: 0.3421 - val_acc: 0.9170\n",
      "Epoch 412/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3080 - acc: 0.9230 - val_loss: 0.3418 - val_acc: 0.9170\n",
      "Epoch 413/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3076 - acc: 0.9240 - val_loss: 0.3415 - val_acc: 0.9180\n",
      "Epoch 414/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3072 - acc: 0.9237 - val_loss: 0.3412 - val_acc: 0.9180\n",
      "Epoch 415/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3067 - acc: 0.9240 - val_loss: 0.3409 - val_acc: 0.9180\n",
      "Epoch 416/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3063 - acc: 0.9248 - val_loss: 0.3406 - val_acc: 0.9180\n",
      "Epoch 417/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3059 - acc: 0.9245 - val_loss: 0.3404 - val_acc: 0.9180\n",
      "Epoch 418/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3055 - acc: 0.9243 - val_loss: 0.3400 - val_acc: 0.9180\n",
      "Epoch 419/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3050 - acc: 0.9237 - val_loss: 0.3398 - val_acc: 0.9180\n",
      "Epoch 420/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3046 - acc: 0.9243 - val_loss: 0.3395 - val_acc: 0.9180\n",
      "Epoch 421/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3042 - acc: 0.9237 - val_loss: 0.3391 - val_acc: 0.9180\n",
      "Epoch 422/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3037 - acc: 0.9240 - val_loss: 0.3388 - val_acc: 0.9180\n",
      "Epoch 423/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3033 - acc: 0.9243 - val_loss: 0.3385 - val_acc: 0.9180\n",
      "Epoch 424/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3029 - acc: 0.9240 - val_loss: 0.3383 - val_acc: 0.9180\n",
      "Epoch 425/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3025 - acc: 0.9248 - val_loss: 0.3380 - val_acc: 0.9180\n",
      "Epoch 426/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3021 - acc: 0.9245 - val_loss: 0.3379 - val_acc: 0.9180\n",
      "Epoch 427/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3017 - acc: 0.9237 - val_loss: 0.3375 - val_acc: 0.9180\n",
      "Epoch 428/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3013 - acc: 0.9245 - val_loss: 0.3372 - val_acc: 0.9180\n",
      "Epoch 429/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3009 - acc: 0.9243 - val_loss: 0.3370 - val_acc: 0.9180\n",
      "Epoch 430/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.3005 - acc: 0.9245 - val_loss: 0.3367 - val_acc: 0.9180\n",
      "Epoch 431/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3001 - acc: 0.9245 - val_loss: 0.3364 - val_acc: 0.9180\n",
      "Epoch 432/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2997 - acc: 0.9250 - val_loss: 0.3360 - val_acc: 0.9180\n",
      "Epoch 433/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.2993 - acc: 0.9245 - val_loss: 0.3357 - val_acc: 0.9180\n",
      "Epoch 434/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2989 - acc: 0.9248 - val_loss: 0.3355 - val_acc: 0.9180\n",
      "Epoch 435/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2985 - acc: 0.9250 - val_loss: 0.3352 - val_acc: 0.9180\n",
      "Epoch 436/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2981 - acc: 0.9245 - val_loss: 0.3349 - val_acc: 0.9180\n",
      "Epoch 437/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2977 - acc: 0.9255 - val_loss: 0.3347 - val_acc: 0.9180\n",
      "Epoch 438/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2973 - acc: 0.9253 - val_loss: 0.3344 - val_acc: 0.9180\n",
      "Epoch 439/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2969 - acc: 0.9253 - val_loss: 0.3341 - val_acc: 0.9180\n",
      "Epoch 440/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2966 - acc: 0.9250 - val_loss: 0.3339 - val_acc: 0.9180\n",
      "Epoch 441/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2961 - acc: 0.9250 - val_loss: 0.3337 - val_acc: 0.9180\n",
      "Epoch 442/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2957 - acc: 0.9255 - val_loss: 0.3334 - val_acc: 0.9180\n",
      "Epoch 443/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2954 - acc: 0.9260 - val_loss: 0.3331 - val_acc: 0.9180\n",
      "Epoch 444/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2950 - acc: 0.9263 - val_loss: 0.3329 - val_acc: 0.9180\n",
      "Epoch 445/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2946 - acc: 0.9257 - val_loss: 0.3327 - val_acc: 0.9180\n",
      "Epoch 446/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2942 - acc: 0.9267 - val_loss: 0.3325 - val_acc: 0.9180\n",
      "Epoch 447/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.2938 - acc: 0.9255 - val_loss: 0.3322 - val_acc: 0.9170\n",
      "Epoch 448/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2935 - acc: 0.9265 - val_loss: 0.3319 - val_acc: 0.9170\n",
      "Epoch 449/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2931 - acc: 0.9265 - val_loss: 0.3316 - val_acc: 0.9170\n",
      "Epoch 450/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2927 - acc: 0.9270 - val_loss: 0.3315 - val_acc: 0.9170\n",
      "Epoch 451/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2923 - acc: 0.9265 - val_loss: 0.3311 - val_acc: 0.9170\n",
      "Epoch 452/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2920 - acc: 0.9270 - val_loss: 0.3309 - val_acc: 0.9170\n",
      "Epoch 453/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2916 - acc: 0.9270 - val_loss: 0.3307 - val_acc: 0.9170\n",
      "Epoch 454/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2912 - acc: 0.9260 - val_loss: 0.3305 - val_acc: 0.9170\n",
      "Epoch 455/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2909 - acc: 0.9270 - val_loss: 0.3302 - val_acc: 0.9170\n",
      "Epoch 456/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2905 - acc: 0.9273 - val_loss: 0.3300 - val_acc: 0.9170\n",
      "Epoch 457/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2901 - acc: 0.9275 - val_loss: 0.3298 - val_acc: 0.9170\n",
      "Epoch 458/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2898 - acc: 0.9273 - val_loss: 0.3295 - val_acc: 0.9170\n",
      "Epoch 459/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2894 - acc: 0.9280 - val_loss: 0.3292 - val_acc: 0.9180\n",
      "Epoch 460/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2890 - acc: 0.9283 - val_loss: 0.3289 - val_acc: 0.9180\n",
      "Epoch 461/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.2887 - acc: 0.9280 - val_loss: 0.3287 - val_acc: 0.9180\n",
      "Epoch 462/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2883 - acc: 0.9280 - val_loss: 0.3285 - val_acc: 0.9180\n",
      "Epoch 463/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2880 - acc: 0.9275 - val_loss: 0.3282 - val_acc: 0.9180\n",
      "Epoch 464/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2876 - acc: 0.9290 - val_loss: 0.3280 - val_acc: 0.9180\n",
      "Epoch 465/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2873 - acc: 0.9285 - val_loss: 0.3278 - val_acc: 0.9170\n",
      "Epoch 466/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2869 - acc: 0.9293 - val_loss: 0.3275 - val_acc: 0.9180\n",
      "Epoch 467/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2865 - acc: 0.9290 - val_loss: 0.3274 - val_acc: 0.9180\n",
      "Epoch 468/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2862 - acc: 0.9290 - val_loss: 0.3272 - val_acc: 0.9180\n",
      "Epoch 469/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2858 - acc: 0.9295 - val_loss: 0.3270 - val_acc: 0.9190\n",
      "Epoch 470/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2855 - acc: 0.9295 - val_loss: 0.3267 - val_acc: 0.9170\n",
      "Epoch 471/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2852 - acc: 0.9295 - val_loss: 0.3265 - val_acc: 0.9180\n",
      "Epoch 472/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2848 - acc: 0.9300 - val_loss: 0.3263 - val_acc: 0.9190\n",
      "Epoch 473/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2845 - acc: 0.9303 - val_loss: 0.3261 - val_acc: 0.9190\n",
      "Epoch 474/1000\n",
      "4000/4000 [==============================] - ETA: 0s - loss: 0.2945 - acc: 0.925 - 0s 22us/step - loss: 0.2841 - acc: 0.9295 - val_loss: 0.3260 - val_acc: 0.9180\n",
      "Epoch 475/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2838 - acc: 0.9297 - val_loss: 0.3257 - val_acc: 0.9180\n",
      "Epoch 476/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.2835 - acc: 0.9303 - val_loss: 0.3254 - val_acc: 0.9190\n",
      "Epoch 477/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2831 - acc: 0.9293 - val_loss: 0.3253 - val_acc: 0.9190\n",
      "Epoch 478/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2828 - acc: 0.9295 - val_loss: 0.3250 - val_acc: 0.9190\n",
      "Epoch 479/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2825 - acc: 0.9295 - val_loss: 0.3248 - val_acc: 0.9190\n",
      "Epoch 480/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2821 - acc: 0.9303 - val_loss: 0.3245 - val_acc: 0.9190\n",
      "Epoch 481/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2818 - acc: 0.9297 - val_loss: 0.3243 - val_acc: 0.9190\n",
      "Epoch 482/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2814 - acc: 0.9305 - val_loss: 0.3241 - val_acc: 0.9190\n",
      "Epoch 483/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2811 - acc: 0.9310 - val_loss: 0.3239 - val_acc: 0.9190\n",
      "Epoch 484/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2808 - acc: 0.9315 - val_loss: 0.3237 - val_acc: 0.9190\n",
      "Epoch 485/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2805 - acc: 0.9305 - val_loss: 0.3235 - val_acc: 0.9190\n",
      "Epoch 486/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2801 - acc: 0.9310 - val_loss: 0.3234 - val_acc: 0.9190\n",
      "Epoch 487/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2798 - acc: 0.9317 - val_loss: 0.3231 - val_acc: 0.9190\n",
      "Epoch 488/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2795 - acc: 0.9313 - val_loss: 0.3229 - val_acc: 0.9190\n",
      "Epoch 489/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.2791 - acc: 0.9315 - val_loss: 0.3227 - val_acc: 0.9190\n",
      "Epoch 490/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2788 - acc: 0.9317 - val_loss: 0.3226 - val_acc: 0.9190\n",
      "Epoch 491/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2785 - acc: 0.9320 - val_loss: 0.3224 - val_acc: 0.9190\n",
      "Epoch 492/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2782 - acc: 0.9317 - val_loss: 0.3221 - val_acc: 0.9190\n",
      "Epoch 493/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2778 - acc: 0.9310 - val_loss: 0.3219 - val_acc: 0.9190\n",
      "Epoch 494/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2775 - acc: 0.9320 - val_loss: 0.3217 - val_acc: 0.9190\n",
      "Epoch 495/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2772 - acc: 0.9315 - val_loss: 0.3215 - val_acc: 0.9190\n",
      "Epoch 496/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2769 - acc: 0.9323 - val_loss: 0.3213 - val_acc: 0.9190\n",
      "Epoch 497/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2766 - acc: 0.9330 - val_loss: 0.3211 - val_acc: 0.9190\n",
      "Epoch 498/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2763 - acc: 0.9320 - val_loss: 0.3209 - val_acc: 0.9190\n",
      "Epoch 499/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2759 - acc: 0.9330 - val_loss: 0.3207 - val_acc: 0.9190\n",
      "Epoch 500/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2756 - acc: 0.9327 - val_loss: 0.3205 - val_acc: 0.9190\n",
      "Epoch 501/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2753 - acc: 0.9335 - val_loss: 0.3203 - val_acc: 0.9190\n",
      "Epoch 502/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2750 - acc: 0.9333 - val_loss: 0.3201 - val_acc: 0.9190\n",
      "Epoch 503/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.2747 - acc: 0.9325 - val_loss: 0.3199 - val_acc: 0.9190\n",
      "Epoch 504/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2744 - acc: 0.9330 - val_loss: 0.3198 - val_acc: 0.9190\n",
      "Epoch 505/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2741 - acc: 0.9343 - val_loss: 0.3196 - val_acc: 0.9190\n",
      "Epoch 506/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2738 - acc: 0.9340 - val_loss: 0.3193 - val_acc: 0.9190\n",
      "Epoch 507/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2734 - acc: 0.9343 - val_loss: 0.3192 - val_acc: 0.9190\n",
      "Epoch 508/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2732 - acc: 0.9343 - val_loss: 0.3191 - val_acc: 0.9190\n",
      "Epoch 509/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2728 - acc: 0.9343 - val_loss: 0.3188 - val_acc: 0.9190\n",
      "Epoch 510/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2725 - acc: 0.9343 - val_loss: 0.3187 - val_acc: 0.9190\n",
      "Epoch 511/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2722 - acc: 0.9347 - val_loss: 0.3185 - val_acc: 0.9180\n",
      "Epoch 512/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2719 - acc: 0.9338 - val_loss: 0.3184 - val_acc: 0.9190\n",
      "Epoch 513/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2716 - acc: 0.9355 - val_loss: 0.3181 - val_acc: 0.9190\n",
      "Epoch 514/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2713 - acc: 0.9353 - val_loss: 0.3179 - val_acc: 0.9180\n",
      "Epoch 515/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2710 - acc: 0.9355 - val_loss: 0.3177 - val_acc: 0.9180\n",
      "Epoch 516/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.2707 - acc: 0.9353 - val_loss: 0.3175 - val_acc: 0.9190\n",
      "Epoch 517/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.2704 - acc: 0.9357 - val_loss: 0.3173 - val_acc: 0.9190\n",
      "Epoch 518/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2701 - acc: 0.9353 - val_loss: 0.3171 - val_acc: 0.9190\n",
      "Epoch 519/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2699 - acc: 0.9350 - val_loss: 0.3170 - val_acc: 0.9190\n",
      "Epoch 520/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2696 - acc: 0.9347 - val_loss: 0.3169 - val_acc: 0.9190\n",
      "Epoch 521/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2693 - acc: 0.9357 - val_loss: 0.3167 - val_acc: 0.9190\n",
      "Epoch 522/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2690 - acc: 0.9353 - val_loss: 0.3167 - val_acc: 0.9180\n",
      "Epoch 523/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2687 - acc: 0.9355 - val_loss: 0.3165 - val_acc: 0.9190\n",
      "Epoch 524/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2684 - acc: 0.9353 - val_loss: 0.3163 - val_acc: 0.9190\n",
      "Epoch 525/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2681 - acc: 0.9353 - val_loss: 0.3160 - val_acc: 0.9190\n",
      "Epoch 526/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2678 - acc: 0.9357 - val_loss: 0.3158 - val_acc: 0.9190\n",
      "Epoch 527/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2675 - acc: 0.9355 - val_loss: 0.3157 - val_acc: 0.9190\n",
      "Epoch 528/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2672 - acc: 0.9355 - val_loss: 0.3154 - val_acc: 0.9190\n",
      "Epoch 529/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2669 - acc: 0.9360 - val_loss: 0.3152 - val_acc: 0.9180\n",
      "Epoch 530/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2666 - acc: 0.9355 - val_loss: 0.3151 - val_acc: 0.9190\n",
      "Epoch 531/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2664 - acc: 0.9363 - val_loss: 0.3151 - val_acc: 0.9190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 532/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2661 - acc: 0.9360 - val_loss: 0.3148 - val_acc: 0.9190\n",
      "Epoch 533/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2658 - acc: 0.9355 - val_loss: 0.3147 - val_acc: 0.9180\n",
      "Epoch 534/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2655 - acc: 0.9357 - val_loss: 0.3144 - val_acc: 0.9190\n",
      "Epoch 535/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2652 - acc: 0.9360 - val_loss: 0.3143 - val_acc: 0.9190\n",
      "Epoch 536/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2650 - acc: 0.9355 - val_loss: 0.3141 - val_acc: 0.9190\n",
      "Epoch 537/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2647 - acc: 0.9360 - val_loss: 0.3140 - val_acc: 0.9190\n",
      "Epoch 538/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2644 - acc: 0.9365 - val_loss: 0.3138 - val_acc: 0.9190\n",
      "Epoch 539/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2641 - acc: 0.9357 - val_loss: 0.3136 - val_acc: 0.9180\n",
      "Epoch 540/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2638 - acc: 0.9360 - val_loss: 0.3135 - val_acc: 0.9190\n",
      "Epoch 541/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2635 - acc: 0.9360 - val_loss: 0.3133 - val_acc: 0.9180\n",
      "Epoch 542/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2633 - acc: 0.9357 - val_loss: 0.3132 - val_acc: 0.9180\n",
      "Epoch 543/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2630 - acc: 0.9357 - val_loss: 0.3130 - val_acc: 0.9190\n",
      "Epoch 544/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2627 - acc: 0.9365 - val_loss: 0.3128 - val_acc: 0.9180\n",
      "Epoch 545/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2625 - acc: 0.9363 - val_loss: 0.3127 - val_acc: 0.9190\n",
      "Epoch 546/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2622 - acc: 0.9368 - val_loss: 0.3125 - val_acc: 0.9180\n",
      "Epoch 547/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2619 - acc: 0.9355 - val_loss: 0.3124 - val_acc: 0.9190\n",
      "Epoch 548/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2616 - acc: 0.9368 - val_loss: 0.3122 - val_acc: 0.9180\n",
      "Epoch 549/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2614 - acc: 0.9365 - val_loss: 0.3121 - val_acc: 0.9180\n",
      "Epoch 550/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2611 - acc: 0.9365 - val_loss: 0.3119 - val_acc: 0.9180\n",
      "Epoch 551/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2608 - acc: 0.9368 - val_loss: 0.3118 - val_acc: 0.9180\n",
      "Epoch 552/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2606 - acc: 0.9368 - val_loss: 0.3116 - val_acc: 0.9190\n",
      "Epoch 553/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2603 - acc: 0.9370 - val_loss: 0.3114 - val_acc: 0.9190\n",
      "Epoch 554/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2600 - acc: 0.9368 - val_loss: 0.3113 - val_acc: 0.9190\n",
      "Epoch 555/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2597 - acc: 0.9368 - val_loss: 0.3112 - val_acc: 0.9190\n",
      "Epoch 556/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2595 - acc: 0.9370 - val_loss: 0.3111 - val_acc: 0.9190\n",
      "Epoch 557/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.2592 - acc: 0.9370 - val_loss: 0.3109 - val_acc: 0.9180\n",
      "Epoch 558/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2589 - acc: 0.9368 - val_loss: 0.3107 - val_acc: 0.9190\n",
      "Epoch 559/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2587 - acc: 0.9365 - val_loss: 0.3105 - val_acc: 0.9190\n",
      "Epoch 560/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2584 - acc: 0.9370 - val_loss: 0.3104 - val_acc: 0.9190\n",
      "Epoch 561/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2582 - acc: 0.9375 - val_loss: 0.3103 - val_acc: 0.9190\n",
      "Epoch 562/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2579 - acc: 0.9368 - val_loss: 0.3101 - val_acc: 0.9180\n",
      "Epoch 563/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2576 - acc: 0.9380 - val_loss: 0.3100 - val_acc: 0.9180\n",
      "Epoch 564/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2574 - acc: 0.9377 - val_loss: 0.3099 - val_acc: 0.9180\n",
      "Epoch 565/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2571 - acc: 0.9377 - val_loss: 0.3097 - val_acc: 0.9180\n",
      "Epoch 566/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2569 - acc: 0.9373 - val_loss: 0.3096 - val_acc: 0.9190\n",
      "Epoch 567/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2566 - acc: 0.9380 - val_loss: 0.3095 - val_acc: 0.9180\n",
      "Epoch 568/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2563 - acc: 0.9373 - val_loss: 0.3094 - val_acc: 0.9180\n",
      "Epoch 569/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2561 - acc: 0.9382 - val_loss: 0.3092 - val_acc: 0.9190\n",
      "Epoch 570/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2558 - acc: 0.9375 - val_loss: 0.3090 - val_acc: 0.9200\n",
      "Epoch 571/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2556 - acc: 0.9377 - val_loss: 0.3088 - val_acc: 0.9200\n",
      "Epoch 572/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.2553 - acc: 0.9380 - val_loss: 0.3087 - val_acc: 0.9200\n",
      "Epoch 573/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2551 - acc: 0.9382 - val_loss: 0.3085 - val_acc: 0.9200\n",
      "Epoch 574/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2548 - acc: 0.9390 - val_loss: 0.3084 - val_acc: 0.9200\n",
      "Epoch 575/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2546 - acc: 0.9385 - val_loss: 0.3082 - val_acc: 0.9200\n",
      "Epoch 576/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2543 - acc: 0.9382 - val_loss: 0.3082 - val_acc: 0.9200\n",
      "Epoch 577/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2540 - acc: 0.9385 - val_loss: 0.3080 - val_acc: 0.9200\n",
      "Epoch 578/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2538 - acc: 0.9385 - val_loss: 0.3079 - val_acc: 0.9200\n",
      "Epoch 579/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2536 - acc: 0.9387 - val_loss: 0.3077 - val_acc: 0.9190\n",
      "Epoch 580/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2533 - acc: 0.9382 - val_loss: 0.3076 - val_acc: 0.9200\n",
      "Epoch 581/1000\n",
      "4000/4000 [==============================] - 0s 21us/step - loss: 0.2531 - acc: 0.9385 - val_loss: 0.3074 - val_acc: 0.9200\n",
      "Epoch 582/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2528 - acc: 0.9395 - val_loss: 0.3073 - val_acc: 0.9200\n",
      "Epoch 583/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2526 - acc: 0.9390 - val_loss: 0.3072 - val_acc: 0.9190\n",
      "Epoch 584/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2523 - acc: 0.9395 - val_loss: 0.3070 - val_acc: 0.9190\n",
      "Epoch 585/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.2521 - acc: 0.9395 - val_loss: 0.3070 - val_acc: 0.9190\n",
      "Epoch 586/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.2518 - acc: 0.9390 - val_loss: 0.3068 - val_acc: 0.9190\n",
      "Epoch 587/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2516 - acc: 0.9393 - val_loss: 0.3067 - val_acc: 0.9190\n",
      "Epoch 588/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2513 - acc: 0.9387 - val_loss: 0.3066 - val_acc: 0.9200\n",
      "Epoch 589/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2511 - acc: 0.9390 - val_loss: 0.3066 - val_acc: 0.9200\n",
      "Epoch 590/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2508 - acc: 0.9385 - val_loss: 0.3064 - val_acc: 0.9190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 591/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2506 - acc: 0.9393 - val_loss: 0.3062 - val_acc: 0.9190\n",
      "Epoch 592/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2504 - acc: 0.9390 - val_loss: 0.3060 - val_acc: 0.9190\n",
      "Epoch 593/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2501 - acc: 0.9387 - val_loss: 0.3058 - val_acc: 0.9200\n",
      "Epoch 594/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2499 - acc: 0.9395 - val_loss: 0.3058 - val_acc: 0.9190\n",
      "Epoch 595/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2496 - acc: 0.9390 - val_loss: 0.3056 - val_acc: 0.9190\n",
      "Epoch 596/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2494 - acc: 0.9397 - val_loss: 0.3055 - val_acc: 0.9200\n",
      "Epoch 597/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2492 - acc: 0.9390 - val_loss: 0.3053 - val_acc: 0.9200\n",
      "Epoch 598/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2489 - acc: 0.9387 - val_loss: 0.3052 - val_acc: 0.9200\n",
      "Epoch 599/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2487 - acc: 0.9390 - val_loss: 0.3050 - val_acc: 0.9200\n",
      "Epoch 600/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2485 - acc: 0.9395 - val_loss: 0.3049 - val_acc: 0.9200\n",
      "Epoch 601/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2482 - acc: 0.9393 - val_loss: 0.3048 - val_acc: 0.9190\n",
      "Epoch 602/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2480 - acc: 0.9395 - val_loss: 0.3047 - val_acc: 0.9190\n",
      "Epoch 603/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2478 - acc: 0.9393 - val_loss: 0.3045 - val_acc: 0.9200\n",
      "Epoch 604/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2475 - acc: 0.9390 - val_loss: 0.3043 - val_acc: 0.9200\n",
      "Epoch 605/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2473 - acc: 0.9397 - val_loss: 0.3042 - val_acc: 0.9200\n",
      "Epoch 606/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2471 - acc: 0.9397 - val_loss: 0.3042 - val_acc: 0.9180\n",
      "Epoch 607/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2468 - acc: 0.9397 - val_loss: 0.3040 - val_acc: 0.9180\n",
      "Epoch 608/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2466 - acc: 0.9395 - val_loss: 0.3039 - val_acc: 0.9180\n",
      "Epoch 609/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2463 - acc: 0.9397 - val_loss: 0.3038 - val_acc: 0.9180\n",
      "Epoch 610/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2461 - acc: 0.9403 - val_loss: 0.3037 - val_acc: 0.9180\n",
      "Epoch 611/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2459 - acc: 0.9403 - val_loss: 0.3036 - val_acc: 0.9180\n",
      "Epoch 612/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2457 - acc: 0.9400 - val_loss: 0.3035 - val_acc: 0.9180\n",
      "Epoch 613/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2454 - acc: 0.9400 - val_loss: 0.3035 - val_acc: 0.9180\n",
      "Epoch 614/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2452 - acc: 0.9395 - val_loss: 0.3033 - val_acc: 0.9180\n",
      "Epoch 615/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2449 - acc: 0.9397 - val_loss: 0.3031 - val_acc: 0.9180\n",
      "Epoch 616/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2447 - acc: 0.9403 - val_loss: 0.3030 - val_acc: 0.9180\n",
      "Epoch 617/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2445 - acc: 0.9403 - val_loss: 0.3029 - val_acc: 0.9180\n",
      "Epoch 618/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2443 - acc: 0.9405 - val_loss: 0.3028 - val_acc: 0.9180\n",
      "Epoch 619/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2440 - acc: 0.9407 - val_loss: 0.3026 - val_acc: 0.9180\n",
      "Epoch 620/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2438 - acc: 0.9403 - val_loss: 0.3025 - val_acc: 0.9180\n",
      "Epoch 621/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2436 - acc: 0.9405 - val_loss: 0.3024 - val_acc: 0.9180\n",
      "Epoch 622/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2434 - acc: 0.9403 - val_loss: 0.3023 - val_acc: 0.9180\n",
      "Epoch 623/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2431 - acc: 0.9403 - val_loss: 0.3022 - val_acc: 0.9180\n",
      "Epoch 624/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2429 - acc: 0.9407 - val_loss: 0.3021 - val_acc: 0.9180\n",
      "Epoch 625/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2427 - acc: 0.9403 - val_loss: 0.3020 - val_acc: 0.9180\n",
      "Epoch 626/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.2425 - acc: 0.9410 - val_loss: 0.3018 - val_acc: 0.9180\n",
      "Epoch 627/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2423 - acc: 0.9407 - val_loss: 0.3017 - val_acc: 0.9180\n",
      "Epoch 628/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2420 - acc: 0.9405 - val_loss: 0.3016 - val_acc: 0.9180\n",
      "Epoch 629/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2418 - acc: 0.9407 - val_loss: 0.3014 - val_acc: 0.9180\n",
      "Epoch 630/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2416 - acc: 0.9412 - val_loss: 0.3014 - val_acc: 0.9180\n",
      "Epoch 631/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2414 - acc: 0.9410 - val_loss: 0.3011 - val_acc: 0.9180\n",
      "Epoch 632/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2412 - acc: 0.9407 - val_loss: 0.3010 - val_acc: 0.9180\n",
      "Epoch 633/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2409 - acc: 0.9410 - val_loss: 0.3008 - val_acc: 0.9180\n",
      "Epoch 634/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2407 - acc: 0.9415 - val_loss: 0.3008 - val_acc: 0.9180\n",
      "Epoch 635/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2405 - acc: 0.9415 - val_loss: 0.3007 - val_acc: 0.9180\n",
      "Epoch 636/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2403 - acc: 0.9407 - val_loss: 0.3006 - val_acc: 0.9180\n",
      "Epoch 637/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2401 - acc: 0.9410 - val_loss: 0.3005 - val_acc: 0.9180\n",
      "Epoch 638/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2398 - acc: 0.9410 - val_loss: 0.3004 - val_acc: 0.9180\n",
      "Epoch 639/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.2396 - acc: 0.9415 - val_loss: 0.3004 - val_acc: 0.9180\n",
      "Epoch 640/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2394 - acc: 0.9412 - val_loss: 0.3003 - val_acc: 0.9180\n",
      "Epoch 641/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2392 - acc: 0.9412 - val_loss: 0.3002 - val_acc: 0.9180\n",
      "Epoch 642/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2390 - acc: 0.9415 - val_loss: 0.3002 - val_acc: 0.9180\n",
      "Epoch 643/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2388 - acc: 0.9410 - val_loss: 0.2999 - val_acc: 0.9180\n",
      "Epoch 644/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2386 - acc: 0.9417 - val_loss: 0.2998 - val_acc: 0.9180\n",
      "Epoch 645/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2383 - acc: 0.9412 - val_loss: 0.2997 - val_acc: 0.9180\n",
      "Epoch 646/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2381 - acc: 0.9412 - val_loss: 0.2996 - val_acc: 0.9180\n",
      "Epoch 647/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2379 - acc: 0.9415 - val_loss: 0.2995 - val_acc: 0.9180\n",
      "Epoch 648/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2377 - acc: 0.9412 - val_loss: 0.2994 - val_acc: 0.9180\n",
      "Epoch 649/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2375 - acc: 0.9412 - val_loss: 0.2992 - val_acc: 0.9180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 650/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2373 - acc: 0.9415 - val_loss: 0.2992 - val_acc: 0.9180\n",
      "Epoch 651/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2371 - acc: 0.9415 - val_loss: 0.2992 - val_acc: 0.9180\n",
      "Epoch 652/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2369 - acc: 0.9423 - val_loss: 0.2990 - val_acc: 0.9180\n",
      "Epoch 653/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.2366 - acc: 0.9420 - val_loss: 0.2989 - val_acc: 0.9180\n",
      "Epoch 654/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2364 - acc: 0.9415 - val_loss: 0.2988 - val_acc: 0.9180\n",
      "Epoch 655/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2362 - acc: 0.9417 - val_loss: 0.2987 - val_acc: 0.9180\n",
      "Epoch 656/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2360 - acc: 0.9417 - val_loss: 0.2985 - val_acc: 0.9180\n",
      "Epoch 657/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2358 - acc: 0.9417 - val_loss: 0.2985 - val_acc: 0.9180\n",
      "Epoch 658/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2356 - acc: 0.9420 - val_loss: 0.2984 - val_acc: 0.9180\n",
      "Epoch 659/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2354 - acc: 0.9420 - val_loss: 0.2982 - val_acc: 0.9180\n",
      "Epoch 660/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2352 - acc: 0.9423 - val_loss: 0.2981 - val_acc: 0.9180\n",
      "Epoch 661/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2350 - acc: 0.9417 - val_loss: 0.2980 - val_acc: 0.9180\n",
      "Epoch 662/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2348 - acc: 0.9425 - val_loss: 0.2980 - val_acc: 0.9180\n",
      "Epoch 663/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2346 - acc: 0.9420 - val_loss: 0.2978 - val_acc: 0.9180\n",
      "Epoch 664/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2344 - acc: 0.9423 - val_loss: 0.2977 - val_acc: 0.9180\n",
      "Epoch 665/1000\n",
      "4000/4000 [==============================] - 0s 32us/step - loss: 0.2342 - acc: 0.9420 - val_loss: 0.2976 - val_acc: 0.9180\n",
      "Epoch 666/1000\n",
      "4000/4000 [==============================] - 0s 32us/step - loss: 0.2340 - acc: 0.9420 - val_loss: 0.2976 - val_acc: 0.9180\n",
      "Epoch 667/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.2338 - acc: 0.9423 - val_loss: 0.2974 - val_acc: 0.9180\n",
      "Epoch 668/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.2335 - acc: 0.9423 - val_loss: 0.2973 - val_acc: 0.9180\n",
      "Epoch 669/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2334 - acc: 0.9425 - val_loss: 0.2972 - val_acc: 0.9180\n",
      "Epoch 670/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2331 - acc: 0.9423 - val_loss: 0.2971 - val_acc: 0.9180\n",
      "Epoch 671/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2329 - acc: 0.9420 - val_loss: 0.2971 - val_acc: 0.9180\n",
      "Epoch 672/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2328 - acc: 0.9423 - val_loss: 0.2969 - val_acc: 0.9180\n",
      "Epoch 673/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2325 - acc: 0.9425 - val_loss: 0.2969 - val_acc: 0.9180\n",
      "Epoch 674/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2323 - acc: 0.9425 - val_loss: 0.2967 - val_acc: 0.9180\n",
      "Epoch 675/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2321 - acc: 0.9425 - val_loss: 0.2966 - val_acc: 0.9180\n",
      "Epoch 676/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2319 - acc: 0.9425 - val_loss: 0.2966 - val_acc: 0.9180\n",
      "Epoch 677/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2318 - acc: 0.9425 - val_loss: 0.2965 - val_acc: 0.9180\n",
      "Epoch 678/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.2315 - acc: 0.9425 - val_loss: 0.2965 - val_acc: 0.9180\n",
      "Epoch 679/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.2313 - acc: 0.9423 - val_loss: 0.2964 - val_acc: 0.9180\n",
      "Epoch 680/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2311 - acc: 0.9427 - val_loss: 0.2962 - val_acc: 0.9180\n",
      "Epoch 681/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2310 - acc: 0.9425 - val_loss: 0.2961 - val_acc: 0.9180\n",
      "Epoch 682/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.2308 - acc: 0.9430 - val_loss: 0.2960 - val_acc: 0.9180\n",
      "Epoch 683/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2306 - acc: 0.9427 - val_loss: 0.2958 - val_acc: 0.9180\n",
      "Epoch 684/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.2304 - acc: 0.9430 - val_loss: 0.2958 - val_acc: 0.9180\n",
      "Epoch 685/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2302 - acc: 0.9433 - val_loss: 0.2957 - val_acc: 0.9180\n",
      "Epoch 686/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2299 - acc: 0.9433 - val_loss: 0.2957 - val_acc: 0.9180\n",
      "Epoch 687/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2298 - acc: 0.9437 - val_loss: 0.2956 - val_acc: 0.9180\n",
      "Epoch 688/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.2296 - acc: 0.9425 - val_loss: 0.2954 - val_acc: 0.9180\n",
      "Epoch 689/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2294 - acc: 0.9435 - val_loss: 0.2952 - val_acc: 0.9180\n",
      "Epoch 690/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2292 - acc: 0.9435 - val_loss: 0.2952 - val_acc: 0.9180\n",
      "Epoch 691/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2290 - acc: 0.9427 - val_loss: 0.2951 - val_acc: 0.9180\n",
      "Epoch 692/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.2288 - acc: 0.9427 - val_loss: 0.2950 - val_acc: 0.9180\n",
      "Epoch 693/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.2286 - acc: 0.9433 - val_loss: 0.2949 - val_acc: 0.9180\n",
      "Epoch 694/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2284 - acc: 0.9435 - val_loss: 0.2948 - val_acc: 0.9180\n",
      "Epoch 695/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.2282 - acc: 0.9440 - val_loss: 0.2948 - val_acc: 0.9180\n",
      "Epoch 696/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.2280 - acc: 0.9435 - val_loss: 0.2947 - val_acc: 0.9180\n",
      "Epoch 697/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2278 - acc: 0.9440 - val_loss: 0.2947 - val_acc: 0.9180\n",
      "Epoch 698/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2277 - acc: 0.9440 - val_loss: 0.2945 - val_acc: 0.9180\n",
      "Epoch 699/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.2275 - acc: 0.9445 - val_loss: 0.2944 - val_acc: 0.9180\n",
      "Epoch 700/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2273 - acc: 0.9440 - val_loss: 0.2942 - val_acc: 0.9180\n",
      "Epoch 701/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2271 - acc: 0.9437 - val_loss: 0.2943 - val_acc: 0.9180\n",
      "Epoch 702/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2269 - acc: 0.9445 - val_loss: 0.2942 - val_acc: 0.9180\n",
      "Epoch 703/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.2267 - acc: 0.9445 - val_loss: 0.2941 - val_acc: 0.9180\n",
      "Epoch 704/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.2265 - acc: 0.9442 - val_loss: 0.2940 - val_acc: 0.9180\n",
      "Epoch 705/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.2263 - acc: 0.9442 - val_loss: 0.2940 - val_acc: 0.9180\n",
      "Epoch 706/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.2262 - acc: 0.9445 - val_loss: 0.2938 - val_acc: 0.9180\n",
      "Epoch 707/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2260 - acc: 0.9447 - val_loss: 0.2937 - val_acc: 0.9180\n",
      "Epoch 708/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2257 - acc: 0.9442 - val_loss: 0.2935 - val_acc: 0.9180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 709/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2256 - acc: 0.9445 - val_loss: 0.2935 - val_acc: 0.9180\n",
      "Epoch 710/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2254 - acc: 0.9442 - val_loss: 0.2934 - val_acc: 0.9180\n",
      "Epoch 711/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2252 - acc: 0.9447 - val_loss: 0.2933 - val_acc: 0.9180\n",
      "Epoch 712/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2250 - acc: 0.9447 - val_loss: 0.2933 - val_acc: 0.9180\n",
      "Epoch 713/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2248 - acc: 0.9442 - val_loss: 0.2932 - val_acc: 0.9180\n",
      "Epoch 714/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2246 - acc: 0.9447 - val_loss: 0.2932 - val_acc: 0.9180\n",
      "Epoch 715/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2245 - acc: 0.9442 - val_loss: 0.2930 - val_acc: 0.9180\n",
      "Epoch 716/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2243 - acc: 0.9442 - val_loss: 0.2928 - val_acc: 0.9180\n",
      "Epoch 717/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.2241 - acc: 0.9445 - val_loss: 0.2928 - val_acc: 0.9180\n",
      "Epoch 718/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.2239 - acc: 0.9442 - val_loss: 0.2927 - val_acc: 0.9180\n",
      "Epoch 719/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2237 - acc: 0.9445 - val_loss: 0.2926 - val_acc: 0.9180\n",
      "Epoch 720/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2235 - acc: 0.9447 - val_loss: 0.2925 - val_acc: 0.9180\n",
      "Epoch 721/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2234 - acc: 0.9445 - val_loss: 0.2925 - val_acc: 0.9180\n",
      "Epoch 722/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2232 - acc: 0.9450 - val_loss: 0.2924 - val_acc: 0.9180\n",
      "Epoch 723/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2230 - acc: 0.9450 - val_loss: 0.2923 - val_acc: 0.9180\n",
      "Epoch 724/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2228 - acc: 0.9450 - val_loss: 0.2923 - val_acc: 0.9180\n",
      "Epoch 725/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2226 - acc: 0.9450 - val_loss: 0.2922 - val_acc: 0.9180\n",
      "Epoch 726/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2225 - acc: 0.9445 - val_loss: 0.2922 - val_acc: 0.9180\n",
      "Epoch 727/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2223 - acc: 0.9445 - val_loss: 0.2920 - val_acc: 0.9180\n",
      "Epoch 728/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2221 - acc: 0.9450 - val_loss: 0.2920 - val_acc: 0.9180\n",
      "Epoch 729/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2219 - acc: 0.9450 - val_loss: 0.2919 - val_acc: 0.9180\n",
      "Epoch 730/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.2217 - acc: 0.9453 - val_loss: 0.2917 - val_acc: 0.9180\n",
      "Epoch 731/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.2215 - acc: 0.9455 - val_loss: 0.2916 - val_acc: 0.9170\n",
      "Epoch 732/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2214 - acc: 0.9457 - val_loss: 0.2915 - val_acc: 0.9170\n",
      "Epoch 733/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2212 - acc: 0.9453 - val_loss: 0.2914 - val_acc: 0.9160\n",
      "Epoch 734/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2210 - acc: 0.9450 - val_loss: 0.2914 - val_acc: 0.9170\n",
      "Epoch 735/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2208 - acc: 0.9453 - val_loss: 0.2913 - val_acc: 0.9170\n",
      "Epoch 736/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2207 - acc: 0.9453 - val_loss: 0.2912 - val_acc: 0.9160\n",
      "Epoch 737/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2205 - acc: 0.9453 - val_loss: 0.2911 - val_acc: 0.9160\n",
      "Epoch 738/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2203 - acc: 0.9455 - val_loss: 0.2911 - val_acc: 0.9160\n",
      "Epoch 739/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2201 - acc: 0.9455 - val_loss: 0.2910 - val_acc: 0.9160\n",
      "Epoch 740/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2200 - acc: 0.9455 - val_loss: 0.2910 - val_acc: 0.9160\n",
      "Epoch 741/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2198 - acc: 0.9450 - val_loss: 0.2909 - val_acc: 0.9160\n",
      "Epoch 742/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2196 - acc: 0.9453 - val_loss: 0.2908 - val_acc: 0.9160\n",
      "Epoch 743/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.2194 - acc: 0.9455 - val_loss: 0.2907 - val_acc: 0.9160\n",
      "Epoch 744/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.2193 - acc: 0.9457 - val_loss: 0.2906 - val_acc: 0.9160\n",
      "Epoch 745/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2191 - acc: 0.9453 - val_loss: 0.2906 - val_acc: 0.9160\n",
      "Epoch 746/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2189 - acc: 0.9457 - val_loss: 0.2905 - val_acc: 0.9160\n",
      "Epoch 747/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2187 - acc: 0.9447 - val_loss: 0.2903 - val_acc: 0.9160\n",
      "Epoch 748/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2185 - acc: 0.9453 - val_loss: 0.2903 - val_acc: 0.9160\n",
      "Epoch 749/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2184 - acc: 0.9457 - val_loss: 0.2902 - val_acc: 0.9160\n",
      "Epoch 750/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2182 - acc: 0.9460 - val_loss: 0.2901 - val_acc: 0.9160\n",
      "Epoch 751/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2180 - acc: 0.9457 - val_loss: 0.2900 - val_acc: 0.9160\n",
      "Epoch 752/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2179 - acc: 0.9457 - val_loss: 0.2899 - val_acc: 0.9160\n",
      "Epoch 753/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2177 - acc: 0.9463 - val_loss: 0.2899 - val_acc: 0.9160\n",
      "Epoch 754/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2175 - acc: 0.9455 - val_loss: 0.2898 - val_acc: 0.9160\n",
      "Epoch 755/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2173 - acc: 0.9460 - val_loss: 0.2897 - val_acc: 0.9160\n",
      "Epoch 756/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.2172 - acc: 0.9463 - val_loss: 0.2897 - val_acc: 0.9160\n",
      "Epoch 757/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.2170 - acc: 0.9460 - val_loss: 0.2896 - val_acc: 0.9160\n",
      "Epoch 758/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2168 - acc: 0.9460 - val_loss: 0.2897 - val_acc: 0.9160\n",
      "Epoch 759/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2167 - acc: 0.9465 - val_loss: 0.2896 - val_acc: 0.9160\n",
      "Epoch 760/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2165 - acc: 0.9463 - val_loss: 0.2895 - val_acc: 0.9160\n",
      "Epoch 761/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2163 - acc: 0.9463 - val_loss: 0.2894 - val_acc: 0.9160\n",
      "Epoch 762/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2162 - acc: 0.9465 - val_loss: 0.2891 - val_acc: 0.9160\n",
      "Epoch 763/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2160 - acc: 0.9467 - val_loss: 0.2891 - val_acc: 0.9160\n",
      "Epoch 764/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2158 - acc: 0.9470 - val_loss: 0.2891 - val_acc: 0.9160\n",
      "Epoch 765/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2156 - acc: 0.9470 - val_loss: 0.2891 - val_acc: 0.9160\n",
      "Epoch 766/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2155 - acc: 0.9460 - val_loss: 0.2890 - val_acc: 0.9160\n",
      "Epoch 767/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2153 - acc: 0.9467 - val_loss: 0.2889 - val_acc: 0.9160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 768/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2151 - acc: 0.9467 - val_loss: 0.2888 - val_acc: 0.9160\n",
      "Epoch 769/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.2150 - acc: 0.9470 - val_loss: 0.2887 - val_acc: 0.9160\n",
      "Epoch 770/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.2148 - acc: 0.9470 - val_loss: 0.2887 - val_acc: 0.9160\n",
      "Epoch 771/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2147 - acc: 0.9463 - val_loss: 0.2885 - val_acc: 0.9160\n",
      "Epoch 772/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2145 - acc: 0.9472 - val_loss: 0.2886 - val_acc: 0.9160\n",
      "Epoch 773/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2143 - acc: 0.9472 - val_loss: 0.2885 - val_acc: 0.9160\n",
      "Epoch 774/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2142 - acc: 0.9472 - val_loss: 0.2883 - val_acc: 0.9160\n",
      "Epoch 775/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2140 - acc: 0.9472 - val_loss: 0.2883 - val_acc: 0.9160\n",
      "Epoch 776/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2138 - acc: 0.9467 - val_loss: 0.2882 - val_acc: 0.9160\n",
      "Epoch 777/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2137 - acc: 0.9472 - val_loss: 0.2882 - val_acc: 0.9160\n",
      "Epoch 778/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2135 - acc: 0.9465 - val_loss: 0.2881 - val_acc: 0.9160\n",
      "Epoch 779/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2133 - acc: 0.9472 - val_loss: 0.2881 - val_acc: 0.9160\n",
      "Epoch 780/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2131 - acc: 0.9470 - val_loss: 0.2880 - val_acc: 0.9160\n",
      "Epoch 781/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2130 - acc: 0.9472 - val_loss: 0.2880 - val_acc: 0.9160\n",
      "Epoch 782/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.2128 - acc: 0.9470 - val_loss: 0.2879 - val_acc: 0.9160\n",
      "Epoch 783/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.2126 - acc: 0.9470 - val_loss: 0.2877 - val_acc: 0.9160\n",
      "Epoch 784/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2125 - acc: 0.9475 - val_loss: 0.2877 - val_acc: 0.9160\n",
      "Epoch 785/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2123 - acc: 0.9475 - val_loss: 0.2876 - val_acc: 0.9160\n",
      "Epoch 786/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2122 - acc: 0.9475 - val_loss: 0.2875 - val_acc: 0.9160\n",
      "Epoch 787/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2120 - acc: 0.9470 - val_loss: 0.2875 - val_acc: 0.9160\n",
      "Epoch 788/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2118 - acc: 0.9475 - val_loss: 0.2874 - val_acc: 0.9160\n",
      "Epoch 789/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2117 - acc: 0.9472 - val_loss: 0.2873 - val_acc: 0.9160\n",
      "Epoch 790/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2115 - acc: 0.9475 - val_loss: 0.2872 - val_acc: 0.9160\n",
      "Epoch 791/1000\n",
      "4000/4000 [==============================] - 0s 21us/step - loss: 0.2114 - acc: 0.9477 - val_loss: 0.2871 - val_acc: 0.9160\n",
      "Epoch 792/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2112 - acc: 0.9472 - val_loss: 0.2870 - val_acc: 0.9160\n",
      "Epoch 793/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2110 - acc: 0.9475 - val_loss: 0.2870 - val_acc: 0.9160\n",
      "Epoch 794/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2109 - acc: 0.9472 - val_loss: 0.2869 - val_acc: 0.9160\n",
      "Epoch 795/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2107 - acc: 0.9475 - val_loss: 0.2868 - val_acc: 0.9160\n",
      "Epoch 796/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2106 - acc: 0.9477 - val_loss: 0.2868 - val_acc: 0.9160\n",
      "Epoch 797/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.2104 - acc: 0.9475 - val_loss: 0.2868 - val_acc: 0.9160\n",
      "Epoch 798/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2102 - acc: 0.9475 - val_loss: 0.2867 - val_acc: 0.9180\n",
      "Epoch 799/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2101 - acc: 0.9477 - val_loss: 0.2867 - val_acc: 0.9180\n",
      "Epoch 800/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2099 - acc: 0.9477 - val_loss: 0.2866 - val_acc: 0.9180\n",
      "Epoch 801/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2098 - acc: 0.9475 - val_loss: 0.2865 - val_acc: 0.9170\n",
      "Epoch 802/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2096 - acc: 0.9480 - val_loss: 0.2864 - val_acc: 0.9180\n",
      "Epoch 803/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.2095 - acc: 0.9480 - val_loss: 0.2863 - val_acc: 0.9180\n",
      "Epoch 804/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2093 - acc: 0.9477 - val_loss: 0.2863 - val_acc: 0.9180\n",
      "Epoch 805/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2091 - acc: 0.9475 - val_loss: 0.2863 - val_acc: 0.9180\n",
      "Epoch 806/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2090 - acc: 0.9477 - val_loss: 0.2861 - val_acc: 0.9180\n",
      "Epoch 807/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2088 - acc: 0.9477 - val_loss: 0.2860 - val_acc: 0.9180\n",
      "Epoch 808/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2087 - acc: 0.9477 - val_loss: 0.2859 - val_acc: 0.9170\n",
      "Epoch 809/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2085 - acc: 0.9477 - val_loss: 0.2860 - val_acc: 0.9180\n",
      "Epoch 810/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.2083 - acc: 0.9477 - val_loss: 0.2858 - val_acc: 0.9180\n",
      "Epoch 811/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.2082 - acc: 0.9477 - val_loss: 0.2858 - val_acc: 0.9180\n",
      "Epoch 812/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2080 - acc: 0.9480 - val_loss: 0.2858 - val_acc: 0.9180\n",
      "Epoch 813/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2079 - acc: 0.9477 - val_loss: 0.2856 - val_acc: 0.9170\n",
      "Epoch 814/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.2077 - acc: 0.9477 - val_loss: 0.2856 - val_acc: 0.9180\n",
      "Epoch 815/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2076 - acc: 0.9487 - val_loss: 0.2855 - val_acc: 0.9180\n",
      "Epoch 816/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2074 - acc: 0.9480 - val_loss: 0.2853 - val_acc: 0.9170\n",
      "Epoch 817/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2073 - acc: 0.9480 - val_loss: 0.2853 - val_acc: 0.9180\n",
      "Epoch 818/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2071 - acc: 0.9480 - val_loss: 0.2853 - val_acc: 0.9180\n",
      "Epoch 819/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2070 - acc: 0.9477 - val_loss: 0.2853 - val_acc: 0.9180\n",
      "Epoch 820/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2068 - acc: 0.9480 - val_loss: 0.2852 - val_acc: 0.9180\n",
      "Epoch 821/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2066 - acc: 0.9480 - val_loss: 0.2850 - val_acc: 0.9180\n",
      "Epoch 822/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2065 - acc: 0.9485 - val_loss: 0.2850 - val_acc: 0.9180\n",
      "Epoch 823/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.2063 - acc: 0.9487 - val_loss: 0.2850 - val_acc: 0.9180\n",
      "Epoch 824/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.2062 - acc: 0.9480 - val_loss: 0.2849 - val_acc: 0.9180\n",
      "Epoch 825/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2060 - acc: 0.9483 - val_loss: 0.2848 - val_acc: 0.9180\n",
      "Epoch 826/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2059 - acc: 0.9480 - val_loss: 0.2847 - val_acc: 0.9180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 827/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2057 - acc: 0.9485 - val_loss: 0.2847 - val_acc: 0.9180\n",
      "Epoch 828/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2056 - acc: 0.9490 - val_loss: 0.2847 - val_acc: 0.9180\n",
      "Epoch 829/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2054 - acc: 0.9483 - val_loss: 0.2846 - val_acc: 0.9180\n",
      "Epoch 830/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2053 - acc: 0.9485 - val_loss: 0.2844 - val_acc: 0.9190\n",
      "Epoch 831/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2051 - acc: 0.9490 - val_loss: 0.2844 - val_acc: 0.9180\n",
      "Epoch 832/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2050 - acc: 0.9487 - val_loss: 0.2845 - val_acc: 0.9180\n",
      "Epoch 833/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2048 - acc: 0.9493 - val_loss: 0.2844 - val_acc: 0.9180\n",
      "Epoch 834/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2047 - acc: 0.9487 - val_loss: 0.2843 - val_acc: 0.9180\n",
      "Epoch 835/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2045 - acc: 0.9485 - val_loss: 0.2842 - val_acc: 0.9180\n",
      "Epoch 836/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.2044 - acc: 0.9490 - val_loss: 0.2841 - val_acc: 0.9190\n",
      "Epoch 837/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.2042 - acc: 0.9490 - val_loss: 0.2841 - val_acc: 0.9180\n",
      "Epoch 838/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2041 - acc: 0.9490 - val_loss: 0.2840 - val_acc: 0.9180\n",
      "Epoch 839/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2039 - acc: 0.9490 - val_loss: 0.2840 - val_acc: 0.9180\n",
      "Epoch 840/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2038 - acc: 0.9493 - val_loss: 0.2839 - val_acc: 0.9180\n",
      "Epoch 841/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2036 - acc: 0.9493 - val_loss: 0.2838 - val_acc: 0.9190\n",
      "Epoch 842/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2035 - acc: 0.9490 - val_loss: 0.2838 - val_acc: 0.9180\n",
      "Epoch 843/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2033 - acc: 0.9495 - val_loss: 0.2837 - val_acc: 0.9190\n",
      "Epoch 844/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2032 - acc: 0.9497 - val_loss: 0.2837 - val_acc: 0.9180\n",
      "Epoch 845/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2030 - acc: 0.9495 - val_loss: 0.2836 - val_acc: 0.9190\n",
      "Epoch 846/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2029 - acc: 0.9495 - val_loss: 0.2835 - val_acc: 0.9190\n",
      "Epoch 847/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2027 - acc: 0.9495 - val_loss: 0.2836 - val_acc: 0.9190\n",
      "Epoch 848/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2026 - acc: 0.9495 - val_loss: 0.2835 - val_acc: 0.9190\n",
      "Epoch 849/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2024 - acc: 0.9500 - val_loss: 0.2835 - val_acc: 0.9180\n",
      "Epoch 850/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2023 - acc: 0.9497 - val_loss: 0.2834 - val_acc: 0.9180\n",
      "Epoch 851/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2021 - acc: 0.9497 - val_loss: 0.2833 - val_acc: 0.9180\n",
      "Epoch 852/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.2020 - acc: 0.9500 - val_loss: 0.2833 - val_acc: 0.9180\n",
      "Epoch 853/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2019 - acc: 0.9497 - val_loss: 0.2832 - val_acc: 0.9180\n",
      "Epoch 854/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2017 - acc: 0.9502 - val_loss: 0.2833 - val_acc: 0.9180\n",
      "Epoch 855/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2016 - acc: 0.9500 - val_loss: 0.2831 - val_acc: 0.9190\n",
      "Epoch 856/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2014 - acc: 0.9500 - val_loss: 0.2830 - val_acc: 0.9190\n",
      "Epoch 857/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2013 - acc: 0.9505 - val_loss: 0.2828 - val_acc: 0.9190\n",
      "Epoch 858/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2011 - acc: 0.9502 - val_loss: 0.2827 - val_acc: 0.9190\n",
      "Epoch 859/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2010 - acc: 0.9502 - val_loss: 0.2828 - val_acc: 0.9190\n",
      "Epoch 860/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.2008 - acc: 0.9502 - val_loss: 0.2827 - val_acc: 0.9190\n",
      "Epoch 861/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.2007 - acc: 0.9502 - val_loss: 0.2827 - val_acc: 0.9190\n",
      "Epoch 862/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.2005 - acc: 0.9505 - val_loss: 0.2826 - val_acc: 0.9190\n",
      "Epoch 863/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.2004 - acc: 0.9502 - val_loss: 0.2825 - val_acc: 0.9190\n",
      "Epoch 864/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2003 - acc: 0.9497 - val_loss: 0.2825 - val_acc: 0.9190\n",
      "Epoch 865/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.2001 - acc: 0.9505 - val_loss: 0.2825 - val_acc: 0.9190\n",
      "Epoch 866/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.2000 - acc: 0.9505 - val_loss: 0.2823 - val_acc: 0.9190\n",
      "Epoch 867/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1998 - acc: 0.9505 - val_loss: 0.2823 - val_acc: 0.9190\n",
      "Epoch 868/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1997 - acc: 0.9502 - val_loss: 0.2822 - val_acc: 0.9190\n",
      "Epoch 869/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.1995 - acc: 0.9505 - val_loss: 0.2822 - val_acc: 0.9190\n",
      "Epoch 870/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.1994 - acc: 0.9502 - val_loss: 0.2822 - val_acc: 0.9190\n",
      "Epoch 871/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1993 - acc: 0.9505 - val_loss: 0.2822 - val_acc: 0.9190\n",
      "Epoch 872/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1991 - acc: 0.9507 - val_loss: 0.2821 - val_acc: 0.9190\n",
      "Epoch 873/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.1990 - acc: 0.9505 - val_loss: 0.2820 - val_acc: 0.9190\n",
      "Epoch 874/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.1988 - acc: 0.9505 - val_loss: 0.2819 - val_acc: 0.9190\n",
      "Epoch 875/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.1987 - acc: 0.9505 - val_loss: 0.2818 - val_acc: 0.9190\n",
      "Epoch 876/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.1985 - acc: 0.9505 - val_loss: 0.2817 - val_acc: 0.9190\n",
      "Epoch 877/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.1984 - acc: 0.9505 - val_loss: 0.2817 - val_acc: 0.9190\n",
      "Epoch 878/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1983 - acc: 0.9505 - val_loss: 0.2816 - val_acc: 0.9190\n",
      "Epoch 879/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.1981 - acc: 0.9505 - val_loss: 0.2816 - val_acc: 0.9190\n",
      "Epoch 880/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.1980 - acc: 0.9505 - val_loss: 0.2816 - val_acc: 0.9190\n",
      "Epoch 881/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.1978 - acc: 0.9505 - val_loss: 0.2815 - val_acc: 0.9190\n",
      "Epoch 882/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1977 - acc: 0.9505 - val_loss: 0.2814 - val_acc: 0.9190\n",
      "Epoch 883/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1976 - acc: 0.9505 - val_loss: 0.2814 - val_acc: 0.9190\n",
      "Epoch 884/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.1974 - acc: 0.9510 - val_loss: 0.2814 - val_acc: 0.9190\n",
      "Epoch 885/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.1973 - acc: 0.9510 - val_loss: 0.2812 - val_acc: 0.9190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 886/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1972 - acc: 0.9507 - val_loss: 0.2812 - val_acc: 0.9190\n",
      "Epoch 887/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.1970 - acc: 0.9510 - val_loss: 0.2812 - val_acc: 0.9190\n",
      "Epoch 888/1000\n",
      "4000/4000 [==============================] - 0s 37us/step - loss: 0.1969 - acc: 0.9505 - val_loss: 0.2811 - val_acc: 0.9190\n",
      "Epoch 889/1000\n",
      "4000/4000 [==============================] - 0s 33us/step - loss: 0.1967 - acc: 0.9510 - val_loss: 0.2810 - val_acc: 0.9190\n",
      "Epoch 890/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.1966 - acc: 0.9510 - val_loss: 0.2810 - val_acc: 0.9190\n",
      "Epoch 891/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.1964 - acc: 0.9510 - val_loss: 0.2809 - val_acc: 0.9190\n",
      "Epoch 892/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.1963 - acc: 0.9507 - val_loss: 0.2809 - val_acc: 0.9190\n",
      "Epoch 893/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.1962 - acc: 0.9513 - val_loss: 0.2809 - val_acc: 0.9190\n",
      "Epoch 894/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.1961 - acc: 0.9510 - val_loss: 0.2809 - val_acc: 0.9190\n",
      "Epoch 895/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.1959 - acc: 0.9507 - val_loss: 0.2808 - val_acc: 0.9190\n",
      "Epoch 896/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.1958 - acc: 0.9515 - val_loss: 0.2808 - val_acc: 0.9190\n",
      "Epoch 897/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.1956 - acc: 0.9515 - val_loss: 0.2807 - val_acc: 0.9190\n",
      "Epoch 898/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.1955 - acc: 0.9515 - val_loss: 0.2806 - val_acc: 0.9190\n",
      "Epoch 899/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.1954 - acc: 0.9510 - val_loss: 0.2805 - val_acc: 0.9190\n",
      "Epoch 900/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.1952 - acc: 0.9517 - val_loss: 0.2804 - val_acc: 0.9190\n",
      "Epoch 901/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.1951 - acc: 0.9520 - val_loss: 0.2804 - val_acc: 0.9190\n",
      "Epoch 902/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.1950 - acc: 0.9520 - val_loss: 0.2803 - val_acc: 0.9190\n",
      "Epoch 903/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.1948 - acc: 0.9517 - val_loss: 0.2803 - val_acc: 0.9190\n",
      "Epoch 904/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.1947 - acc: 0.9517 - val_loss: 0.2802 - val_acc: 0.9190\n",
      "Epoch 905/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.1945 - acc: 0.9520 - val_loss: 0.2802 - val_acc: 0.9190\n",
      "Epoch 906/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.1944 - acc: 0.9517 - val_loss: 0.2801 - val_acc: 0.9190\n",
      "Epoch 907/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.1943 - acc: 0.9515 - val_loss: 0.2801 - val_acc: 0.9190\n",
      "Epoch 908/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.1941 - acc: 0.9520 - val_loss: 0.2801 - val_acc: 0.9190\n",
      "Epoch 909/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.1940 - acc: 0.9517 - val_loss: 0.2801 - val_acc: 0.9190\n",
      "Epoch 910/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.1939 - acc: 0.9515 - val_loss: 0.2799 - val_acc: 0.9190\n",
      "Epoch 911/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.1937 - acc: 0.9520 - val_loss: 0.2799 - val_acc: 0.9190\n",
      "Epoch 912/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.1936 - acc: 0.9523 - val_loss: 0.2798 - val_acc: 0.9190\n",
      "Epoch 913/1000\n",
      "4000/4000 [==============================] - 0s 32us/step - loss: 0.1935 - acc: 0.9525 - val_loss: 0.2798 - val_acc: 0.9190\n",
      "Epoch 914/1000\n",
      "4000/4000 [==============================] - 0s 34us/step - loss: 0.1933 - acc: 0.9520 - val_loss: 0.2797 - val_acc: 0.9190\n",
      "Epoch 915/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.1932 - acc: 0.9525 - val_loss: 0.2797 - val_acc: 0.9190\n",
      "Epoch 916/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.1930 - acc: 0.9520 - val_loss: 0.2796 - val_acc: 0.9190\n",
      "Epoch 917/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.1929 - acc: 0.9517 - val_loss: 0.2796 - val_acc: 0.9190\n",
      "Epoch 918/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1928 - acc: 0.9523 - val_loss: 0.2796 - val_acc: 0.9190\n",
      "Epoch 919/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.1926 - acc: 0.9520 - val_loss: 0.2795 - val_acc: 0.9190\n",
      "Epoch 920/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.1925 - acc: 0.9520 - val_loss: 0.2794 - val_acc: 0.9190\n",
      "Epoch 921/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1924 - acc: 0.9520 - val_loss: 0.2794 - val_acc: 0.9190\n",
      "Epoch 922/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.1923 - acc: 0.9520 - val_loss: 0.2793 - val_acc: 0.9190\n",
      "Epoch 923/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.1921 - acc: 0.9515 - val_loss: 0.2792 - val_acc: 0.9190\n",
      "Epoch 924/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1920 - acc: 0.9523 - val_loss: 0.2792 - val_acc: 0.9190\n",
      "Epoch 925/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1919 - acc: 0.9525 - val_loss: 0.2791 - val_acc: 0.9190\n",
      "Epoch 926/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.1917 - acc: 0.9520 - val_loss: 0.2792 - val_acc: 0.9190\n",
      "Epoch 927/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.1916 - acc: 0.9520 - val_loss: 0.2790 - val_acc: 0.9190\n",
      "Epoch 928/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.1915 - acc: 0.9523 - val_loss: 0.2790 - val_acc: 0.9190\n",
      "Epoch 929/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.1913 - acc: 0.9515 - val_loss: 0.2789 - val_acc: 0.9190\n",
      "Epoch 930/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.1912 - acc: 0.9525 - val_loss: 0.2789 - val_acc: 0.9190\n",
      "Epoch 931/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1911 - acc: 0.9523 - val_loss: 0.2789 - val_acc: 0.9190\n",
      "Epoch 932/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.1909 - acc: 0.9523 - val_loss: 0.2788 - val_acc: 0.9190\n",
      "Epoch 933/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1908 - acc: 0.9525 - val_loss: 0.2787 - val_acc: 0.9190\n",
      "Epoch 934/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1907 - acc: 0.9523 - val_loss: 0.2787 - val_acc: 0.9190\n",
      "Epoch 935/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.1906 - acc: 0.9525 - val_loss: 0.2786 - val_acc: 0.9190\n",
      "Epoch 936/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1904 - acc: 0.9525 - val_loss: 0.2786 - val_acc: 0.9190\n",
      "Epoch 937/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1903 - acc: 0.9523 - val_loss: 0.2785 - val_acc: 0.9190\n",
      "Epoch 938/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1902 - acc: 0.9525 - val_loss: 0.2786 - val_acc: 0.9190\n",
      "Epoch 939/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1901 - acc: 0.9527 - val_loss: 0.2785 - val_acc: 0.9190\n",
      "Epoch 940/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.1899 - acc: 0.9527 - val_loss: 0.2785 - val_acc: 0.9190\n",
      "Epoch 941/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.1898 - acc: 0.9525 - val_loss: 0.2784 - val_acc: 0.9190\n",
      "Epoch 942/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1897 - acc: 0.9520 - val_loss: 0.2784 - val_acc: 0.9190\n",
      "Epoch 943/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.1895 - acc: 0.9525 - val_loss: 0.2782 - val_acc: 0.9190\n",
      "Epoch 944/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.1894 - acc: 0.9525 - val_loss: 0.2783 - val_acc: 0.9190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 945/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.1893 - acc: 0.9525 - val_loss: 0.2782 - val_acc: 0.9200\n",
      "Epoch 946/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.1891 - acc: 0.9530 - val_loss: 0.2782 - val_acc: 0.9200\n",
      "Epoch 947/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.1890 - acc: 0.9520 - val_loss: 0.2782 - val_acc: 0.9200\n",
      "Epoch 948/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.1889 - acc: 0.9527 - val_loss: 0.2781 - val_acc: 0.9200\n",
      "Epoch 949/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.1888 - acc: 0.9525 - val_loss: 0.2780 - val_acc: 0.9190\n",
      "Epoch 950/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.1886 - acc: 0.9530 - val_loss: 0.2780 - val_acc: 0.9190\n",
      "Epoch 951/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.1885 - acc: 0.9530 - val_loss: 0.2779 - val_acc: 0.9200\n",
      "Epoch 952/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.1884 - acc: 0.9530 - val_loss: 0.2778 - val_acc: 0.9200\n",
      "Epoch 953/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.1882 - acc: 0.9530 - val_loss: 0.2778 - val_acc: 0.9200\n",
      "Epoch 954/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.1881 - acc: 0.9527 - val_loss: 0.2778 - val_acc: 0.9200\n",
      "Epoch 955/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.1880 - acc: 0.9527 - val_loss: 0.2777 - val_acc: 0.9200\n",
      "Epoch 956/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.1879 - acc: 0.9527 - val_loss: 0.2777 - val_acc: 0.9200\n",
      "Epoch 957/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.1878 - acc: 0.9525 - val_loss: 0.2776 - val_acc: 0.9210\n",
      "Epoch 958/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.1876 - acc: 0.9532 - val_loss: 0.2775 - val_acc: 0.9200\n",
      "Epoch 959/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1875 - acc: 0.9532 - val_loss: 0.2776 - val_acc: 0.9200\n",
      "Epoch 960/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1873 - acc: 0.9527 - val_loss: 0.2776 - val_acc: 0.9210\n",
      "Epoch 961/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.1872 - acc: 0.9530 - val_loss: 0.2776 - val_acc: 0.9210\n",
      "Epoch 962/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1871 - acc: 0.9530 - val_loss: 0.2776 - val_acc: 0.9210\n",
      "Epoch 963/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.1870 - acc: 0.9527 - val_loss: 0.2775 - val_acc: 0.9200\n",
      "Epoch 964/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.1869 - acc: 0.9530 - val_loss: 0.2774 - val_acc: 0.9210\n",
      "Epoch 965/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.1867 - acc: 0.9525 - val_loss: 0.2773 - val_acc: 0.9210\n",
      "Epoch 966/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.1866 - acc: 0.9532 - val_loss: 0.2772 - val_acc: 0.9210\n",
      "Epoch 967/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.1865 - acc: 0.9535 - val_loss: 0.2772 - val_acc: 0.9210\n",
      "Epoch 968/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1863 - acc: 0.9530 - val_loss: 0.2772 - val_acc: 0.9210\n",
      "Epoch 969/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.1862 - acc: 0.9530 - val_loss: 0.2771 - val_acc: 0.9210\n",
      "Epoch 970/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1861 - acc: 0.9527 - val_loss: 0.2771 - val_acc: 0.9210\n",
      "Epoch 971/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.1860 - acc: 0.9527 - val_loss: 0.2771 - val_acc: 0.9210\n",
      "Epoch 972/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1859 - acc: 0.9535 - val_loss: 0.2770 - val_acc: 0.9210\n",
      "Epoch 973/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1857 - acc: 0.9537 - val_loss: 0.2770 - val_acc: 0.9210\n",
      "Epoch 974/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1856 - acc: 0.9527 - val_loss: 0.2770 - val_acc: 0.9210\n",
      "Epoch 975/1000\n",
      "4000/4000 [==============================] - ETA: 0s - loss: 0.1878 - acc: 0.952 - 0s 22us/step - loss: 0.1855 - acc: 0.9532 - val_loss: 0.2769 - val_acc: 0.9210\n",
      "Epoch 976/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.1854 - acc: 0.9532 - val_loss: 0.2768 - val_acc: 0.9210\n",
      "Epoch 977/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1852 - acc: 0.9532 - val_loss: 0.2768 - val_acc: 0.9210\n",
      "Epoch 978/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.1851 - acc: 0.9535 - val_loss: 0.2768 - val_acc: 0.9210\n",
      "Epoch 979/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.1850 - acc: 0.9535 - val_loss: 0.2767 - val_acc: 0.9210\n",
      "Epoch 980/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.1849 - acc: 0.9532 - val_loss: 0.2767 - val_acc: 0.9210\n",
      "Epoch 981/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1847 - acc: 0.9535 - val_loss: 0.2766 - val_acc: 0.9220\n",
      "Epoch 982/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1846 - acc: 0.9532 - val_loss: 0.2765 - val_acc: 0.9220\n",
      "Epoch 983/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.1845 - acc: 0.9530 - val_loss: 0.2765 - val_acc: 0.9220\n",
      "Epoch 984/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.1844 - acc: 0.9537 - val_loss: 0.2764 - val_acc: 0.9220\n",
      "Epoch 985/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1843 - acc: 0.9537 - val_loss: 0.2764 - val_acc: 0.9220\n",
      "Epoch 986/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.1841 - acc: 0.9540 - val_loss: 0.2763 - val_acc: 0.9220\n",
      "Epoch 987/1000\n",
      "4000/4000 [==============================] - 0s 22us/step - loss: 0.1840 - acc: 0.9537 - val_loss: 0.2763 - val_acc: 0.9220\n",
      "Epoch 988/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.1839 - acc: 0.9535 - val_loss: 0.2762 - val_acc: 0.9220\n",
      "Epoch 989/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1838 - acc: 0.9537 - val_loss: 0.2762 - val_acc: 0.9220\n",
      "Epoch 990/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1837 - acc: 0.9543 - val_loss: 0.2762 - val_acc: 0.9220\n",
      "Epoch 991/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.1835 - acc: 0.9543 - val_loss: 0.2761 - val_acc: 0.9220\n",
      "Epoch 992/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.1834 - acc: 0.9543 - val_loss: 0.2761 - val_acc: 0.9220\n",
      "Epoch 993/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.1833 - acc: 0.9537 - val_loss: 0.2759 - val_acc: 0.9220\n",
      "Epoch 994/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.1832 - acc: 0.9540 - val_loss: 0.2759 - val_acc: 0.9220\n",
      "Epoch 995/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.1831 - acc: 0.9537 - val_loss: 0.2759 - val_acc: 0.9220\n",
      "Epoch 996/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.1829 - acc: 0.9543 - val_loss: 0.2759 - val_acc: 0.9230\n",
      "Epoch 997/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.1828 - acc: 0.9543 - val_loss: 0.2759 - val_acc: 0.9230\n",
      "Epoch 998/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.1827 - acc: 0.9543 - val_loss: 0.2758 - val_acc: 0.9230\n",
      "Epoch 999/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.1826 - acc: 0.9540 - val_loss: 0.2757 - val_acc: 0.9230\n",
      "Epoch 1000/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.1825 - acc: 0.9553 - val_loss: 0.2756 - val_acc: 0.9230\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=64, epochs=1000, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how we did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 19us/step\n",
      "Evaluation result on Test Data : Cost = 0.2756446511745453, accuracy = 92.30000000000001\n"
     ]
    }
   ],
   "source": [
    "[test_cost, test_acc] = model.evaluate(X_test, y_test)\n",
    "print(\"Evaluation result on Test Data : Cost = {}, accuracy = {}\".format(test_cost, (test_acc*100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too shabby! Not as good as our hand-crafted model, but remember, we cheated then! Let's draw some plots to visualize our cost and accuracy as the model became more familiar with the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Accuracy Curves')"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAGKCAYAAADkN4OIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzs3Xd4lfX9//HnOzuBsMPeYYsgEBRBUcGBVdEiWrdVrFqto7a21LaOln7VWrVafxZH6yriABcOHDgBlS0yZA9ZspdZJPn8/riTc7JITsJJ7uSc1+O67uvc43Pu8z7H4OveH3POISIiIpEpxu8CREREpOYo6EVERCKYgl5ERCSCKehFREQimIJeREQkginoRUREIpiCXqSOMLPjzewVM9tiZrlmtsvMPjSzK80sNsyfdbKZ3W1mIf8/wMwamNkfzGyBmR0ws2wzW2Fmj5lZt3DWJyLhY7qPXsR/ZnYr8BDwMfAcsAFoCpwOXAVc7Jx7M4yfdzdwFxDvnMsLoX0b4COgLfAYMBPIBfoAVwMxzrkB4apPRMInzu8CRKKdmQ3HC/nHnHM3l1r8ppk9BDSo/cpKeAFoAxzrnFtVbP4nZvY4cG44PsTMEp1zOeFYl4h4dOhexH/jgd3A78pb6Jxb45xbXDRtZsea2UdmdtDMfjSzGWZ2bPH3mNngwsP+u8ws08zWFgZy8b15gENm5szssIf2Ctc9Evi/UiFfVJ9zzr1RrL0r/Izi6+hcOP/nxeY9a2abCk9ZzDazLODvZvaumc0vp442ZpZXePSjaF4XM5tkZjvMLMfMFpnZT0u9r4eZvW5m2wtPN2w0s1fNTDs6EhX0hy7io8Jz7ycDbzjnskNo3w/4DFgG/BxweBsKn5nZEOfcN2bWEHgfmFPY5gDQGRhauJqngfbAOOAEIL+Sjz218PWtEL9WVTQGXgL+AdwBZAFdgMlm1sc5t6xY20sKXycDmFkH4GtgO/BrYAfwM2CqmZ3nnCuq921gL/BLYCfQDvgJ2tGRKKGgF/FXCyAZ75x8KO4EcoCRzrm9AGb2IbAeby99DNAL7/z+74ofCQCeBXDObTKzTYXzvg7hHH2HwtdQa6yKhsBlxa8/MLPFwH7gcuAPxdpeDnzgnPuhcPpuwICTnHO7Cue9X7gB8BfgLTNrAXQHzi0W/AAv1sB3EamTtEUrUr8MB94uCnkA59x+vL3tkwpnrcLbg33CzC4rDL66Kg9vjzvAOZcFTAUuNTMDMLOjgf7A88WajgLeBfaZWVzRgHc0o7+ZNQJ2AWuB+8zsF2bWvca/kUgdo6AX8dcuvMPVnUJs3wzYWs78bXh78Tjn9gGnAFuAx4GNZrbEzM6vZo3fF76GWmNVbHfOlXfq4Hm8IwknF05fjncKovidBy2BK4BDpYYHCpc3d95tRacB84B7gZWF1yv8MszfQ6TOUtCL+KjwsPmnwGlmlhjCW3YDrcuZ37pwWdF6FznnzsfbMDgeWAO8YmZ9q1HmR4Wv54TYPgdIKDWv+WHaHu4iwM+AjcBlhff6XwxMKdzbL7ILmAIMPsywBcA5t9Y5dwWQBgzAu4XxcTM7M8TvI1KvKehF/HcfXhA+UN7CwivL+xVOfgacZWapxZan4oXwZ6Xf65zLc859BfwZ799778JFRbewJVdWnHNuDjADuONwD8Yxs+K3120ASm9QnFXZ55T6TAdMAsbiXTjXnpKH7QGmA/2Apc65eeUMOaXX6ZxbBNxWOKs6Gz0i9Y4uxhPxmXPuczO7DXjIzHrjXTS3Ee9Q/EjgGrwrzhcDfwXOBmaY2f14e8S/B1LwLkDDzM4GrgXeANbh3YN/M96h7y8LP7boavbfmNl7QL5zbl4FZV6Ot2c/18z+RfCBOb3wHpgTT/Cw+kvAn8zsj8BXwIl4e+RV9TzexXgT8U4flN6QuRPvzoLPzewxvAsSm+IFeFfn3NWFG0iPAC8Dq4FYvDsR8vD27EUin3NOgwYNdWDAu/3tVbxz8IfwDsV/AFyG9+S5onbH4YXuQeBHvL3tY4st74kXbOuAbLzbzt4FjivWJhb4f3i3phVQuBNdSX0N8W6BW1j4uTnACrwg7VqsXVLhvK14GxcvA8fibZT8vFi7Z4FNlXzm3ML3/d9hlrfHu11wM96Gx1bgQ7wr+cE7j/8csBLILPxNPwPO8Pu/twYNtTXoEbgiIiIRTOfoRUREIpiCXkREJIIp6EVERCKYgl5ERCSCKehFREQiWETcR9+iRQvXuXNnv8sQERGpNfPnz9/pnEurrF1EBH3nzp2ZN6+iZ32IiIhEFjMLqUdJHboXERGJYAp6ERGRCKagFxERiWAKehERkQimoBcREYlgCnoREZEIpqAXERGJYBFxH72I1F/Z2dns2LGD7Oxs8vLy/C5HxDdxcXEkJSWRlpZGUlJS+NYbtjWJiFTRvn37+OGHH0hLS6N169bExcVhZn6XJVLrnHPk5eVx8OBBNm7cSKtWrWjcuHFY1q2gFxHf7Ny5k/bt25OSkuJ3KSK+MjPi4+Np2rQpiYmJbNu2LWxBr3P05dmyBRYt8rsKkYiXm5tLcnKy32WI1CnJycnk5OSEbX0K+uIWL4Z+/aBdO7j+er+rEYkKOlQvUlK4/03o0H0xrl17li8pYCp/otGcA9yyezc0a+Z3WSIiItWmoC/ms2+bcYpbAkAnt56bP/wI+9mFPlclIiJSfTp0X8zQodA4MRuADXRm4aRlPlckInJkxo8fj5mxbdu2ar0/OzsbM+N6nc6stxT0xSQkwDmnHAhMv/ZhKoTxgggRiU5mFvKwfv16v8uVYlavXs3dd9/NkiVL/C6l2nTovpQx1zTnf9O98UnZY7jnjbeI/dkF/hYlIvXaCy+8UGL6iy++4Mknn+Taa6/lxBNPLLEsLS0trJ89YcIE7r777mo/gCUpKYmsrCzi4qIzLlavXs0999xDr1696Nu3r9/lVEt0/perwKifxNAsOZPdWSmspwvvPfAcZyvoReQIXHbZZSWm8/LyePLJJzn++OPLLDsc5xyZmZk0aNCgSp8dFxd3xCEdzqe0Se3ToftSkpPh6styA9OPzR8COpQmIrVo+vTpmBmTJ0/mkUceoVevXiQmJvKvf/0LgNmzZ3PFFVfQvXt3UlJSaNSoEcOHD+ftt98us67yztEXzVu3bh2333477dq1IykpiYEDB/Lhhx+WeH955+iLz/v888854YQTSElJIS0tjeuvv57MzMwydXz00Uccd9xxJCUl0aZNG37729+ycOFCzIz77rsvpN9l7969jB8/np49e5KUlESLFi0YPnw4U6dOLdFuwYIFjB49mmbNmpGUlETfvn15+OGHKSgoKNFu3bp1XHnllXTs2JHExERatmzJCSecwIsvvgjAxIkTOfPMMwG4+OKLA6dXRo0aFVK9dYX26Mvxy/FNePCpAhwxvM8olk14iD5P3+Z3WSISZe6//3727dvH1VdfTcuWLenatSsAr776KmvWrOGiiy6iY8eO7Nixg2effZZzzjmHqVOnMmbMmJDWf/HFF5OcnMzvfvc7srKyePjhhxk9ejSrV6+mXbt2lb5/zpw5vPrqq1xzzTVcdtllzJgxgyeeeIKEhAQeffTRQLsZM2Zw5pln0rJlS+644w5SU1N56aWX+PTTT0P+LXbu3MmwYcNYuXIlF110Eb/61a84dOgQ8+fP55133uH8888HvI2gkSNHkpyczI033khaWhpvvPEGt912G0uWLOE///kPADk5OZx66qns3LmTG264gW7durF3714WLVrEzJkzueSSSxg5ciS33347DzzwADfeeCNDhgwBoG3btiHXXSc45+r9MGjQIBdu5x631YFz4NxVif9zLjMz7J8hEu2WLVvmdwm+eOaZZxzgnnnmmXKXv/feew5waWlpbteuXWWWHzx4sMy8AwcOuC5durgBAwaUmP/73//eAW7r1q1l5o0ZM8YVFBQE5n/++ecOcHfffXdgXlZWlgPcddddV2ZebGysW7BgQYnPGzFihEtMTHTZ2dmBef369XMpKSlu48aNgXk5OTlu0KBBDnD33ntvub9DcVdddZUD3HPPPVdmWX5+fmB84MCBLj4+vsTfVn5+vhs9erQD3MyZM51zzn399dcOcI888kiFn1v032Ly5MmV1hhOofzbAOa5EDJSh+4P43cPBC+I+V/OBWyeOM3HakSijFndHWrR1VdfTbNyHtpV/Dx9ZmYmu3btIjs7m5NOOolFixaF/PjUW2+9tcRT2E444QQSEhJYtWpVSO8/6aSTGDBgQIl5I0aMICcnh++//x6ADRs2sHjxYsaOHUuHDh0C7RISErj55ptD+py8vDxeeeUVBgwYwBVXXFFmeUyMF2UbN25kwYIFjB07lt69e5dY/oc//AGA119/HSDwHPkZM2awc+fOkOqorxT0hzH0xFhO6LwJgEMk8Mh9Wd4OvohILenRo0e587du3crVV19NWloaDRo0oEWLFqSlpfHss8/inGPfvn0hrb/oVEARM6Np06bs2rWrWu8HaN68OUBgHevWrQOgZ8+eZdqWN688W7Zs4ccff+SYY46psF3RZx111FFllhVdMb927drAZ//2t79l2rRptG7dmsGDBzN+/HgWLFgQUk31iYK+Ar+b0CgwPnH7T9k3Y56P1YhItCmvV7/8/HxGjhzJ5MmTGTduHK+88grvv/8+H374IWPHjgUoc9HZ4cTGxpY734W4U3O49xdfR6jrqkjROip7BnxVP+uBBx5g5cqVPPjgg3Tq1ImJEyeSkZHBnXfeWe1a6yIFfQXOurgRvZtsAeAAjXji9tU+VyQSJZyru4PP5s2bx/Lly7nzzju57777uOCCCzj99NM59dRTOXTokN/lldGlSxcAVqxYUWZZefPK065dOxo0aMDChQsrbJeeng7A0qVLyyxbtsx70mnpoxDdunXjlltuYcqUKWzZsoXjjjuOCRMmBI6KREKnSwr6CsTEwO23BG+1++eik8jZ+IOPFYlItCvaiy6997pgwQLeeecdP0qqUOfOnenbty9TpkwJnLcHr4vi4lfmVyQuLo6f/exnLFy4kEmTJpVZXvRbdOjQgYEDBzJ16lRWrlxZYnnRLXw//elPAe9Wvby8vBLrSUlJoWfPnjjn2Lt3LwANGzYEYPfu3aF+5TpHt9dV4pI/dOZP9+5gS24aW2nLpFumcfXr5/hdlohEqX79+tGjRw8mTJjA3r176d69O8uXL+epp56iX79+dfIc80MPPcSZZ57JkCFDuP7660lNTWXy5MmBveVQ9prvv/9+PvvsMy6//HLeeecdhg4dSn5+PgsWLCAuLi5w29y//vUvRo4cydChQ7nhhhtIS0vjzTffZMaMGVx99dUMGzYM8J5VcOutt3L++efTo0cPUlJSmDNnDi+88ALDhw+nU6dOgPd7p6Sk8MgjjxAbG0vjxo1p06YNJ510Ug39WuGnoK9EYiLcev4mfjfZuwr/8Xc6cvWhQxAf73NlIhKNEhISePfdd7n99tv573//S1ZWFkcffTSTJ09m5syZdTLoTzvtNN555x3+9Kc/8be//Y2mTZtyySWXcN555zF8+HCSk5MrXUeLFi2YM2cOEyZM4I033mDKlCk0atSIo446iltvvTXQbujQocyaNYu77rqLRx99lKysLLp168aDDz5Yot2gQYMYPXo0M2bM4Pnnn8c5R8eOHbnrrrv49a9/HWiXmprKiy++yF133cUtt9xCTk4OZ5xxRr0KegvHhRJ+y8jIcPPm1dyFcnt+yKVt63yy8f4YF/79Q465/bQa+zyRaLF8+fISt0FJdJk0aRKXXXYZr7/+Ouedd57f5dQpofzbMLP5zrmMytalc/QhaNoqgTF9g/eV/uefBypoLSIixRUUFJCbm1tiXk5ODv/85z9JTEws07GPhJcO3Ydo3J/b8uLPvPFJW07mgQXLSBrYx9+iRETqgf3799O7d28uvfRSevTowY4dO5g8eTJLly7lrrvuCtx7LzVDQR+ik8e2oEuDH1j3Yyv20Iy3/+8Dxk5R0IuIVCY5OZnTTz+d1157LdC5Tq9evXjiiSe49tprfa4u8inoQxQTA5eftZu/vNIKgKkfpDLWuVp/JKaISH2TmJjIc88953cZUUvn6Ktg7G+7BMbfPjCc7AXLfKxGRESkcgr6KuibkUT3hlsBOEgqHzzwjc8ViYiIVExBXwVmMPa0YGcRU6Y3qKC1iIiI/xT0VTT2N50C49P2DSdvzQYfqxEREamYgr6KBgxNpl3iDgD20pQ5Ty7yuSIREZHDU9BXkRmMOmZbYPr9t3J8rEZERKRiCvpqOONnTQLj01d2hTrYNaSIiAgo6Kvl1CvbE0M+AHMLBrLrg/k+VyQiIlI+BX01NG1mDE7zLsJzxDDzuTU+VyQi0e6EE06gW7duJeZddtllxMWF9ly01atXY2ZMmDAh7LXl5eVhZlxzzTVhX7dUTkFfTcOHBM/Nf/GlHjAoIod3wQUXYGYsWnT4i3edc3Tp0oUmTZqQlZVVi9WFx+7du7n77rv5/PPP/S6lznnttdf4y1/+4tvnK+ir6cSfpgXGv9jcFUr1zCQiUmTcuHEAPPPMM4dt88knn7B+/XouuuiikPpnD8UzzzzDjz/+GJZ1VWb37t3cc8895QZ9XFwcWVlZTJw4sVZqqWsU9PXUsHNbBMbnuwEcnKWn5IlI+U4//XQ6dOjApEmTynTXWqRoI6BooyAc4uPjSUxMDNv6jkRSUlLIpxEkvBT01dSsGfRt8j0A+cTx1Uvr/S1IROqsmJgYfv7zn7Nr1y7eeuutMsv379/Pa6+9Rt++fRk8eHBg/osvvsg555xDx44dSUxMJC0tjTFjxrBkyZKQPvdw5+g///xzhg4dSnJyMq1bt+bmm28ud88/Ly+PCRMmcOKJJ9KqVSsSEhLo1KkTN954I7t37w60++ijj+jevTsAf/7znzEzzCxwzUBF5+ifeOIJBgwYQHJyMk2aNOGMM85g9uzZZeooev/MmTM58cQTSUlJoUWLFlx77bVVOmoxf/58xo4dS6tWrUhMTKRjx45ccsklrFu3rsp1AUybNo3hw4fTokULkpOT6dSpE+effz6rV68GvGsnJk2aRH5+fuB3MTP+97//hVzzkdLm1RE4sd9+lhQepfriswJO9bccEanDrrrqKiZMmMAzzzzD2LFjSyx76aWXyMzMLLM3/9hjj9GqVSuuu+46WrVqxerVq3nyyScZOnQoCxcuJD09vcp1zJ49m9NOO40mTZowfvx4GjVqxOTJk5k5c2aZttnZ2Tz44IOcf/75nHfeeTRo0IA5c+bw5JNPMmvWLObOnUt8fDx9+/blH//4B7/97W8ZO3Ys5557LgCpqakV1vKb3/yGhx56iCFDhnDvvfeyb98+nnjiCU4++WTefvttTj/99BLt58+fz+uvv864ceO47LLL+Pjjj3nqqaeIi4vj8ccfr/S7v/nmm1xwwQWkpqZyzTXXkJ6ezrZt25g+fTrLli2jS5cuVaprxowZnHfeefTr14877riDxo0bs3nzZj766CPWrl1Lt27duPPOO7nnnnv46quvSvTgN2zYsErrDRvnXL0fBg0a5Pww+e8bHTgHzp0S/7lzBQW+1CFSXy1btszvEmrViBEjXGxsrNu8eXOJ+UOGDHEJCQlux44dJeYfPHiwzDq+/fZbFx8f72666aYS84cNG+bS09NLzLv00ktdbGxsiXmDBw92CQkJbtWqVYF52dnZbuDAgQ5wf/3rXwPz8/PzXWZmZpkaJk6c6AA3derUwLxVq1aVeX+RQ4cOOcCNGzcuMG/p0qUOcMOHD3e5ubmB+d9//71LTU11Xbt2dfn5+SXeHxMT4+bOnVti3aeffrpLSEgot87iDhw44Jo1a+ZatWrltmzZUmZ50WdVpa6bbrrJAW7Xrl0VfnZ5/x0qE8q/DWCeCyEjdej+CBw/tl1gfP6hfhSsWOVjNSKRw6zuDkdi3Lhx5Ofn88ILLwTmfffdd3z11VeMHj2aFi1alGjfoIHXcZZzjv3797Nz505at25Nt27d+Prrr6v8+Vu2bGHu3LmMGTOmxK14iYmJ3HrrrWXax8TEBC4MzM/PZ+/evezcuZMRI0YAVKuGIm+88QYAv//974mPjw/Mb9++PVdccQVr165l8eLFJd5zwgknkJGRUWLeiBEjyM3NZcOGivsdee+999i9eze33347bdq0KbM8JiamynU1btwYgClTppCfnx/S9/aDgv4IdOwcQ4t4rze7/TRmzbsrfK5IROqyMWPG0KRJkxJX3//3v/8F4Oqrry7Tfv78+fzkJz8hNTWVxo0bk5aWRlpaGsuXL2fPnj1V/vy1a9cC0KtXrzLL+vTpU+57XnrpJQYPHkxycjJNmzYlLS2NHj16AFSrhiJF58SPOuqoMsv69u1bot4iXbt2LdO2efPmAOzatavCz1u1ytsRGzBgQNjquvnmm+nfvz/XXXcdzZo146yzzuKxxx5j586dFX5GbVPQHwEzGNThh8D0vBn7KmgtItEuKSmJSy65hBUrVjB79uzA3n379u3LnI9ev349w4cP59tvv+XOO+/k9ddf54MPPuDDDz+kV69eFBQUVPnzvaO9YOUcmihaVtwrr7zCxRdfTFxcHI8++ijTpk3jww8/5J133gGoVg0VfV5lYmNjq72+ir57detKS0tj/vz5fPzxx9x4443s27ePW265hR49ejBnzpyQ11PTdDHeEcoYWMD7hRud879N4GJ/yxGJCNXIgHpj3LhxPP744zzzzDPs3r2bbdu28cc//rFMiE2dOpXMzEymT5/OiSeeGJjvnGPnzp2Bw8ZVUXTx3vLly8ssK2/eCy+8QEpKCp988glJSUmB+eVd9V9ZgB6ulqVLl9KpU6cSy5YtWwaUvwdfXT179gRg4cKFnHLKKWGrKzY2llNOOSWwzoULFzJ48GD+9re/8eabbwJV/23CTXv0R2jQqc0C4/O2tI3s/0OJyBEbOHAgxxxzDC+//DKPPfYYZsZVV11Vpl1R8Jfew5w4cWK1Dw23bduWjIwMXnvttcDtXwA5OTn885//LLeGmJiYEnvuzrlyH5PbsGFDgBK33VWk6Mr8Bx54gLy8vMD8zZs389xzz9G1a1f69esX2hcLwahRo2jWrBkPPPAA27ZtK7O86HeuSl3l/Xfo06cPiYmJJX6Hhg0bkp+fz/79+8P2fapCe/RHKOPM4BPyFuT3o2D1WmK6V/2WFxGJHuPGjeOmm27i/fff5+STTy73NrmzzjqLO+64g0svvZQbb7yRxo0bM2vWLKZPnx64Daw6HnroIUaOHMmwYcO44YYbaNy4MS+++GK5h6zHjh3Lm2++yYgRI7j88svJycnh9ddfJzs7u0zbVq1a0blzZyZNmkTnzp1p2bIlqampnHXWWeXW0adPH2677TYeeughTjrpJC688EL279/PxIkTycrK4vHHHw9cIBcODRs25Omnn+bCCy/k6KOPZty4caSnp7N9+3amT5/O+PHjOeuss6pU11VXXcX27ds57bTT6NSpE5mZmUyePJnMzEyuuOKKwGcPGTKEiRMncv3113PmmWcSHx/P8ccfX+aIQY0J5dL8cA1AB+ATYDmwFLilnDYGPAqsBhYDAytbr1+31znn3VHXMmF34Da77x5+17daROqbaLu9rsju3btdUlKSA9zzzz9/2HaffPKJGzp0qGvYsKFr0qSJO+uss9zSpUvLvZUu1NvritY7ZMgQl5iY6Fq2bOl+9atfuUWLFpV7e9y///1v16tXL5eYmOjatGnjrrvuOrd9+/Yyt8s559zs2bPd8ccf71JSUhwQqKe82+uKTJw40fXv398lJia61NRUd9ppp7mZM2eWaFPR+5966ikHuC+++OKwv2NxX375pRs9erRr1qyZS0hIcB06dHCXXnqpW7duXZXrevXVV93ZZ5/t2rVr5xISElxaWpo7+eST3WuvvVaiXV5envv1r3/t2rZt62JiYhzgXnjhhQrrDOftdeZq8VCzmbUB2jjnFphZKjAfOM85t6xYm58ANwE/AY4DHnHOHVfRejMyMty8efNqsPKK/SR9Be+t9c7/TDpnMpe8pTP1IqFYvnw5vXv39rsMkTonlH8bZjbfOZdRYSNq+Ry9c26rc25B4fgBvD37dqWanQsUbeJ+BTQp3ECoswb0D56/+vZbHwsREREpxbeL8cysMzAAKP3EhXbA98WmN1F2Y6BO6Ts8eEHeki3NfaxERESkJF+C3swaAlOBW51zpS9DLO8+hDLnF8zsWjObZ2bzduzYURNlhqzvKcEL8pbkdoc69rAEERGJXrUe9GYWjxfyk5xzr5XTZBPeRXtF2gNbSjdyzj3pnMtwzmWkpaWVXlyrevaOIQ7vNoz1dOHA3O98rUdERKRIrQa9eU8N+A+w3Dn30GGavQVcYZ4hwD7n3NZaK7IaEhKgZ5PgfZlLZ5S9R1NERMQPtX0f/TDgcuBbM1tUOO8OoCOAc24i8C7eFfergUyg7JMk6qC+nQ6wdK83vmReNkP8LUdERASo5aB3zs2k/HPwxds44MbaqSh8+vaL4eVvvPElqxL9LUakHnHO+f6IUJG6JNy3vesRuGHS98RiV95vT9OjcEVCEBsby6FDh/wuQ6ROOXToUIUd+FSVgj5M+p4c7Ef627zeUM6zlEWkpNTUVN+e/y1SV+3fv5/U1NSwrU9BHyZduhrJMd7zn7fTil2z1Te9SGWaNWvGnj172LlzJ7m5uWE/ZClSXzjnyM3NZefOnezZs4dmzZpV/qYQqVObMImNhe5NdrB4t3dn4MovfuD4830uSqSOS0xMpGPHjuzevZv169eTn5/vd0kivomNjSU1NZWOHTuSmBi+a70U9GHUo30miwt7Jly5OJvj/S1HpF5ITEykTZs2tGlTp590LVJv6dB9GPXsFbxyeMXaeB8rERER8Sjow6jnoODFEyu3N/GxEhEREY+CPox6DA1eeb8iqwNkZvpYjYiIiII+rHr2DR6uX0V3Clas8rEaERERBX1YNWkCLRP2AJBDEhtnb/K5IhERiXYK+jDr0WJPYHzFXD0IRERE/KWgD7OenXMC4yuX5flYiYiIiIKD+WU1AAAgAElEQVQ+7HocFTxPv2Jjko+ViIiIKOjDrudxwdvqVu5qrs5tRETEVwr6MOs5tHlgfEVeOuza5WM1IiIS7RT0YdY13YjFOze/kU5kfbva54pERCSaKejDLCEBOjXYGZheO2urj9WIiEi0U9DXgPSWBwPja745WEFLERGRmqWgrwHpnYK31a1ZrYvxRETEPwr6GpDeOyEwvmaLbrETERH/KOhrQPrAxoHxNXua+ViJiIhEOwV9DUjPaBoYX3OoI+zd62M1IiISzRT0NaBrt+DPup7O5K9c42M1IiISzRT0NaBhQ2iV6HVuc4gEvp+7zeeKREQkWinoa0h6s2AvdmsWqhc7ERHxh4K+hqS3D/Zit2bFIR8rERGRaKagryHdegR/2tUbE32sREREopmCvoak908NjK/Z2cjHSkREJJop6GtI+rHBXuzWZLaB3FwfqxERkWiloK8h6X2Ch+vXkI5bv8HHakREJFop6GtIixaQGvsjAAdJZceC732uSEREopGCvoaYQXrjXYHpNXN3+1iNiIhEKwV9DUpv82NgfM2ynApaioiI1AwFfQ1K7xocX7NOP7WIiNQ+pU8NSu+bHBhf80MDHysREZFopaCvQemDg13UrtmfBs75WI2IiEQjBX0N6lrsoTlrCzrDDz/4V4yIiEQlBX0N6tDRiCMPgG204ccl63yuSEREoo2CvgbFxUHnhjsC02u/3lFBaxERkfBT0New9LRgF7Vrv/2xgpYiIiLhp6CvYV075gfG16wq8LESERGJRgr6GpbeKz4wvmZzko+ViIhINFLQ17D0gY0D42v2NPWxEhERiUYK+hrWdXCwu9q1ue3hwAEfqxERkWijoK9hXbvHBsbX05n8VWt9rEZERKKNgr6GNWwIrRK8nusOkcD3c7b6XJGIiEQTBX0t6Npsb2B87cJ9PlYiIiLRRkFfC9LbBbuoXbM818dKREQk2ijoa0F6j+B5+jUb4ytoKSIiEl4K+lrQtV/DwPjaHakVtBQREQkvBX0tSD82eIvdmszWkKvD9yIiUjsU9LUgvU9iYHwN6bj1G3ysRkREoomCvha0agUpMVkA7KMJuxdt9LkiERGJFgr6WmAGXRvvCkyvnbfbx2pERCSaKOhrSXrrYBe1a5Zm+ViJiIhEEwV9LUnvGhxfs1Y/u4iI1A4lTi3p2ifYRe3abSk+ViIiItFEQV9L0gc3C4yv2dcCCgp8rEZERKKFgr6WpB8TfFDOGtcFtqpzGxERqXkK+lrSqRPEkA/AZtqRvUzd1YqISM2r1aA3s/+a2XYzW3KY5Seb2T4zW1Q43Fmb9dWkhATokOLdVueIYf3cHT5XJCIi0aC29+ifBUZV0uYL59wxhcNfaqGmWpPecn9gfM03B32sREREokWtBr1z7nMgap8W07VjXmB8zWpdjCciIjWvLp6jP97MvjGz98zsKL+LCaf0XgmB8dWbkn2sREREokVdC/oFQCfnXH/gX8Abh2toZtea2Twzm7djR/04390jo1FgfNWe5hW0FBERCY86FfTOuf3OuYOF4+8C8WbW4jBtn3TOZTjnMtLS0mq1zurqfmzTwPiqQ51h717/ihERkahQp4LezFqbmRWOH4tX366K31V/dOsR/LnX0YXc73SLnYiI1Ky42vwwM5sMnAy0MLNNwF1APIBzbiIwFvilmeUBWcBFzjlXmzXWpORk6Ji8nY1ZLSkglrWzt9FriN9ViYhIJKvVoHfOXVzJ8seAx2qpHF/0SNvLxo0tAVg5/wC9fK5HREQiW506dB8NenQ5FBhfuTzfx0pERCQaKOhrWY+jEwPjK3WLnYiI1DAFfS3rMSTYi92q3c0hci5BEBGROkhBX8t6HBe8xW5lfjr88IOP1YiISKRT0NeyTp2NOPMehbuFdhxcuMrnikREJJIp6GtZXBykp24PTK+atb2C1iIiIkdGQe+DHm2CPdetXJTpYyUiIhLpFPQ+6NE9eAHeylXmYyUiIhLpFPQ+6HFMSmB81daGPlYiIiKRTkHvgx5Dg/30rDzQBnJzfaxGREQimYLeBz36Bx+Us4IeuLXrfKxGREQimYLeB23aQMNY7yK8vTRl+5z1/hYkIiIRS0HvAzPo3Tx4W93y2Xt8rEZERCKZgt4nfTpnBcaXLc7zsRIREYlkIQe9meWb2bGHWTbIzNQVWxX07hsbGF+2Xp3biIhIzajKHn1FN3zHAuqdpQr6DAs+8375jjR1biMiIjWi0qA3sxgzK9r9jCmcLj40AM4EdtZopRGm94nBW+yW5XVX5zYiIlIjKgx6M7sLOATk4u2xzyqcLj7sB+4EXq3RSiNMl65GouUAsI027Plqhc8ViYhIJIqrZPmnha+GF+b/ATaVapMDLAPeDmtlES42Fno1/YFvdncEYPlnPzD0PJ+LEhGRiFNh0DvnPgM+AzAzBzztnNtcG4VFg94dM/lmtze+bEE2Q/0tR0REIlDIF+M55+4pHfJm1sfMzjeztuEvLfL1OTp45f3y1Qk+ViIiIpGqKrfXPWZmE4tNjwG+wTs3v8zMBtdAfRGt99DglffLtrfQlfciIhJ2Vbm97kxgdrHpe/DOy/cH5gB3hbGuqNDnxOaB8eV53WDrVh+rERGRSFSVoG8NrAcws/bAUcC9zrlvgUcB7dFXUbfuRpx5T8XbQGcOzP3O54pERCTSVCXos4CiztNPwrutbl7h9EEgNYx1RYWEBOjVZFtgesknO3ysRkREIlFVgn4BcKOZ9QVuBD50zhUULusC6LhzNfTrciAwvnie+qUXEZHwqkrQ/xEYgncBXk/gr8WWnYd3nl6qqN8xwSvvF69O8bESERGJRJU9MCfAOTfXzDoCvYBVzrn9xRY/CawKd3HRoN+IFvBfb3zxjjZQUAAx6lRQRETCo0qJ4pz70Tk3v1TI45x7xzm3MrylRYd+JwVvsVtccBRuzVofqxERkUhTpaA3s6PNbIqZ7TCzPDPbbmavFJ63l2po285oFu9tN+2nMRs/0vaSiIiET1UemDMY+Bo4Be/++QeAd4ARwNdmNqhGKoxwZtCvdfBq+8Wf7PKxGhERiTQhn6MH7gWWACOdc4FLxc0sFfiocPnp4S0vOvTrk8en33vj3yyGc/wtR0REIkhVDt0PwXtAzoHiMwun7weOD2dh0aTfsOAjCBZvbOJjJSIiEmmqEvSVPYhdD2qvpn6ntgyML87qDjt3+liNiIhEkqoE/dfAHYWH6gPMrAHwe+CrcBYWTY7qH0cM+QCsojuZX3/rc0UiIhIpqnKO/g7gU2CDmb2N9yS81sBZQAreY3GlGlJSoGeTH1i+ty0FxLJo+jaGnuV3VSIiEgmq0h/9HLzz9B8DZwC3AaMKp49zzs2tkQqjREaP4KMJ5n2V52MlIiISSSoMejOLMbNziu6Td84tds6Ndc61cs7FO+da4T0Kt3Mt1BrRBg9NCIzPW9XYx0pERCSSVLZHfxkwGfixgjYHgMlmdnHYqopCGWe3DozP29cNDh70sRoREYkUoQT9M865dYdr4JxbD/wHuDKMdUWd/senEIt3yP47enFg5jc+VyQiIpGgsqAfCHwQwno+AjKOvJzolZICRzX1evp1xLDwre99rkhERCJBZUGfCuwJYT17CtvKEcjoFTxcP2+2+qYXEZEjV1nQ7wQ6hbCejoVt5QhknBTsj37u6qYVtBQREQlNZUE/k9DOvf+8sK0cgYxz2gbG5/3YC3bsqKC1iIhI5SoL+n8CI83sYTNLKL3QzOLN7BG8HuwerokCo0m/QfHE2yEAVtOdPZ8s8rkiERGp7yp8Mp5z7ksz+w3wIHCpmX0AbChc3Ak4DWgO/MY5p0fgHqHEROiXtpX52zsCsOCdrYy80OeiRESkXqv0EbjOuX+a2QJgPPBTILlwURbeI3Hvc859UWMVRpmMo7KZv90bn/NVASP9LUdEROq5kJ5175z7HPjczGKAFoWzdznn8mussih17KmpPPGJNz57XRtwDsz8LUpEROqtqvReh3OuwDm3vXBQyNeAYWOCT8ibfSiDguUrfKxGRETquyoFvdS8Hj2NFgn7ANhNc1ZMXeJzRSIiUp8p6OsYMxjabXtgetZ7+ytoLSIiUjEFfR007OTgnYyzljTysRIREanvFPR10LCxbQLjsw70g61bfaxGRETqMwV9HTTo+AQSzHvW/Sp6sOPduT5XJCIi9ZWCvg5KSoKMdsG9+NlvbK+gtYiIyOEp6OuoYccF7178fE6Sj5WIiEh9pqCvo066oGVg/OPtR8HevT5WIyIi9ZWCvo468cyGxJIHwCIGsOutWT5XJCIi9ZGCvo5q1AiObbs5MP3J5G0+ViMiIvWVgr4OG3FS8Dz9x1+l+FiJiIjUV7Ua9Gb2XzPbbmblPtfVPI+a2WozW2xmA2uzvrpm5OVtA+Mz9g6ELVt8rEZEROqj2t6jfxYYVcHyM4HuhcO1wL9roaY66/hTkkiKyQFgJT3ZNOUrnysSEZH6plaDvrC7290VNDkXeN55vgKamFmbCtpHtKQkGNY5uBf/8au7fKxGRETqo7p2jr4d8H2x6U2F86LWiNNiA+MfL2ji9U8vIiISoroW9FbOvHKTzcyuNbN5ZjZvx44dNVyWf4qfp/8o83jct+q2VkREQlfXgn4T0KHYdHug3CvQnHNPOucynHMZaWlptVKcHwYdF0eT+IMAbKY93z79tc8ViYhIfVLXgv4t4IrCq++HAPucc1HddVtcHJwxcGdg+p0383ysRkRE6pvavr1uMvAl0NPMNpnZODO73syuL2zyLrAWWA08BdxQm/XVVWdf2Tww/vbGo2Hnzgpai4iIBJmLgIu7MjIy3Lx58/wuo8bs3Amt0vIpIJYY8vnh8ddo8csL/C5LRER8ZGbznXMZlbWra4fupRwtWsCQjt4ZjAJimf7cDz5XJCIi9YWCvp44+7y4wPjbC9rCoUM+ViMiIvWFgr6eOHtcq8D4+4dO4dBns32sRkRE6gsFfT3R92ijQ0PvoYJ7acqsx7/xuSIREakPFPT1hBmcPSIzMP3a+w0gP7+Cd4iIiCjo65Wxvwo+9n9q5igKvpjlYzUiIlIfKOjrkeGnxJKWdACALbRj9qORe0uhiIiEh4K+HomLgzGnHQhMvzK9kQ7fi4hIhRT09cyFNwWvvp+adSYFn33hYzUiIlLXKejrmeGnxJKWHDx8P+vR+T5XJCIidZmCvp6Ji4Pzix2+nzS9GeTk+FiRiIjUZQr6eujS24KH71/OOY/s1971sRoREanLFPT10LDhsXRpEnx4ztv/+M7nikREpK5S0NdDZnDF5cHp5xf0ha1b/StIRETqLAV9PXX5Lc0C4+8xiu0TX/OxGhERqasU9PVUejoM6+51V5tHPP+beBCc87kqERGpaxT09djPb24cGH9i+3m42V/6WI2IiNRFCvp67OKrkmgU73V0s5KefHznp/4WJCIidY6Cvh5r0ACuGHMwMD3xk56wbZuPFYmISF2joK/nrv9zy8D4G240Wx980cdqRESkrlHQ13NHHQUn9toOeBflPT0xDw4d8rkqERGpKxT0EeCXdwRvtZt48FJyX3nDx2pERKQuUdBHgDEXxtGqgXeufgvtePnPS3SrnYiIAAr6iJCYCDfdFJz+x7oxuBkf+1eQiIjUGQr6CPHL2xuSEuf1YreY/nx4+wc+VyQiInWBgj5CNGsG4y7NDkz/fdFpMGeOjxWJiEhdoKCPIL++qzExVgDADE5l9m/1/HsRkWinoI8gXbrApWfvC0zf/cUIWLTIx4pERMRvCvoI8+cHmxJr+QB8yOnMumGSzxWJiIifFPQRpnt3uOyc/YHpu748A2bN8rEiERHxk4I+Av35oeBe/QxO5dNfvqz76kVEopSCPgKlp8MV52cGpn/z7ZUUvPOejxWJiIhfFPQR6p6HUkmKzQVgAYOYdO1nkJvrc1UiIlLbFPQRqkMHuO3GYLDfsfVXZD74bx8rEhERPyjoI9j4CQ1p2fBHADbRgfvvzoKtW32uSkREapOCPoKlpsKEBxID0/fn3sqqGx72sSIREaltCvoId/Uv4hjc03uITg5J3PTGCNwXM32uSkREaouCPsLFxsK/JzXG8B6N+z6jePnCqZCV5XNlIiJSGxT0UWDQIPjlFT8Gpn+17Y/8cNv9PlYkIiK1RUEfJe79Vyodmh0EYBctuGHi0bhZs32uSkREapqCPko0agRPv9ggMP0a5/PS2CmQmVnBu0REpL5T0EeR088wfnHxwcD0Ddv+zIZr/upjRSIiUtMU9FHmHxMb0qn5AQD20pRLJ59F3ktTfK5KRERqioI+yjRqBC++2TDQ6c0sTuCeK9fCunU+VyYiIjVBQR+Fhg4z7vnjocD0hNzf8capj+mWOxGRCKSgj1Lj707i1GP3BaYvX3s3yy68W93ZiohEGAV9lIqNhZfebUyX5vsBOEgq5759Dbv+NtHnykREJJwU9FGseXN4Y0YqKXE5AKymO+f++WiyXn3b58pERCRcFPRRrl9/4/kXYgKPyJ3FCVx6UT75s77yuTIREQkHBb1w/kXxPPSX4CNyXy84lxtOXYFbucrHqkREJBwU9ALArX9O5bar9gSmn8y+kl9nfIHbsNHHqkRE5Egp6CXggaebctmonYHpRw5czR3HvIP7fpOPVYmIyJFQ0EtATAw8M60FY0/YFph3395fcuvRMyjYtMXHykREpLoU9FJCXBxMmtGac44Nhv2j+67k6qO+Im+tDuOLiNQ3CnopIyEBpnzRmguGbg7Me27/GM4/6jt+nP+dj5WJiEhVKeilXAkJMPnzdlxzavAZ+G9ln87w47LZPG2Bj5WJiEhVKOjlsGJj4ckPuvD7n60PzFuQfwzHnduKr+771Le6REQkdAp6qZAZ3PdSZ574w3piyQNgs2vH8D8M5ZEzp+PyC3yuUEREKqKgl5Bc+3+dmf7MNprEeB3hHCKBW6eP4sKOX7Fv80GfqxMRkcNR0EvITv15exbMc2Q0WhGYN2XLUDK67mLulA0+ViYiIoejoJcq6TKgCTO3pHNDv5mBeatzO3H8Be24+6ffcOhQBW8WEZFaV+tBb2ajzGyFma02s/HlLP+5me0ws0WFwzW1XaNULLFBHP/vmxOYfP1nNOQAAPnEcc8b/RncagNffqhD+SIidUWtBr2ZxQL/DzgT6ANcbGZ9ymn6snPumMLh6dqsUUJ30b9P4ptp33Ni8tzAvG/2dGLo6Q35xajv2bXLx+JERASo/T36Y4HVzrm1zrlc4CXg3FquQcKo69l9+GRrbx4Y9BLJZAbmP/1+B3q2O8DTj2ZSoAvzRUR8U9tB3w74vtj0psJ5pZ1vZovNbIqZdShvRWZ2rZnNM7N5O3bsqIlaJUSxjRvy23kXsfzxTzk34b3A/F05qfzilhQGd9vNjI+cjxWKiESv2g56K2de6QSYBnR2zvUDPgKeK29FzrknnXMZzrmMtLS0MJcp1dHplz/hjU0ZvHXC3+lM8Il6C9Y149TTjFEnHOCbb3wsUEQkCtV20G8Ciu+htwdKdIvmnNvlnMspnHwKGFRLtUk4pKVxzhe/Y+mkb/hTg4dIIiuw6P1ZqRxzDIw+M5eZMytYh4iIhE1tB/1coLuZdTGzBOAi4K3iDcysTbHJ0cDyWqxPwiTlkvP46+ZxrL7mfsbZf4ghP7Bs2vQETjwRhh2fz1tvoXP4IiI1qFaD3jmXB/wKeB8vwF9xzi01s7+Y2ejCZjeb2VIz+wa4Gfh5bdYoYdS4Me2eupunFx/H4oxx/JTXMIKpPvurWM49F47uW8Czz0Jurn+liohEKnOu/l8klZGR4ebNm+d3GVIR52DqVL777dM8sOECXuByDpFQoknLlo5x44xf/AK6dPGpThGResLM5jvnMiprpyfjSe0wg7Fj6bVqGv+ZmMf6tGO5nb+Tyv5Ak+3bjXvvhfR0xxlnwPPPw4EDPtYsIhIBFPRSu+Lj4brraLtuFn+/t4CNzQZwL+Npx6ZAE+eMDz6AK6+EVq3gootg2jQd2hcRqQ4FvfijQQMYP54mGxcz/uHWrG8zlDc4l1G8V+I8flYWvPwyjB4NbdrA9dfDF1/oAj4RkVDpHL3UDTk58Oyz8NBDbFyZxWQu5kUuYTH9y23erh2cfbY3jBwJycm1W66IiN9CPUevoJe6paAAPvgAHn0U3nuPJRzFJC7lRS5hI53KfUtyMpx6qhf6P/kJtG9fyzWLiPhAQS/138qV8Nhj8MwzFBz8kdkMZRKX8ioXsIsWh31bt25w8snBoV15D1kWEannFPQSOfbv907UP/00zJlDHrF8yfFM4xymcQ7f0bvCtyv4RSQSKeglMi1ZAv/5D0yeDD/8AMBq0nmbs3mbs5nFMLKp+IR9UfCfcAIMGAC9e3s3A4iI1CcKeolseXnwySde4L/2GuzbB0AOCczhWD7lZD7lZGYztNLgT0iAo46CY44JDv37Q+PGtfFFRESqR0Ev0SMnB957zwv9adO8e/KKFhUPfjuF2TaM7ILEkFbbpUsw+AcM8F7bt/ee/SMi4jcFvUSnzEyYMcML/Lffhq1bSywuHvwLkoexKHYQ6w+G3s1xs2Yl9/yPPto7FdCwYbi/iIhIxRT0IgUFsGCBF/rTpsHCheU220MTvqE/i1qNYlHTU1iU3ZOlmxqTlxf6rnubNl7gd+8eHLp184YGDcL1hUREghT0IqVt2uTt5b/zDnz6KRw8eNimOSSwvPNPWNRxNIuSjmPR3i4sWpHEvn1VP27ftm0w+Is2Arp2hc6doUmT6n8dEYluCnqRihw6BPPmeYf5Z8yA2bMrfZi+a5HGhv6jWdRmFAtjBrFoZ3u+Wx3P2rXetYHV0bgxdOrkhX7Hjt6tf23blhwaN9Z1ASJSloJepCoyM2HWLC/0v/gC5s71NgYqYgY9epA3YDAbu5zEqiaDWWU9WLUpmVWrYNUqWLcO8vOPrLTkZC/wy9sIKD7oFIFIdFHQixyJrCwv7L/4whu++ipwC1+lunWDQYNg0CAO9RvEhhaDWLW9cSD8V6+G9eu9ITs7fCU3blz+BkDLltCiRXBo3lx9A4hEAgW9SDgVFMCKFV7gf/ml97psWei76507Q58+3tC7N/Tpg+vVm+05jdmwwQv9TZtgyxZv2Lw5+FrsbsGwSUkpGf6lh+bNy04nhnZXoojUEgW9SE3LzITFi70r++fP94alS6t2wr5t20Dwl3hNSwMznPOeAFy0AVB6Q6D4UNmZhiOVmgpNm5YcmjQp+9q4cXBo1Cg4Hhtbs/WJRBsFvYgfsrPh22+90C/aAPj226qncPPmJYO/aPwwT+xxDnbtKhv+W7bAzp1lh5reKChP48bexkCjRt5GQ4MG3tCwYfnjlS1r0ECPLpbopqAXqStycrye+JYv9w73F72uXFnplf5lNGwYDP7evSE93btXr2tXL0VD4BwcOBAM/V27yt8YKL38SC8qrAkJCVXfQAhlWUoKxMT4/e1EKqagF6nr8vJg7Vov+ItvBCxfDj/+WPX1NWkSDP3iQ5cu3r17CQnVLrWgwLsWcc+eksPevd5QNL5nj9du/37vtWjYv7/aH+2blJTDbyCUnk5J8S5wLBoSE4NDUlL546WntWEhVaWgF6mvCgq8K/OK7/0Xve7ZU711xsRAhw5lNwI6d/bmt25doyfR8/ODGwIHDnjPKvrxx+Br6fFQlxUU1FjJtS4+PrQNglCXhdouPt7bBiw9xMbq+Q11nYJeJNI4B9u3B4N/xQrvRv21a73hSC7Pj4vzbtTv2NEL/g4dguNFr02b1qn/8zvnnRWpaAOhuhsTmZl+f7u6obwNgNIbCgkJ3sZCdYbi6y2aLq9N0XhcXMllRdPlvcbF1ak/1xqhoBeJJs7BDz94gV88/IuGzZu9NkciJaXkhkD79t5D/osPrVtHxBVyBQVe2IeyUXDwoLeNVTRkZ3sbIEWvpccPt0zCLza27EbA4TYMYmODQ9GGQvGNhqKh+PLY2JJtKtroiI/3bqa54ILwfT8FvYgEZWfDhg1lNwA2bvSGnTvD91ktWpTdAChvSEkJ32fWc855d0IcbuMglA2F6ozn5pYdDh3yXuvixZf13dFHe3fkhkuoQR8Xvo8UkTorKQl69vSG8mRmetcFfP+9F/zff19yfOPG0C8QLLpc/9tvK27XqFH5GwBt25acbtQo4o/BmgUPYdcV+fnB0C/aACi9sZCd7c0/kiE311tX6XnljRcNeXmVv9bFDRW/DnYp6EXE27vu0cMbyuOcdzVd8eDfsgW2bi05bN8e+imC/fu9YcWKitslJ3vHPFu2LP+19DwdKQiLosPYSUl+V1I9znmBX3ojoLwNg/z84MZB0XjpoXS74vNKr7O8DY+8PO9slx8U9CJSObPg4/D69z98u7w8L+xLbwCUHrZtC/2pPVlZwVMMoUhJqXhjoHlzaNYsODRtGhHXFUhJZsFz8tHet4OCXkTCJy4u2JtORQoKYPfuyjcItm6t+iXwmZnBXoNClZpaMvwrGoo2FJo2rb+7uxJVFPQiUvtiYoI95hx99OHbFT3Gb8cOb9i+vex46deqPm0QvM84cMC7YLEqUlJC30AoPqSkRPx1B1J3KOhFpO4y8y7Ga9TIe9xvZYp6Aapoo2DPHu9oQtGwZ0/1n7yTmRm8kLEqEhIq3xho2rT8HoIaNtRj9KRKFPQiEjnMgsHYrVto7yko8DYOiod/KMOuXVXrqbC43FzvOoVt26r+3qKNn9LdA5bXZeDh5jVq5J1mkaig/9IiEt1iYrx+Aor6CgiVc97TcqqzgXAkT8hxLtiJwJFISan+RkJqarALQp2CqPMU9CIi1WEWDLxOnar23qysyjcIinoIKt1LUHU6PCpP0WmHrVurvw4z71RC0e9Q2VBZ26QkbTjUAAW9iEhtS072+hZo167q783LCz6DoHgXgeV1G3i4+fv3H/kjkSF4seSBA33Ft3MAAAmFSURBVEe+LvBu3D/cBkLRUNR1YPHxiuapz2EFvYhIvRIXF7xgr7oKCrwjA6FsFJSet39/MNyPpCOl8hR1c7h3b3jXm5JSvY2E0n0Tl+67OD6+XhyBUNCLiESbmJjg3vKRPK4tL8+7TqEo+A83hNLmwIHq3RoZiqLTFOEWG3v4DYHSGwUpKd5vfcMN4a+jEgp6ERGpnri44IWM4ZCbe/gNhOLdBh48GNp4Tfc5nJ8fPI0Sij59FPQiIhLFEhK8Jw82bx6+debnB/scLm9DIJSNhqKhdN/FVe05p0GD8H2vKlDQi4hI5Cp+gV+45eaW3QA43PiPP0KrVuGvIQQKehERkeoo6lu4aVO/K6lQdN9zICIiEuEU9CIiIhFMQS8iIhLBFPQiIiIRTEEvIiISwRT0IiIiEUxBLyIiEsEU9CIiIhFMQS8iIhLBFPQiIiIRTEEvIiISwRT0IiIiEcycc37XcMTMbAewIYyrbAHsDOP6opV+xyOn3/DI6Tc8cvoNwyPcv2Mn51xaZY0iIujDzczmOecy/K6jvtPveOT0Gx45/YZHTr9hePj1O+rQvYiISART0IuIiEQwBX35nvS7gAih3/HI6Tc8cvoNj5x+w/Dw5XfUOXoREZEIpj16ERGRCKagL8XMRpnZCjNbbWbj/a6nrjKzDmb2iZktN7OlZnZL4fxmZvahma0qfG1aON/M7NHC33WxmQ309xvUHWYWa2YLzeztwukuZvZ14W/4spklFM5PLJxeXbi8s5911yVm1sTMppjZd4V/k8frb7FqzOzXhf+Wl5jZZDNL0t9ixczsv2a23cyWFJtX5b87M7uysP0qM7sy3HUq6Isxs1jg/wFnAn2Ai82sj79V1Vl5wG+cc72BIcCNhb/VeGCGc647MKNwGrzftHvhcC3w79ovuc66BVhebPp+4OHC33APMK5w/jhgj3OuG/BwYTvxPAJMd871Avrj/Z76WwyRmbUDbgYynHN9gVjgIvS3WJlngVGl5lXp787MmgF3AccBxwJ3FW0chIuCvqRjgdXOubXOuVzgJeBcn2uqk5xzW51zCwrHD+D9j7Ud3u/1XGGz54DzCsfPBZ53nq+AJmbWppbLrnPMrD1wFvB04bQBI4AphU1K/4ZFv+0UYGRh+//f3v3GyFXVYRz/PrEKLQqCSsEWtU0ajTGR+gJRTP1fTW0kJihiqxaNwT8vNFFILIlG0qBRY/SFYtSGP23RgDZAUGMRUExMEDAEG6HtogS2tBQorZHGUtzHF+cM3J2dbWe22+x65/kkNzP33DN37p6czW/uub97z1CTdCKwDFgPYPsZ2/tIXxzUHGCupDnAPGAX6YuHZfsOYG9X8aD97n3ALbb32n4KuIWJPx6OSgL9eAuARxrro7UsDqMO2y0F7gTm294F5ccAcGqtlrbt7fvAJcBYXX8ZsM/2s3W92U7PtWHdvr/WH3aLgceBK+slkJ9JOoH0xb7Z3gl8F3iYEuD3A/eQvjgVg/a7Y94fE+jH6/WLNLclHIakFwO/Ar5k+1+Hq9qjbKjbVtJKYI/te5rFPaq6j23DbA7wJuAK20uBp3l+uLSXtGOXOlR8LrAIeCVwAmWouVv64tRN1mbHvC0T6McbBc5orC8EHp2hY5n1JL2QEuQ32d5cix/rDIPW1z21PG070TnAByU9RLlM9C7KGf5L6/ApjG+n59qwbj+JicOGw2gUGLV9Z13/JSXwpy/27z3AP20/bvsQsBl4K+mLUzFovzvm/TGBfry7gCU10/RFlGSUm2b4mGalej1uPXC/7e81Nt0EdLJGPwnc2Cj/RM08PRvY3xneGla2v2p7oe3XUPrabbZXAbcD59Vq3W3Yadvzav2hP4uyvRt4RNJra9G7gb+TvjiIh4GzJc2r/9udNkxfHNyg/e53wHJJJ9eRleW1bPrYztJYgBXAduBB4NKZPp7ZugBvowwv3QfcW5cVlOt0twI76usptb4odzQ8CPyNkt0743/HbFmAdwA31/eLgb8AI8D1wHG1/Pi6PlK3L57p454tC3AmcHftjzcAJ6cvDtyG3wAeALYCG4Dj0heP2GY/p+Q0HKKcmX96Kv0O+FRtyxHgwuk+zjwZLyIiosUydB8REdFiCfQREREtlkAfERHRYgn0ERERLZZAHxER0WIJ9BEtIGmNJE+y7JvhY7tK0uhMHkPEMJtz5CoR8X/kw5T7eZue7VUxIoZDAn1Eu9xre2SmDyIiZo8M3UcMkcYQ/zJJN0j6t6QnJf1Q0tyuuqdLukbSE5IOSrpP0uoe+1wkaYOk3bXePyT9oEe9pZL+JOmApB2SPtu1/TRJV0t6tO5nl6SbJZ3ava+I6F/O6CPa5QWNSUg6xmyPdZVtBK4DfgScBXyNMmPZGoA6zesfKY+SXUuZRnM1sEHSPNs/qfUWUR6BegD4OuWxn2dQntfddCJwLWXSnsuAC4ErJG2zfXutswF4NXBx/b75lGeuz5tKQ0REkUAf0S4P9Cj7NbCyq+w3tr9S32+RZOAySZfb3k4JxEuAd9r+Q633W0nzgXWS1tv+L+X56HOBN9puzrh1ddf3vQT4fCeoS7qD8mPgAsrEKQBvAdba3tT43PV9/dURMakE+oh2+RATk/F6Zd1f17X+C2Ad5ex+O7AM2NkI8h0bgSuB11Mm5lhOmYznSNNqHmicuWP7oKQdwKsade4CLq6zp90GbHUm44g4agn0Ee2ytc9kvMcmWV9QX0+hzMrVbXdjO5SZuvq5de6pHmUHKbOgdZxPGf6/hDLEv0vSj4F1PS49RESfkowXMZzmT7K+s77uBU7r8blO2ZP19Qme/3FwVGzvsf0F2wuA1wFXUS4NXDQd+48YVgn0EcPpI13rHwXGKIl1UBLxFko6p6vex4A9wP11fQuwUtLp03lwtrfZXksZCXjDdO47Ythk6D6iXc6U9PIe5Xfbbj44Z4Wk71AC9VmUIfNraiIelLPpLwKbJV1KGZ5fBbwXuKgm4lE/9wHgz5IuB0YoZ/jvtz3hVrzJSDoJ+D2wiZJQeAg4l5L1v6Xf/UTERAn0Ee0yWZb6KyjD7B2rgS8DnwOeAX4KdLLwsf20pLcD3wa+Rcma3wZ83PbGRr2HJL2Zksj3zVpvJ3DjgMf9H+CvwGcot9iN1e9bZXvQfUVEg5LUGjE8JK2hZM0vyRP0IoZDrtFHRES0WAJ9REREi2XoPiIiosVyRh8REdFiCfQREREtlkAfERHRYgn0ERERLZZAHxER0WIJ9BERES32Py8gqHnDFMmDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot cost\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training cost', 'Validation cost'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Cost',fontsize=16)\n",
    "plt.title('Cost Curves',fontsize=16)\n",
    " \n",
    "#Plot accuracy\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history.history['acc'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_acc'],'b',linewidth=3.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves',fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our training cost and accuracy only get better the more the model sees the data, while for the validation set, we've plateaued! This is a clear indicator that our model is overfit. In fact if you crank up the Epochs to 1000 or 10000, you'd see the test cost and accuracy get worse and worse while the training cost and accuracy get better and better. So, more Epochs in not necessarily a good thing! One fix for this overfitting problem is called dropout. You can think of dropout as additional neural network regularization, on top of the regularization we already implemented.\n",
    "\n",
    "In a nut shell, dropout randomly turns off a percentage of the units or neurons in a given layer which discourages overfitting. Since the hidden size of the neural network in this assignment is very small, we'll only shut off 15% of the neurons. In neural networks with larger hidden sizes, the dropout number is usually much higher, like 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    " \n",
    "model_dropout = Sequential()\n",
    "model_dropout.add(Dense(25, activation='sigmoid', input_shape=(400,)))\n",
    "model_dropout.add(Dropout(0.15))\n",
    "model_dropout.add(Dense(10, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can just configure a new model in the same way as before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "4000/4000 [==============================] - 0s 113us/step - loss: 2.3313 - acc: 0.0877 - val_loss: 2.3314 - val_acc: 0.0500\n",
      "Epoch 2/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 2.3227 - acc: 0.0865 - val_loss: 2.3182 - val_acc: 0.0570\n",
      "Epoch 3/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 2.3109 - acc: 0.0940 - val_loss: 2.3079 - val_acc: 0.0640\n",
      "Epoch 4/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 2.2993 - acc: 0.0985 - val_loss: 2.2992 - val_acc: 0.0750\n",
      "Epoch 5/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 2.2926 - acc: 0.1028 - val_loss: 2.2916 - val_acc: 0.0900\n",
      "Epoch 6/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 2.2872 - acc: 0.1090 - val_loss: 2.2847 - val_acc: 0.0980\n",
      "Epoch 7/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 2.2840 - acc: 0.1170 - val_loss: 2.2783 - val_acc: 0.1110\n",
      "Epoch 8/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 2.2726 - acc: 0.1275 - val_loss: 2.2722 - val_acc: 0.1170\n",
      "Epoch 9/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 2.2661 - acc: 0.1260 - val_loss: 2.2665 - val_acc: 0.1290\n",
      "Epoch 10/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 2.2638 - acc: 0.1320 - val_loss: 2.2608 - val_acc: 0.1410\n",
      "Epoch 11/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 2.2553 - acc: 0.1470 - val_loss: 2.2552 - val_acc: 0.1570\n",
      "Epoch 12/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 2.2524 - acc: 0.1532 - val_loss: 2.2496 - val_acc: 0.1710\n",
      "Epoch 13/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 2.2506 - acc: 0.1580 - val_loss: 2.2440 - val_acc: 0.1960\n",
      "Epoch 14/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 2.2420 - acc: 0.1743 - val_loss: 2.2384 - val_acc: 0.2190\n",
      "Epoch 15/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 2.2390 - acc: 0.1797 - val_loss: 2.2326 - val_acc: 0.2470\n",
      "Epoch 16/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 2.2324 - acc: 0.1945 - val_loss: 2.2267 - val_acc: 0.2690\n",
      "Epoch 17/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 2.2242 - acc: 0.2097 - val_loss: 2.2207 - val_acc: 0.3030\n",
      "Epoch 18/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 2.2218 - acc: 0.2150 - val_loss: 2.2146 - val_acc: 0.3270\n",
      "Epoch 19/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 2.2115 - acc: 0.2405 - val_loss: 2.2082 - val_acc: 0.3570\n",
      "Epoch 20/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 2.2064 - acc: 0.2563 - val_loss: 2.2017 - val_acc: 0.3850\n",
      "Epoch 21/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 2.1988 - acc: 0.2638 - val_loss: 2.1949 - val_acc: 0.4100\n",
      "Epoch 22/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 2.1946 - acc: 0.2787 - val_loss: 2.1878 - val_acc: 0.4360\n",
      "Epoch 23/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 2.1868 - acc: 0.2845 - val_loss: 2.1806 - val_acc: 0.4630\n",
      "Epoch 24/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 2.1827 - acc: 0.2960 - val_loss: 2.1731 - val_acc: 0.4800\n",
      "Epoch 25/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 2.1756 - acc: 0.3062 - val_loss: 2.1654 - val_acc: 0.4960\n",
      "Epoch 26/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 2.1645 - acc: 0.3242 - val_loss: 2.1573 - val_acc: 0.5110\n",
      "Epoch 27/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 2.1544 - acc: 0.3347 - val_loss: 2.1488 - val_acc: 0.5370\n",
      "Epoch 28/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 2.1451 - acc: 0.3525 - val_loss: 2.1400 - val_acc: 0.5570\n",
      "Epoch 29/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 2.1446 - acc: 0.3488 - val_loss: 2.1310 - val_acc: 0.5710\n",
      "Epoch 30/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 2.1312 - acc: 0.3588 - val_loss: 2.1217 - val_acc: 0.5870\n",
      "Epoch 31/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 2.1210 - acc: 0.3855 - val_loss: 2.1118 - val_acc: 0.5950\n",
      "Epoch 32/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 2.1178 - acc: 0.3762 - val_loss: 2.1017 - val_acc: 0.6080\n",
      "Epoch 33/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 2.1028 - acc: 0.3843 - val_loss: 2.0912 - val_acc: 0.6190\n",
      "Epoch 34/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 2.0929 - acc: 0.4015 - val_loss: 2.0802 - val_acc: 0.6310\n",
      "Epoch 35/1000\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 2.0866 - acc: 0.4027 - val_loss: 2.0690 - val_acc: 0.6400\n",
      "Epoch 36/1000\n",
      "4000/4000 [==============================] - 0s 33us/step - loss: 2.0741 - acc: 0.4168 - val_loss: 2.0573 - val_acc: 0.6550\n",
      "Epoch 37/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 2.0639 - acc: 0.4240 - val_loss: 2.0454 - val_acc: 0.6570\n",
      "Epoch 38/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 2.0522 - acc: 0.4333 - val_loss: 2.0329 - val_acc: 0.6650\n",
      "Epoch 39/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 2.0401 - acc: 0.4432 - val_loss: 2.0200 - val_acc: 0.6700\n",
      "Epoch 40/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 2.0268 - acc: 0.4515 - val_loss: 2.0068 - val_acc: 0.6740\n",
      "Epoch 41/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 2.0139 - acc: 0.4567 - val_loss: 1.9930 - val_acc: 0.6850\n",
      "Epoch 42/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 1.9988 - acc: 0.4645 - val_loss: 1.9789 - val_acc: 0.6870\n",
      "Epoch 43/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 1.9926 - acc: 0.4572 - val_loss: 1.9646 - val_acc: 0.6950\n",
      "Epoch 44/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 1.9764 - acc: 0.4718 - val_loss: 1.9499 - val_acc: 0.6960\n",
      "Epoch 45/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 1.9618 - acc: 0.4775 - val_loss: 1.9347 - val_acc: 0.6950\n",
      "Epoch 46/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 1.9468 - acc: 0.4905 - val_loss: 1.9192 - val_acc: 0.7000\n",
      "Epoch 47/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 1.9335 - acc: 0.4943 - val_loss: 1.9034 - val_acc: 0.7010\n",
      "Epoch 48/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 1.9172 - acc: 0.5018 - val_loss: 1.8874 - val_acc: 0.7060\n",
      "Epoch 49/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 1.9002 - acc: 0.5090 - val_loss: 1.8710 - val_acc: 0.7050\n",
      "Epoch 50/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 1.8855 - acc: 0.5100 - val_loss: 1.8543 - val_acc: 0.7100\n",
      "Epoch 51/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 1.8728 - acc: 0.5125 - val_loss: 1.8376 - val_acc: 0.7110\n",
      "Epoch 52/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 1.8578 - acc: 0.5190 - val_loss: 1.8207 - val_acc: 0.7150\n",
      "Epoch 53/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 1.8399 - acc: 0.5323 - val_loss: 1.8033 - val_acc: 0.7190\n",
      "Epoch 54/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 1.8237 - acc: 0.5390 - val_loss: 1.7860 - val_acc: 0.7210\n",
      "Epoch 55/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 1.8077 - acc: 0.5360 - val_loss: 1.7685 - val_acc: 0.7230\n",
      "Epoch 56/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 1.7921 - acc: 0.5443 - val_loss: 1.7510 - val_acc: 0.7240\n",
      "Epoch 57/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 1.7703 - acc: 0.5553 - val_loss: 1.7334 - val_acc: 0.7270\n",
      "Epoch 58/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 1.7588 - acc: 0.5543 - val_loss: 1.7157 - val_acc: 0.7280\n",
      "Epoch 59/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 1.7447 - acc: 0.5590 - val_loss: 1.6982 - val_acc: 0.7310\n",
      "Epoch 60/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 26us/step - loss: 1.7263 - acc: 0.5687 - val_loss: 1.6805 - val_acc: 0.7350\n",
      "Epoch 61/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 1.7084 - acc: 0.5605 - val_loss: 1.6627 - val_acc: 0.7370\n",
      "Epoch 62/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 1.6974 - acc: 0.5700 - val_loss: 1.6454 - val_acc: 0.7390\n",
      "Epoch 63/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 1.6822 - acc: 0.5700 - val_loss: 1.6280 - val_acc: 0.7400\n",
      "Epoch 64/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 1.6679 - acc: 0.5735 - val_loss: 1.6108 - val_acc: 0.7430\n",
      "Epoch 65/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 1.6480 - acc: 0.5820 - val_loss: 1.5933 - val_acc: 0.7440\n",
      "Epoch 66/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 1.6316 - acc: 0.5872 - val_loss: 1.5762 - val_acc: 0.7490\n",
      "Epoch 67/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 1.6225 - acc: 0.5885 - val_loss: 1.5594 - val_acc: 0.7520\n",
      "Epoch 68/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 1.6001 - acc: 0.5978 - val_loss: 1.5427 - val_acc: 0.7570\n",
      "Epoch 69/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 1.5961 - acc: 0.5950 - val_loss: 1.5259 - val_acc: 0.7570\n",
      "Epoch 70/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 1.5722 - acc: 0.6042 - val_loss: 1.5091 - val_acc: 0.7630\n",
      "Epoch 71/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 1.5591 - acc: 0.6080 - val_loss: 1.4927 - val_acc: 0.7650\n",
      "Epoch 72/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 1.5447 - acc: 0.6162 - val_loss: 1.4764 - val_acc: 0.7670\n",
      "Epoch 73/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 1.5291 - acc: 0.6243 - val_loss: 1.4604 - val_acc: 0.7680\n",
      "Epoch 74/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 1.5148 - acc: 0.6093 - val_loss: 1.4444 - val_acc: 0.7690\n",
      "Epoch 75/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 1.5016 - acc: 0.6238 - val_loss: 1.4289 - val_acc: 0.7690\n",
      "Epoch 76/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 1.4885 - acc: 0.6238 - val_loss: 1.4134 - val_acc: 0.7730\n",
      "Epoch 77/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 1.4782 - acc: 0.6252 - val_loss: 1.3982 - val_acc: 0.7780\n",
      "Epoch 78/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 1.4628 - acc: 0.6340 - val_loss: 1.3828 - val_acc: 0.7820\n",
      "Epoch 79/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 1.4497 - acc: 0.6317 - val_loss: 1.3677 - val_acc: 0.7840\n",
      "Epoch 80/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 1.4286 - acc: 0.6300 - val_loss: 1.3529 - val_acc: 0.7860\n",
      "Epoch 81/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 1.4174 - acc: 0.6422 - val_loss: 1.3381 - val_acc: 0.7940\n",
      "Epoch 82/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 1.4101 - acc: 0.6442 - val_loss: 1.3234 - val_acc: 0.7930\n",
      "Epoch 83/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 1.3886 - acc: 0.6550 - val_loss: 1.3092 - val_acc: 0.7920\n",
      "Epoch 84/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 1.3775 - acc: 0.6565 - val_loss: 1.2947 - val_acc: 0.8030\n",
      "Epoch 85/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 1.3694 - acc: 0.6568 - val_loss: 1.2808 - val_acc: 0.8050\n",
      "Epoch 86/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 1.3521 - acc: 0.6603 - val_loss: 1.2670 - val_acc: 0.8060\n",
      "Epoch 87/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 1.3423 - acc: 0.6603 - val_loss: 1.2535 - val_acc: 0.8040\n",
      "Epoch 88/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 1.3326 - acc: 0.6673 - val_loss: 1.2403 - val_acc: 0.8070\n",
      "Epoch 89/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 1.3232 - acc: 0.6617 - val_loss: 1.2271 - val_acc: 0.8080\n",
      "Epoch 90/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 1.3014 - acc: 0.6685 - val_loss: 1.2140 - val_acc: 0.8080\n",
      "Epoch 91/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 1.2887 - acc: 0.6685 - val_loss: 1.2011 - val_acc: 0.8090\n",
      "Epoch 92/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 1.2858 - acc: 0.6720 - val_loss: 1.1886 - val_acc: 0.8120\n",
      "Epoch 93/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 1.2691 - acc: 0.6845 - val_loss: 1.1761 - val_acc: 0.8130\n",
      "Epoch 94/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 1.2623 - acc: 0.6750 - val_loss: 1.1641 - val_acc: 0.8130\n",
      "Epoch 95/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 1.2528 - acc: 0.6820 - val_loss: 1.1522 - val_acc: 0.8170\n",
      "Epoch 96/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 1.2398 - acc: 0.6917 - val_loss: 1.1402 - val_acc: 0.8170\n",
      "Epoch 97/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 1.2318 - acc: 0.6882 - val_loss: 1.1287 - val_acc: 0.8190\n",
      "Epoch 98/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 1.2151 - acc: 0.7050 - val_loss: 1.1168 - val_acc: 0.8220\n",
      "Epoch 99/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 1.2112 - acc: 0.6955 - val_loss: 1.1057 - val_acc: 0.8210\n",
      "Epoch 100/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 1.2027 - acc: 0.6925 - val_loss: 1.0946 - val_acc: 0.8230\n",
      "Epoch 101/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 1.1794 - acc: 0.7152 - val_loss: 1.0835 - val_acc: 0.8270\n",
      "Epoch 102/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 1.1740 - acc: 0.6955 - val_loss: 1.0726 - val_acc: 0.8250\n",
      "Epoch 103/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 1.1640 - acc: 0.7010 - val_loss: 1.0621 - val_acc: 0.8300\n",
      "Epoch 104/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 1.1656 - acc: 0.7043 - val_loss: 1.0519 - val_acc: 0.8310\n",
      "Epoch 105/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 1.1451 - acc: 0.7147 - val_loss: 1.0417 - val_acc: 0.8310\n",
      "Epoch 106/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 1.1411 - acc: 0.7075 - val_loss: 1.0316 - val_acc: 0.8320\n",
      "Epoch 107/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 1.1294 - acc: 0.7107 - val_loss: 1.0221 - val_acc: 0.8330\n",
      "Epoch 108/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 1.1148 - acc: 0.7305 - val_loss: 1.0125 - val_acc: 0.8330\n",
      "Epoch 109/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 1.1034 - acc: 0.7250 - val_loss: 1.0028 - val_acc: 0.8330\n",
      "Epoch 110/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 1.1128 - acc: 0.7242 - val_loss: 0.9933 - val_acc: 0.8340\n",
      "Epoch 111/1000\n",
      "4000/4000 [==============================] - 0s 34us/step - loss: 1.1023 - acc: 0.7240 - val_loss: 0.9845 - val_acc: 0.8340\n",
      "Epoch 112/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 1.0830 - acc: 0.7218 - val_loss: 0.9755 - val_acc: 0.8370\n",
      "Epoch 113/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 1.0736 - acc: 0.7278 - val_loss: 0.9667 - val_acc: 0.8390\n",
      "Epoch 114/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 1.0751 - acc: 0.7313 - val_loss: 0.9581 - val_acc: 0.8380\n",
      "Epoch 115/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 1.0546 - acc: 0.7370 - val_loss: 0.9495 - val_acc: 0.8390\n",
      "Epoch 116/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 1.0423 - acc: 0.7438 - val_loss: 0.9409 - val_acc: 0.8400\n",
      "Epoch 117/1000\n",
      "4000/4000 [==============================] - 0s 33us/step - loss: 1.0409 - acc: 0.7400 - val_loss: 0.9327 - val_acc: 0.8400\n",
      "Epoch 118/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 1.0374 - acc: 0.7345 - val_loss: 0.9246 - val_acc: 0.8400\n",
      "Epoch 119/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 1.0255 - acc: 0.7460 - val_loss: 0.9167 - val_acc: 0.8410\n",
      "Epoch 120/1000\n",
      "4000/4000 [==============================] - 0s 32us/step - loss: 1.0238 - acc: 0.7440 - val_loss: 0.9090 - val_acc: 0.8430\n",
      "Epoch 121/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 1.0168 - acc: 0.7463 - val_loss: 0.9015 - val_acc: 0.8430\n",
      "Epoch 122/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 1.0167 - acc: 0.7440 - val_loss: 0.8939 - val_acc: 0.8430\n",
      "Epoch 123/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.9977 - acc: 0.7528 - val_loss: 0.8867 - val_acc: 0.8450\n",
      "Epoch 124/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.9993 - acc: 0.7448 - val_loss: 0.8796 - val_acc: 0.8460\n",
      "Epoch 125/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.9972 - acc: 0.7320 - val_loss: 0.8729 - val_acc: 0.8460\n",
      "Epoch 126/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.9837 - acc: 0.7395 - val_loss: 0.8659 - val_acc: 0.8460\n",
      "Epoch 127/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.9818 - acc: 0.7502 - val_loss: 0.8593 - val_acc: 0.8470\n",
      "Epoch 128/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.9620 - acc: 0.7600 - val_loss: 0.8523 - val_acc: 0.8490\n",
      "Epoch 129/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.9704 - acc: 0.7463 - val_loss: 0.8459 - val_acc: 0.8490\n",
      "Epoch 130/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.9544 - acc: 0.7555 - val_loss: 0.8392 - val_acc: 0.8490\n",
      "Epoch 131/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.9503 - acc: 0.7532 - val_loss: 0.8329 - val_acc: 0.8480\n",
      "Epoch 132/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.9443 - acc: 0.7555 - val_loss: 0.8268 - val_acc: 0.8480\n",
      "Epoch 133/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.9372 - acc: 0.7678 - val_loss: 0.8206 - val_acc: 0.8480\n",
      "Epoch 134/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.9398 - acc: 0.7582 - val_loss: 0.8145 - val_acc: 0.8480\n",
      "Epoch 135/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.9214 - acc: 0.7665 - val_loss: 0.8085 - val_acc: 0.8490\n",
      "Epoch 136/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.9258 - acc: 0.7618 - val_loss: 0.8029 - val_acc: 0.8510\n",
      "Epoch 137/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.9158 - acc: 0.7705 - val_loss: 0.7969 - val_acc: 0.8510\n",
      "Epoch 138/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.9139 - acc: 0.7717 - val_loss: 0.7914 - val_acc: 0.8510\n",
      "Epoch 139/1000\n",
      "4000/4000 [==============================] - 0s 32us/step - loss: 0.9128 - acc: 0.7618 - val_loss: 0.7860 - val_acc: 0.8520\n",
      "Epoch 140/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.8912 - acc: 0.7715 - val_loss: 0.7802 - val_acc: 0.8520\n",
      "Epoch 141/1000\n",
      "4000/4000 [==============================] - 0s 33us/step - loss: 0.8874 - acc: 0.7745 - val_loss: 0.7748 - val_acc: 0.8560\n",
      "Epoch 142/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.8893 - acc: 0.7712 - val_loss: 0.7696 - val_acc: 0.8550\n",
      "Epoch 143/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.8840 - acc: 0.7680 - val_loss: 0.7645 - val_acc: 0.8540\n",
      "Epoch 144/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.8824 - acc: 0.7687 - val_loss: 0.7595 - val_acc: 0.8560\n",
      "Epoch 145/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.8749 - acc: 0.7835 - val_loss: 0.7545 - val_acc: 0.8570\n",
      "Epoch 146/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.8736 - acc: 0.7705 - val_loss: 0.7493 - val_acc: 0.8570\n",
      "Epoch 147/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.8730 - acc: 0.7735 - val_loss: 0.7446 - val_acc: 0.8580\n",
      "Epoch 148/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.8639 - acc: 0.7788 - val_loss: 0.7397 - val_acc: 0.8570\n",
      "Epoch 149/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.8688 - acc: 0.7758 - val_loss: 0.7351 - val_acc: 0.8570\n",
      "Epoch 150/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.8531 - acc: 0.7863 - val_loss: 0.7305 - val_acc: 0.8580\n",
      "Epoch 151/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.8480 - acc: 0.7820 - val_loss: 0.7262 - val_acc: 0.8580\n",
      "Epoch 152/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.8488 - acc: 0.7837 - val_loss: 0.7218 - val_acc: 0.8600\n",
      "Epoch 153/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.8365 - acc: 0.7845 - val_loss: 0.7176 - val_acc: 0.8600\n",
      "Epoch 154/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.8378 - acc: 0.7788 - val_loss: 0.7134 - val_acc: 0.8590\n",
      "Epoch 155/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.8273 - acc: 0.7850 - val_loss: 0.7090 - val_acc: 0.8620\n",
      "Epoch 156/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.8262 - acc: 0.7850 - val_loss: 0.7048 - val_acc: 0.8650\n",
      "Epoch 157/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.8299 - acc: 0.7780 - val_loss: 0.7007 - val_acc: 0.8640\n",
      "Epoch 158/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.8215 - acc: 0.7890 - val_loss: 0.6964 - val_acc: 0.8660\n",
      "Epoch 159/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.8152 - acc: 0.7925 - val_loss: 0.6925 - val_acc: 0.8660\n",
      "Epoch 160/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.8022 - acc: 0.7968 - val_loss: 0.6886 - val_acc: 0.8670\n",
      "Epoch 161/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.8051 - acc: 0.7897 - val_loss: 0.6847 - val_acc: 0.8670\n",
      "Epoch 162/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.8038 - acc: 0.7865 - val_loss: 0.6809 - val_acc: 0.8670\n",
      "Epoch 163/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.7912 - acc: 0.7933 - val_loss: 0.6771 - val_acc: 0.8670\n",
      "Epoch 164/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.7982 - acc: 0.7923 - val_loss: 0.6733 - val_acc: 0.8670\n",
      "Epoch 165/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.7931 - acc: 0.7923 - val_loss: 0.6697 - val_acc: 0.8670\n",
      "Epoch 166/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.7944 - acc: 0.7933 - val_loss: 0.6660 - val_acc: 0.8660\n",
      "Epoch 167/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.7743 - acc: 0.7987 - val_loss: 0.6624 - val_acc: 0.8670\n",
      "Epoch 168/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.7745 - acc: 0.7963 - val_loss: 0.6590 - val_acc: 0.8660\n",
      "Epoch 169/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.7805 - acc: 0.8023 - val_loss: 0.6557 - val_acc: 0.8660\n",
      "Epoch 170/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.7708 - acc: 0.7925 - val_loss: 0.6521 - val_acc: 0.8670\n",
      "Epoch 171/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.7706 - acc: 0.7925 - val_loss: 0.6490 - val_acc: 0.8660\n",
      "Epoch 172/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.7820 - acc: 0.7955 - val_loss: 0.6457 - val_acc: 0.8680\n",
      "Epoch 173/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.7714 - acc: 0.7927 - val_loss: 0.6426 - val_acc: 0.8690\n",
      "Epoch 174/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.7545 - acc: 0.8003 - val_loss: 0.6391 - val_acc: 0.8680\n",
      "Epoch 175/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.7583 - acc: 0.8020 - val_loss: 0.6359 - val_acc: 0.8710\n",
      "Epoch 176/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.7653 - acc: 0.7935 - val_loss: 0.6327 - val_acc: 0.8720\n",
      "Epoch 177/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.7464 - acc: 0.8017 - val_loss: 0.6298 - val_acc: 0.8730\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.7491 - acc: 0.8000 - val_loss: 0.6267 - val_acc: 0.8720\n",
      "Epoch 179/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.7411 - acc: 0.7987 - val_loss: 0.6237 - val_acc: 0.8720\n",
      "Epoch 180/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.7415 - acc: 0.8025 - val_loss: 0.6208 - val_acc: 0.8710\n",
      "Epoch 181/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.7424 - acc: 0.7980 - val_loss: 0.6179 - val_acc: 0.8710\n",
      "Epoch 182/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.7432 - acc: 0.8073 - val_loss: 0.6150 - val_acc: 0.8700\n",
      "Epoch 183/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.7371 - acc: 0.8110 - val_loss: 0.6123 - val_acc: 0.8710\n",
      "Epoch 184/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.7328 - acc: 0.8123 - val_loss: 0.6094 - val_acc: 0.8720\n",
      "Epoch 185/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.7346 - acc: 0.8040 - val_loss: 0.6069 - val_acc: 0.8720\n",
      "Epoch 186/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.7289 - acc: 0.8050 - val_loss: 0.6042 - val_acc: 0.8730\n",
      "Epoch 187/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.7175 - acc: 0.8177 - val_loss: 0.6013 - val_acc: 0.8740\n",
      "Epoch 188/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.7155 - acc: 0.8160 - val_loss: 0.5986 - val_acc: 0.8740\n",
      "Epoch 189/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.7246 - acc: 0.8053 - val_loss: 0.5961 - val_acc: 0.8730\n",
      "Epoch 190/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.7207 - acc: 0.8045 - val_loss: 0.5936 - val_acc: 0.8720\n",
      "Epoch 191/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.7197 - acc: 0.8150 - val_loss: 0.5912 - val_acc: 0.8730\n",
      "Epoch 192/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.6988 - acc: 0.8203 - val_loss: 0.5885 - val_acc: 0.8750\n",
      "Epoch 193/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.7124 - acc: 0.8130 - val_loss: 0.5861 - val_acc: 0.8740\n",
      "Epoch 194/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.7153 - acc: 0.8107 - val_loss: 0.5837 - val_acc: 0.8740\n",
      "Epoch 195/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.6973 - acc: 0.8100 - val_loss: 0.5810 - val_acc: 0.8740\n",
      "Epoch 196/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.7009 - acc: 0.8160 - val_loss: 0.5786 - val_acc: 0.8770\n",
      "Epoch 197/1000\n",
      "4000/4000 [==============================] - 0s 34us/step - loss: 0.6999 - acc: 0.8137 - val_loss: 0.5765 - val_acc: 0.8750\n",
      "Epoch 198/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.6934 - acc: 0.8200 - val_loss: 0.5740 - val_acc: 0.8770\n",
      "Epoch 199/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.7041 - acc: 0.8123 - val_loss: 0.5716 - val_acc: 0.8770\n",
      "Epoch 200/1000\n",
      "4000/4000 [==============================] - 0s 33us/step - loss: 0.6901 - acc: 0.8197 - val_loss: 0.5692 - val_acc: 0.8780\n",
      "Epoch 201/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.6828 - acc: 0.8170 - val_loss: 0.5669 - val_acc: 0.8770\n",
      "Epoch 202/1000\n",
      "4000/4000 [==============================] - 0s 32us/step - loss: 0.6858 - acc: 0.8177 - val_loss: 0.5645 - val_acc: 0.8770\n",
      "Epoch 203/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.6891 - acc: 0.8095 - val_loss: 0.5624 - val_acc: 0.8780\n",
      "Epoch 204/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.6846 - acc: 0.8143 - val_loss: 0.5602 - val_acc: 0.8780\n",
      "Epoch 205/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.6796 - acc: 0.8157 - val_loss: 0.5581 - val_acc: 0.8780\n",
      "Epoch 206/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.6783 - acc: 0.8210 - val_loss: 0.5559 - val_acc: 0.8790\n",
      "Epoch 207/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.6710 - acc: 0.8183 - val_loss: 0.5538 - val_acc: 0.8800\n",
      "Epoch 208/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.6846 - acc: 0.8135 - val_loss: 0.5518 - val_acc: 0.8780\n",
      "Epoch 209/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.6754 - acc: 0.8225 - val_loss: 0.5498 - val_acc: 0.8780\n",
      "Epoch 210/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.6784 - acc: 0.8180 - val_loss: 0.5477 - val_acc: 0.8790\n",
      "Epoch 211/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.6645 - acc: 0.8197 - val_loss: 0.5458 - val_acc: 0.8790\n",
      "Epoch 212/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.6735 - acc: 0.8118 - val_loss: 0.5439 - val_acc: 0.8790\n",
      "Epoch 213/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.6711 - acc: 0.8140 - val_loss: 0.5422 - val_acc: 0.8800\n",
      "Epoch 214/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.6683 - acc: 0.8187 - val_loss: 0.5404 - val_acc: 0.8810\n",
      "Epoch 215/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.6608 - acc: 0.8237 - val_loss: 0.5384 - val_acc: 0.8810\n",
      "Epoch 216/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.6550 - acc: 0.8273 - val_loss: 0.5364 - val_acc: 0.8800\n",
      "Epoch 217/1000\n",
      "4000/4000 [==============================] - 0s 32us/step - loss: 0.6572 - acc: 0.8170 - val_loss: 0.5345 - val_acc: 0.8790\n",
      "Epoch 218/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.6563 - acc: 0.8320 - val_loss: 0.5327 - val_acc: 0.8810\n",
      "Epoch 219/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.6548 - acc: 0.8240 - val_loss: 0.5311 - val_acc: 0.8790\n",
      "Epoch 220/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.6562 - acc: 0.8155 - val_loss: 0.5291 - val_acc: 0.8790\n",
      "Epoch 221/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.6464 - acc: 0.8260 - val_loss: 0.5274 - val_acc: 0.8810\n",
      "Epoch 222/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.6456 - acc: 0.8320 - val_loss: 0.5257 - val_acc: 0.8800\n",
      "Epoch 223/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.6414 - acc: 0.8307 - val_loss: 0.5236 - val_acc: 0.8820\n",
      "Epoch 224/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.6451 - acc: 0.8330 - val_loss: 0.5220 - val_acc: 0.8820\n",
      "Epoch 225/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.6422 - acc: 0.8337 - val_loss: 0.5205 - val_acc: 0.8840\n",
      "Epoch 226/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.6380 - acc: 0.8290 - val_loss: 0.5187 - val_acc: 0.8850\n",
      "Epoch 227/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.6322 - acc: 0.8293 - val_loss: 0.5170 - val_acc: 0.8830\n",
      "Epoch 228/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.6382 - acc: 0.8305 - val_loss: 0.5154 - val_acc: 0.8850\n",
      "Epoch 229/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.6345 - acc: 0.8273 - val_loss: 0.5134 - val_acc: 0.8850\n",
      "Epoch 230/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.6238 - acc: 0.8342 - val_loss: 0.5120 - val_acc: 0.8860\n",
      "Epoch 231/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.6231 - acc: 0.8365 - val_loss: 0.5104 - val_acc: 0.8840\n",
      "Epoch 232/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.6193 - acc: 0.8282 - val_loss: 0.5087 - val_acc: 0.8830\n",
      "Epoch 233/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.6145 - acc: 0.8312 - val_loss: 0.5071 - val_acc: 0.8840\n",
      "Epoch 234/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.6204 - acc: 0.8357 - val_loss: 0.5053 - val_acc: 0.8860\n",
      "Epoch 235/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.6190 - acc: 0.8325 - val_loss: 0.5039 - val_acc: 0.8860\n",
      "Epoch 236/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.6130 - acc: 0.8330 - val_loss: 0.5024 - val_acc: 0.8870\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.6165 - acc: 0.8385 - val_loss: 0.5011 - val_acc: 0.8870\n",
      "Epoch 238/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.6243 - acc: 0.8325 - val_loss: 0.4994 - val_acc: 0.8870\n",
      "Epoch 239/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.6150 - acc: 0.8267 - val_loss: 0.4978 - val_acc: 0.8880\n",
      "Epoch 240/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.6106 - acc: 0.8390 - val_loss: 0.4963 - val_acc: 0.8880\n",
      "Epoch 241/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.6119 - acc: 0.8390 - val_loss: 0.4950 - val_acc: 0.8880\n",
      "Epoch 242/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.6119 - acc: 0.8387 - val_loss: 0.4937 - val_acc: 0.8880\n",
      "Epoch 243/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.6089 - acc: 0.8307 - val_loss: 0.4921 - val_acc: 0.8890\n",
      "Epoch 244/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.6174 - acc: 0.8337 - val_loss: 0.4909 - val_acc: 0.8880\n",
      "Epoch 245/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.6024 - acc: 0.8413 - val_loss: 0.4896 - val_acc: 0.8890\n",
      "Epoch 246/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.6109 - acc: 0.8387 - val_loss: 0.4883 - val_acc: 0.8880\n",
      "Epoch 247/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.5969 - acc: 0.8383 - val_loss: 0.4867 - val_acc: 0.8890\n",
      "Epoch 248/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.6106 - acc: 0.8333 - val_loss: 0.4855 - val_acc: 0.8900\n",
      "Epoch 249/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.5945 - acc: 0.8393 - val_loss: 0.4840 - val_acc: 0.8910\n",
      "Epoch 250/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.6005 - acc: 0.8337 - val_loss: 0.4824 - val_acc: 0.8910\n",
      "Epoch 251/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.5920 - acc: 0.8370 - val_loss: 0.4811 - val_acc: 0.8910\n",
      "Epoch 252/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.5895 - acc: 0.8420 - val_loss: 0.4797 - val_acc: 0.8900\n",
      "Epoch 253/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.5889 - acc: 0.8370 - val_loss: 0.4785 - val_acc: 0.8910\n",
      "Epoch 254/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.6006 - acc: 0.8387 - val_loss: 0.4771 - val_acc: 0.8910\n",
      "Epoch 255/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.5893 - acc: 0.8397 - val_loss: 0.4757 - val_acc: 0.8920\n",
      "Epoch 256/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.5896 - acc: 0.8407 - val_loss: 0.4746 - val_acc: 0.8900\n",
      "Epoch 257/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.5888 - acc: 0.8363 - val_loss: 0.4733 - val_acc: 0.8910\n",
      "Epoch 258/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.5812 - acc: 0.8390 - val_loss: 0.4719 - val_acc: 0.8910\n",
      "Epoch 259/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.5890 - acc: 0.8377 - val_loss: 0.4707 - val_acc: 0.8920\n",
      "Epoch 260/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.5884 - acc: 0.8360 - val_loss: 0.4696 - val_acc: 0.8940\n",
      "Epoch 261/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.5760 - acc: 0.8430 - val_loss: 0.4683 - val_acc: 0.8940\n",
      "Epoch 262/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.5909 - acc: 0.8385 - val_loss: 0.4672 - val_acc: 0.8940\n",
      "Epoch 263/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.5741 - acc: 0.8455 - val_loss: 0.4660 - val_acc: 0.8940\n",
      "Epoch 264/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.5660 - acc: 0.8435 - val_loss: 0.4648 - val_acc: 0.8930\n",
      "Epoch 265/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.5767 - acc: 0.8443 - val_loss: 0.4635 - val_acc: 0.8940\n",
      "Epoch 266/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.5770 - acc: 0.8375 - val_loss: 0.4625 - val_acc: 0.8940\n",
      "Epoch 267/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.5665 - acc: 0.8450 - val_loss: 0.4611 - val_acc: 0.8950\n",
      "Epoch 268/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.5767 - acc: 0.8470 - val_loss: 0.4602 - val_acc: 0.8950\n",
      "Epoch 269/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.5762 - acc: 0.8410 - val_loss: 0.4590 - val_acc: 0.8960\n",
      "Epoch 270/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.5708 - acc: 0.8492 - val_loss: 0.4579 - val_acc: 0.8960\n",
      "Epoch 271/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.5717 - acc: 0.8385 - val_loss: 0.4568 - val_acc: 0.8960\n",
      "Epoch 272/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.5602 - acc: 0.8515 - val_loss: 0.4558 - val_acc: 0.8960\n",
      "Epoch 273/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.5653 - acc: 0.8470 - val_loss: 0.4547 - val_acc: 0.8960\n",
      "Epoch 274/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.5614 - acc: 0.8490 - val_loss: 0.4536 - val_acc: 0.8960\n",
      "Epoch 275/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.5639 - acc: 0.8458 - val_loss: 0.4526 - val_acc: 0.8970\n",
      "Epoch 276/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.5668 - acc: 0.8413 - val_loss: 0.4517 - val_acc: 0.8970\n",
      "Epoch 277/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.5658 - acc: 0.8430 - val_loss: 0.4505 - val_acc: 0.8960\n",
      "Epoch 278/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.5676 - acc: 0.8435 - val_loss: 0.4496 - val_acc: 0.8960\n",
      "Epoch 279/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.5674 - acc: 0.8508 - val_loss: 0.4483 - val_acc: 0.8960\n",
      "Epoch 280/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.5598 - acc: 0.8475 - val_loss: 0.4474 - val_acc: 0.8970\n",
      "Epoch 281/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.5568 - acc: 0.8480 - val_loss: 0.4464 - val_acc: 0.8980\n",
      "Epoch 282/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.5543 - acc: 0.8522 - val_loss: 0.4455 - val_acc: 0.8990\n",
      "Epoch 283/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.5490 - acc: 0.8533 - val_loss: 0.4445 - val_acc: 0.8990\n",
      "Epoch 284/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.5627 - acc: 0.8492 - val_loss: 0.4437 - val_acc: 0.9000\n",
      "Epoch 285/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.5433 - acc: 0.8482 - val_loss: 0.4425 - val_acc: 0.9000\n",
      "Epoch 286/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.5571 - acc: 0.8478 - val_loss: 0.4415 - val_acc: 0.9000\n",
      "Epoch 287/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.5445 - acc: 0.8515 - val_loss: 0.4406 - val_acc: 0.9010\n",
      "Epoch 288/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.5499 - acc: 0.8535 - val_loss: 0.4396 - val_acc: 0.9000\n",
      "Epoch 289/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.5424 - acc: 0.8508 - val_loss: 0.4385 - val_acc: 0.9020\n",
      "Epoch 290/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.5487 - acc: 0.8435 - val_loss: 0.4376 - val_acc: 0.9010\n",
      "Epoch 291/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.5395 - acc: 0.8563 - val_loss: 0.4366 - val_acc: 0.9010\n",
      "Epoch 292/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.5459 - acc: 0.8555 - val_loss: 0.4357 - val_acc: 0.9020\n",
      "Epoch 293/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.5418 - acc: 0.8593 - val_loss: 0.4349 - val_acc: 0.9020\n",
      "Epoch 294/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.5432 - acc: 0.8462 - val_loss: 0.4340 - val_acc: 0.9000\n",
      "Epoch 295/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.5434 - acc: 0.8455 - val_loss: 0.4332 - val_acc: 0.9000\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.5534 - acc: 0.8482 - val_loss: 0.4323 - val_acc: 0.9020\n",
      "Epoch 297/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.5420 - acc: 0.8478 - val_loss: 0.4314 - val_acc: 0.9020\n",
      "Epoch 298/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.5461 - acc: 0.8500 - val_loss: 0.4307 - val_acc: 0.9000\n",
      "Epoch 299/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.5407 - acc: 0.8555 - val_loss: 0.4299 - val_acc: 0.9000\n",
      "Epoch 300/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.5324 - acc: 0.8500 - val_loss: 0.4291 - val_acc: 0.9020\n",
      "Epoch 301/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.5269 - acc: 0.8565 - val_loss: 0.4282 - val_acc: 0.9020\n",
      "Epoch 302/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.5342 - acc: 0.8588 - val_loss: 0.4273 - val_acc: 0.9030\n",
      "Epoch 303/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.5440 - acc: 0.8503 - val_loss: 0.4264 - val_acc: 0.9040\n",
      "Epoch 304/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.5248 - acc: 0.8568 - val_loss: 0.4254 - val_acc: 0.9030\n",
      "Epoch 305/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.5324 - acc: 0.8550 - val_loss: 0.4247 - val_acc: 0.9040\n",
      "Epoch 306/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.5287 - acc: 0.8565 - val_loss: 0.4237 - val_acc: 0.9030\n",
      "Epoch 307/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.5352 - acc: 0.8578 - val_loss: 0.4228 - val_acc: 0.9040\n",
      "Epoch 308/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.5249 - acc: 0.8558 - val_loss: 0.4221 - val_acc: 0.9040\n",
      "Epoch 309/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.5304 - acc: 0.8548 - val_loss: 0.4214 - val_acc: 0.9030\n",
      "Epoch 310/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.5316 - acc: 0.8505 - val_loss: 0.4208 - val_acc: 0.9050\n",
      "Epoch 311/1000\n",
      "4000/4000 [==============================] - 0s 36us/step - loss: 0.5236 - acc: 0.8590 - val_loss: 0.4197 - val_acc: 0.9050\n",
      "Epoch 312/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.5272 - acc: 0.8618 - val_loss: 0.4191 - val_acc: 0.9050\n",
      "Epoch 313/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.5222 - acc: 0.8538 - val_loss: 0.4181 - val_acc: 0.9040\n",
      "Epoch 314/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.5236 - acc: 0.8602 - val_loss: 0.4173 - val_acc: 0.9040\n",
      "Epoch 315/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.5254 - acc: 0.8628 - val_loss: 0.4165 - val_acc: 0.9040\n",
      "Epoch 316/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.5294 - acc: 0.8563 - val_loss: 0.4158 - val_acc: 0.9040\n",
      "Epoch 317/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.5247 - acc: 0.8602 - val_loss: 0.4149 - val_acc: 0.9040\n",
      "Epoch 318/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.5143 - acc: 0.8598 - val_loss: 0.4142 - val_acc: 0.9060\n",
      "Epoch 319/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.5158 - acc: 0.8628 - val_loss: 0.4133 - val_acc: 0.9050\n",
      "Epoch 320/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.5185 - acc: 0.8568 - val_loss: 0.4128 - val_acc: 0.9050\n",
      "Epoch 321/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.5214 - acc: 0.8558 - val_loss: 0.4119 - val_acc: 0.9040\n",
      "Epoch 322/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.5141 - acc: 0.8642 - val_loss: 0.4111 - val_acc: 0.9060\n",
      "Epoch 323/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.5069 - acc: 0.8645 - val_loss: 0.4105 - val_acc: 0.9040\n",
      "Epoch 324/1000\n",
      "4000/4000 [==============================] - 0s 32us/step - loss: 0.5116 - acc: 0.8623 - val_loss: 0.4096 - val_acc: 0.9050\n",
      "Epoch 325/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.5242 - acc: 0.8548 - val_loss: 0.4090 - val_acc: 0.9050\n",
      "Epoch 326/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.5144 - acc: 0.8572 - val_loss: 0.4084 - val_acc: 0.9060\n",
      "Epoch 327/1000\n",
      "4000/4000 [==============================] - 0s 38us/step - loss: 0.5100 - acc: 0.8612 - val_loss: 0.4077 - val_acc: 0.9060\n",
      "Epoch 328/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.5085 - acc: 0.8595 - val_loss: 0.4070 - val_acc: 0.9060\n",
      "Epoch 329/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.5124 - acc: 0.8625 - val_loss: 0.4064 - val_acc: 0.9080\n",
      "Epoch 330/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.5112 - acc: 0.8535 - val_loss: 0.4056 - val_acc: 0.9070\n",
      "Epoch 331/1000\n",
      "4000/4000 [==============================] - 0s 33us/step - loss: 0.4989 - acc: 0.8705 - val_loss: 0.4048 - val_acc: 0.9060\n",
      "Epoch 332/1000\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 0.5109 - acc: 0.8602 - val_loss: 0.4044 - val_acc: 0.9040\n",
      "Epoch 333/1000\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 0.5100 - acc: 0.8635 - val_loss: 0.4039 - val_acc: 0.9050\n",
      "Epoch 334/1000\n",
      "4000/4000 [==============================] - 0s 34us/step - loss: 0.5039 - acc: 0.8595 - val_loss: 0.4033 - val_acc: 0.9050\n",
      "Epoch 335/1000\n",
      "4000/4000 [==============================] - 0s 32us/step - loss: 0.5027 - acc: 0.8630 - val_loss: 0.4025 - val_acc: 0.9050\n",
      "Epoch 336/1000\n",
      "4000/4000 [==============================] - 0s 37us/step - loss: 0.5108 - acc: 0.8600 - val_loss: 0.4018 - val_acc: 0.9050\n",
      "Epoch 337/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.4975 - acc: 0.8685 - val_loss: 0.4011 - val_acc: 0.9050\n",
      "Epoch 338/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.5123 - acc: 0.8538 - val_loss: 0.4007 - val_acc: 0.9060\n",
      "Epoch 339/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.5092 - acc: 0.8563 - val_loss: 0.4002 - val_acc: 0.9070\n",
      "Epoch 340/1000\n",
      "4000/4000 [==============================] - 0s 34us/step - loss: 0.5050 - acc: 0.8580 - val_loss: 0.3996 - val_acc: 0.9050\n",
      "Epoch 341/1000\n",
      "4000/4000 [==============================] - 0s 32us/step - loss: 0.5095 - acc: 0.8618 - val_loss: 0.3991 - val_acc: 0.9060\n",
      "Epoch 342/1000\n",
      "4000/4000 [==============================] - 0s 32us/step - loss: 0.5011 - acc: 0.8632 - val_loss: 0.3984 - val_acc: 0.9060\n",
      "Epoch 343/1000\n",
      "4000/4000 [==============================] - 0s 33us/step - loss: 0.4927 - acc: 0.8632 - val_loss: 0.3978 - val_acc: 0.9050\n",
      "Epoch 344/1000\n",
      "4000/4000 [==============================] - 0s 35us/step - loss: 0.5060 - acc: 0.8602 - val_loss: 0.3974 - val_acc: 0.9070\n",
      "Epoch 345/1000\n",
      "4000/4000 [==============================] - 0s 33us/step - loss: 0.4914 - acc: 0.8668 - val_loss: 0.3967 - val_acc: 0.9070\n",
      "Epoch 346/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4969 - acc: 0.8642 - val_loss: 0.3960 - val_acc: 0.9090\n",
      "Epoch 347/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.4927 - acc: 0.8598 - val_loss: 0.3952 - val_acc: 0.9080\n",
      "Epoch 348/1000\n",
      "4000/4000 [==============================] - 0s 34us/step - loss: 0.4962 - acc: 0.8602 - val_loss: 0.3945 - val_acc: 0.9070\n",
      "Epoch 349/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.4949 - acc: 0.8623 - val_loss: 0.3939 - val_acc: 0.9070\n",
      "Epoch 350/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.4927 - acc: 0.8680 - val_loss: 0.3932 - val_acc: 0.9060\n",
      "Epoch 351/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.4964 - acc: 0.8645 - val_loss: 0.3927 - val_acc: 0.9070\n",
      "Epoch 352/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.4960 - acc: 0.8665 - val_loss: 0.3922 - val_acc: 0.9070\n",
      "Epoch 353/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.4980 - acc: 0.8683 - val_loss: 0.3918 - val_acc: 0.9060\n",
      "Epoch 354/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.4978 - acc: 0.8610 - val_loss: 0.3914 - val_acc: 0.9070\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.4934 - acc: 0.8625 - val_loss: 0.3908 - val_acc: 0.9070\n",
      "Epoch 356/1000\n",
      "4000/4000 [==============================] - 0s 33us/step - loss: 0.4884 - acc: 0.8638 - val_loss: 0.3902 - val_acc: 0.9070\n",
      "Epoch 357/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.4920 - acc: 0.8600 - val_loss: 0.3896 - val_acc: 0.9070\n",
      "Epoch 358/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4864 - acc: 0.8638 - val_loss: 0.3890 - val_acc: 0.9070\n",
      "Epoch 359/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4903 - acc: 0.8698 - val_loss: 0.3885 - val_acc: 0.9070\n",
      "Epoch 360/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.4774 - acc: 0.8722 - val_loss: 0.3880 - val_acc: 0.9070\n",
      "Epoch 361/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4876 - acc: 0.8685 - val_loss: 0.3875 - val_acc: 0.9070\n",
      "Epoch 362/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4854 - acc: 0.8722 - val_loss: 0.3871 - val_acc: 0.9080\n",
      "Epoch 363/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4914 - acc: 0.8650 - val_loss: 0.3864 - val_acc: 0.9100\n",
      "Epoch 364/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4903 - acc: 0.8670 - val_loss: 0.3860 - val_acc: 0.9090\n",
      "Epoch 365/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.4868 - acc: 0.8683 - val_loss: 0.3855 - val_acc: 0.9090\n",
      "Epoch 366/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.4732 - acc: 0.8720 - val_loss: 0.3847 - val_acc: 0.9090\n",
      "Epoch 367/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.4793 - acc: 0.8650 - val_loss: 0.3841 - val_acc: 0.9080\n",
      "Epoch 368/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.4801 - acc: 0.8650 - val_loss: 0.3836 - val_acc: 0.9080\n",
      "Epoch 369/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.4843 - acc: 0.8640 - val_loss: 0.3832 - val_acc: 0.9080\n",
      "Epoch 370/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.4802 - acc: 0.8672 - val_loss: 0.3828 - val_acc: 0.9080\n",
      "Epoch 371/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4785 - acc: 0.8668 - val_loss: 0.3825 - val_acc: 0.9090\n",
      "Epoch 372/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4812 - acc: 0.8688 - val_loss: 0.3819 - val_acc: 0.9090\n",
      "Epoch 373/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4790 - acc: 0.8680 - val_loss: 0.3812 - val_acc: 0.9080\n",
      "Epoch 374/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.4827 - acc: 0.8670 - val_loss: 0.3807 - val_acc: 0.9080\n",
      "Epoch 375/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4774 - acc: 0.8668 - val_loss: 0.3802 - val_acc: 0.9080\n",
      "Epoch 376/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4820 - acc: 0.8638 - val_loss: 0.3797 - val_acc: 0.9080\n",
      "Epoch 377/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4779 - acc: 0.8628 - val_loss: 0.3791 - val_acc: 0.9080\n",
      "Epoch 378/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.4664 - acc: 0.8748 - val_loss: 0.3786 - val_acc: 0.9080\n",
      "Epoch 379/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4784 - acc: 0.8715 - val_loss: 0.3781 - val_acc: 0.9080\n",
      "Epoch 380/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.4794 - acc: 0.8630 - val_loss: 0.3776 - val_acc: 0.9080\n",
      "Epoch 381/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.4782 - acc: 0.8655 - val_loss: 0.3773 - val_acc: 0.9090\n",
      "Epoch 382/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4696 - acc: 0.8715 - val_loss: 0.3769 - val_acc: 0.9100\n",
      "Epoch 383/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4708 - acc: 0.8675 - val_loss: 0.3763 - val_acc: 0.9100\n",
      "Epoch 384/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4764 - acc: 0.8688 - val_loss: 0.3760 - val_acc: 0.9090\n",
      "Epoch 385/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.4766 - acc: 0.8675 - val_loss: 0.3757 - val_acc: 0.9100\n",
      "Epoch 386/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4758 - acc: 0.8690 - val_loss: 0.3751 - val_acc: 0.9090\n",
      "Epoch 387/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4656 - acc: 0.8710 - val_loss: 0.3745 - val_acc: 0.9100\n",
      "Epoch 388/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.4694 - acc: 0.8708 - val_loss: 0.3740 - val_acc: 0.9080\n",
      "Epoch 389/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4577 - acc: 0.8713 - val_loss: 0.3734 - val_acc: 0.9080\n",
      "Epoch 390/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4622 - acc: 0.8715 - val_loss: 0.3728 - val_acc: 0.9080\n",
      "Epoch 391/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4639 - acc: 0.8675 - val_loss: 0.3724 - val_acc: 0.9080\n",
      "Epoch 392/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.4685 - acc: 0.8732 - val_loss: 0.3721 - val_acc: 0.9100\n",
      "Epoch 393/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.4630 - acc: 0.8740 - val_loss: 0.3715 - val_acc: 0.9110\n",
      "Epoch 394/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4766 - acc: 0.8662 - val_loss: 0.3712 - val_acc: 0.9100\n",
      "Epoch 395/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4655 - acc: 0.8725 - val_loss: 0.3708 - val_acc: 0.9110\n",
      "Epoch 396/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4660 - acc: 0.8785 - val_loss: 0.3704 - val_acc: 0.9110\n",
      "Epoch 397/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4646 - acc: 0.8678 - val_loss: 0.3699 - val_acc: 0.9110\n",
      "Epoch 398/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.4579 - acc: 0.8755 - val_loss: 0.3692 - val_acc: 0.9110\n",
      "Epoch 399/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4579 - acc: 0.8750 - val_loss: 0.3689 - val_acc: 0.9110\n",
      "Epoch 400/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4612 - acc: 0.8700 - val_loss: 0.3687 - val_acc: 0.9110\n",
      "Epoch 401/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4606 - acc: 0.8750 - val_loss: 0.3682 - val_acc: 0.9120\n",
      "Epoch 402/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4584 - acc: 0.8683 - val_loss: 0.3678 - val_acc: 0.9120\n",
      "Epoch 403/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4592 - acc: 0.8750 - val_loss: 0.3672 - val_acc: 0.9120\n",
      "Epoch 404/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4535 - acc: 0.8725 - val_loss: 0.3668 - val_acc: 0.9120\n",
      "Epoch 405/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.4554 - acc: 0.8738 - val_loss: 0.3662 - val_acc: 0.9120\n",
      "Epoch 406/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4569 - acc: 0.8787 - val_loss: 0.3660 - val_acc: 0.9120\n",
      "Epoch 407/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4523 - acc: 0.8778 - val_loss: 0.3656 - val_acc: 0.9120\n",
      "Epoch 408/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4577 - acc: 0.8765 - val_loss: 0.3652 - val_acc: 0.9120\n",
      "Epoch 409/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.4572 - acc: 0.8645 - val_loss: 0.3649 - val_acc: 0.9120\n",
      "Epoch 410/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.4529 - acc: 0.8755 - val_loss: 0.3645 - val_acc: 0.9120\n",
      "Epoch 411/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4539 - acc: 0.8708 - val_loss: 0.3642 - val_acc: 0.9110\n",
      "Epoch 412/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4493 - acc: 0.8782 - val_loss: 0.3636 - val_acc: 0.9120\n",
      "Epoch 413/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4668 - acc: 0.8705 - val_loss: 0.3634 - val_acc: 0.9120\n",
      "Epoch 414/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4577 - acc: 0.8780 - val_loss: 0.3631 - val_acc: 0.9120\n",
      "Epoch 415/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4592 - acc: 0.8710 - val_loss: 0.3629 - val_acc: 0.9130\n",
      "Epoch 416/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4520 - acc: 0.8720 - val_loss: 0.3624 - val_acc: 0.9120\n",
      "Epoch 417/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.4537 - acc: 0.8718 - val_loss: 0.3619 - val_acc: 0.9120\n",
      "Epoch 418/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.4576 - acc: 0.8713 - val_loss: 0.3615 - val_acc: 0.9120\n",
      "Epoch 419/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.4475 - acc: 0.8748 - val_loss: 0.3611 - val_acc: 0.9120\n",
      "Epoch 420/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4569 - acc: 0.8740 - val_loss: 0.3609 - val_acc: 0.9120\n",
      "Epoch 421/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4438 - acc: 0.8752 - val_loss: 0.3604 - val_acc: 0.9120\n",
      "Epoch 422/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4481 - acc: 0.8745 - val_loss: 0.3603 - val_acc: 0.9140\n",
      "Epoch 423/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4458 - acc: 0.8735 - val_loss: 0.3599 - val_acc: 0.9120\n",
      "Epoch 424/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.4455 - acc: 0.8700 - val_loss: 0.3594 - val_acc: 0.9120\n",
      "Epoch 425/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4492 - acc: 0.8720 - val_loss: 0.3589 - val_acc: 0.9120\n",
      "Epoch 426/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4436 - acc: 0.8770 - val_loss: 0.3584 - val_acc: 0.9130\n",
      "Epoch 427/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4493 - acc: 0.8735 - val_loss: 0.3581 - val_acc: 0.9130\n",
      "Epoch 428/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4458 - acc: 0.8762 - val_loss: 0.3577 - val_acc: 0.9120\n",
      "Epoch 429/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4451 - acc: 0.8750 - val_loss: 0.3574 - val_acc: 0.9130\n",
      "Epoch 430/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.4373 - acc: 0.8778 - val_loss: 0.3568 - val_acc: 0.9120\n",
      "Epoch 431/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.4439 - acc: 0.8722 - val_loss: 0.3568 - val_acc: 0.9130\n",
      "Epoch 432/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4424 - acc: 0.8790 - val_loss: 0.3564 - val_acc: 0.9130\n",
      "Epoch 433/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4440 - acc: 0.8725 - val_loss: 0.3560 - val_acc: 0.9130\n",
      "Epoch 434/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4457 - acc: 0.8740 - val_loss: 0.3559 - val_acc: 0.9140\n",
      "Epoch 435/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4465 - acc: 0.8710 - val_loss: 0.3553 - val_acc: 0.9130\n",
      "Epoch 436/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.4491 - acc: 0.8713 - val_loss: 0.3552 - val_acc: 0.9140\n",
      "Epoch 437/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4343 - acc: 0.8815 - val_loss: 0.3546 - val_acc: 0.9150\n",
      "Epoch 438/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4366 - acc: 0.8775 - val_loss: 0.3542 - val_acc: 0.9140\n",
      "Epoch 439/1000\n",
      "4000/4000 [==============================] - 0s 38us/step - loss: 0.4439 - acc: 0.8750 - val_loss: 0.3537 - val_acc: 0.9140\n",
      "Epoch 440/1000\n",
      "4000/4000 [==============================] - 0s 52us/step - loss: 0.4354 - acc: 0.8817 - val_loss: 0.3533 - val_acc: 0.9140\n",
      "Epoch 441/1000\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.4442 - acc: 0.8752 - val_loss: 0.3532 - val_acc: 0.9160\n",
      "Epoch 442/1000\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 0.4385 - acc: 0.8780 - val_loss: 0.3530 - val_acc: 0.9130\n",
      "Epoch 443/1000\n",
      "4000/4000 [==============================] - 0s 38us/step - loss: 0.4379 - acc: 0.8728 - val_loss: 0.3525 - val_acc: 0.9130\n",
      "Epoch 444/1000\n",
      "4000/4000 [==============================] - 0s 32us/step - loss: 0.4455 - acc: 0.8760 - val_loss: 0.3522 - val_acc: 0.9130\n",
      "Epoch 445/1000\n",
      "4000/4000 [==============================] - 0s 37us/step - loss: 0.4479 - acc: 0.8730 - val_loss: 0.3520 - val_acc: 0.9110\n",
      "Epoch 446/1000\n",
      "4000/4000 [==============================] - 0s 35us/step - loss: 0.4342 - acc: 0.8790 - val_loss: 0.3518 - val_acc: 0.9100\n",
      "Epoch 447/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.4341 - acc: 0.8732 - val_loss: 0.3513 - val_acc: 0.9120\n",
      "Epoch 448/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.4395 - acc: 0.8790 - val_loss: 0.3510 - val_acc: 0.9130\n",
      "Epoch 449/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4351 - acc: 0.8765 - val_loss: 0.3506 - val_acc: 0.9130\n",
      "Epoch 450/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4398 - acc: 0.8702 - val_loss: 0.3503 - val_acc: 0.9150\n",
      "Epoch 451/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.4382 - acc: 0.8775 - val_loss: 0.3501 - val_acc: 0.9140\n",
      "Epoch 452/1000\n",
      "4000/4000 [==============================] - 0s 32us/step - loss: 0.4401 - acc: 0.8743 - val_loss: 0.3498 - val_acc: 0.9140\n",
      "Epoch 453/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.4295 - acc: 0.8792 - val_loss: 0.3495 - val_acc: 0.9150\n",
      "Epoch 454/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4257 - acc: 0.8825 - val_loss: 0.3490 - val_acc: 0.9160\n",
      "Epoch 455/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4383 - acc: 0.8765 - val_loss: 0.3489 - val_acc: 0.9160\n",
      "Epoch 456/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4390 - acc: 0.8770 - val_loss: 0.3484 - val_acc: 0.9150\n",
      "Epoch 457/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.4298 - acc: 0.8812 - val_loss: 0.3481 - val_acc: 0.9160\n",
      "Epoch 458/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.4355 - acc: 0.8732 - val_loss: 0.3479 - val_acc: 0.9160\n",
      "Epoch 459/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.4274 - acc: 0.8768 - val_loss: 0.3476 - val_acc: 0.9160\n",
      "Epoch 460/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4406 - acc: 0.8745 - val_loss: 0.3474 - val_acc: 0.9160\n",
      "Epoch 461/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4237 - acc: 0.8852 - val_loss: 0.3469 - val_acc: 0.9160\n",
      "Epoch 462/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4400 - acc: 0.8708 - val_loss: 0.3468 - val_acc: 0.9160\n",
      "Epoch 463/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4252 - acc: 0.8838 - val_loss: 0.3466 - val_acc: 0.9160\n",
      "Epoch 464/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.4300 - acc: 0.8822 - val_loss: 0.3465 - val_acc: 0.9160\n",
      "Epoch 465/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.4231 - acc: 0.8815 - val_loss: 0.3460 - val_acc: 0.9160\n",
      "Epoch 466/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4229 - acc: 0.8838 - val_loss: 0.3458 - val_acc: 0.9160\n",
      "Epoch 467/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4192 - acc: 0.8825 - val_loss: 0.3456 - val_acc: 0.9160\n",
      "Epoch 468/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.4322 - acc: 0.8770 - val_loss: 0.3452 - val_acc: 0.9160\n",
      "Epoch 469/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4218 - acc: 0.8838 - val_loss: 0.3447 - val_acc: 0.9160\n",
      "Epoch 470/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.4293 - acc: 0.8847 - val_loss: 0.3444 - val_acc: 0.9160\n",
      "Epoch 471/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4285 - acc: 0.8810 - val_loss: 0.3442 - val_acc: 0.9160\n",
      "Epoch 472/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4268 - acc: 0.8808 - val_loss: 0.3439 - val_acc: 0.9160\n",
      "Epoch 473/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.4239 - acc: 0.8798 - val_loss: 0.3436 - val_acc: 0.9160\n",
      "Epoch 474/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4295 - acc: 0.8770 - val_loss: 0.3434 - val_acc: 0.9160\n",
      "Epoch 475/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4292 - acc: 0.8798 - val_loss: 0.3431 - val_acc: 0.9160\n",
      "Epoch 476/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4248 - acc: 0.8835 - val_loss: 0.3428 - val_acc: 0.9160\n",
      "Epoch 477/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.4249 - acc: 0.8785 - val_loss: 0.3424 - val_acc: 0.9160\n",
      "Epoch 478/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.4228 - acc: 0.8787 - val_loss: 0.3423 - val_acc: 0.9160\n",
      "Epoch 479/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4311 - acc: 0.8760 - val_loss: 0.3421 - val_acc: 0.9160\n",
      "Epoch 480/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.4279 - acc: 0.8840 - val_loss: 0.3417 - val_acc: 0.9160\n",
      "Epoch 481/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4197 - acc: 0.8805 - val_loss: 0.3414 - val_acc: 0.9160\n",
      "Epoch 482/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.4225 - acc: 0.8780 - val_loss: 0.3410 - val_acc: 0.9160\n",
      "Epoch 483/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.4268 - acc: 0.8713 - val_loss: 0.3407 - val_acc: 0.9160\n",
      "Epoch 484/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.4278 - acc: 0.8762 - val_loss: 0.3405 - val_acc: 0.9160\n",
      "Epoch 485/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.4293 - acc: 0.8775 - val_loss: 0.3404 - val_acc: 0.9160\n",
      "Epoch 486/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.4188 - acc: 0.8825 - val_loss: 0.3400 - val_acc: 0.9160\n",
      "Epoch 487/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4291 - acc: 0.8845 - val_loss: 0.3399 - val_acc: 0.9160\n",
      "Epoch 488/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.4234 - acc: 0.8810 - val_loss: 0.3398 - val_acc: 0.9170\n",
      "Epoch 489/1000\n",
      "4000/4000 [==============================] - 0s 32us/step - loss: 0.4147 - acc: 0.8845 - val_loss: 0.3395 - val_acc: 0.9170\n",
      "Epoch 490/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.4196 - acc: 0.8787 - val_loss: 0.3394 - val_acc: 0.9160\n",
      "Epoch 491/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4199 - acc: 0.8765 - val_loss: 0.3392 - val_acc: 0.9170\n",
      "Epoch 492/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.4162 - acc: 0.8822 - val_loss: 0.3390 - val_acc: 0.9170\n",
      "Epoch 493/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.4189 - acc: 0.8802 - val_loss: 0.3388 - val_acc: 0.9180\n",
      "Epoch 494/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4235 - acc: 0.8792 - val_loss: 0.3387 - val_acc: 0.9180\n",
      "Epoch 495/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4060 - acc: 0.8862 - val_loss: 0.3383 - val_acc: 0.9170\n",
      "Epoch 496/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.4172 - acc: 0.8822 - val_loss: 0.3380 - val_acc: 0.9180\n",
      "Epoch 497/1000\n",
      "4000/4000 [==============================] - 0s 36us/step - loss: 0.4172 - acc: 0.8868 - val_loss: 0.3378 - val_acc: 0.9170\n",
      "Epoch 498/1000\n",
      "4000/4000 [==============================] - 0s 32us/step - loss: 0.4212 - acc: 0.8780 - val_loss: 0.3375 - val_acc: 0.9170\n",
      "Epoch 499/1000\n",
      "4000/4000 [==============================] - 0s 35us/step - loss: 0.4179 - acc: 0.8785 - val_loss: 0.3372 - val_acc: 0.9170\n",
      "Epoch 500/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.4247 - acc: 0.8772 - val_loss: 0.3369 - val_acc: 0.9170\n",
      "Epoch 501/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.4189 - acc: 0.8817 - val_loss: 0.3366 - val_acc: 0.9170\n",
      "Epoch 502/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.4118 - acc: 0.8870 - val_loss: 0.3364 - val_acc: 0.9170\n",
      "Epoch 503/1000\n",
      "4000/4000 [==============================] - 0s 32us/step - loss: 0.4108 - acc: 0.8805 - val_loss: 0.3362 - val_acc: 0.9170\n",
      "Epoch 504/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.4178 - acc: 0.8810 - val_loss: 0.3360 - val_acc: 0.9170\n",
      "Epoch 505/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4158 - acc: 0.8795 - val_loss: 0.3358 - val_acc: 0.9170\n",
      "Epoch 506/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.4171 - acc: 0.8877 - val_loss: 0.3356 - val_acc: 0.9170\n",
      "Epoch 507/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.4090 - acc: 0.8862 - val_loss: 0.3355 - val_acc: 0.9170\n",
      "Epoch 508/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.4142 - acc: 0.8845 - val_loss: 0.3353 - val_acc: 0.9170\n",
      "Epoch 509/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.4109 - acc: 0.8865 - val_loss: 0.3350 - val_acc: 0.9180\n",
      "Epoch 510/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.4136 - acc: 0.8852 - val_loss: 0.3347 - val_acc: 0.9180\n",
      "Epoch 511/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.4128 - acc: 0.8810 - val_loss: 0.3346 - val_acc: 0.9180\n",
      "Epoch 512/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.4102 - acc: 0.8845 - val_loss: 0.3343 - val_acc: 0.9180\n",
      "Epoch 513/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.4025 - acc: 0.8912 - val_loss: 0.3338 - val_acc: 0.9180\n",
      "Epoch 514/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.4084 - acc: 0.8830 - val_loss: 0.3335 - val_acc: 0.9180\n",
      "Epoch 515/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4137 - acc: 0.8868 - val_loss: 0.3332 - val_acc: 0.9180\n",
      "Epoch 516/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4057 - acc: 0.8850 - val_loss: 0.3330 - val_acc: 0.9180\n",
      "Epoch 517/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4127 - acc: 0.8870 - val_loss: 0.3328 - val_acc: 0.9180\n",
      "Epoch 518/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4132 - acc: 0.8870 - val_loss: 0.3327 - val_acc: 0.9180\n",
      "Epoch 519/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4058 - acc: 0.8832 - val_loss: 0.3325 - val_acc: 0.9180\n",
      "Epoch 520/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.4098 - acc: 0.8835 - val_loss: 0.3322 - val_acc: 0.9180\n",
      "Epoch 521/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4069 - acc: 0.8830 - val_loss: 0.3321 - val_acc: 0.9170\n",
      "Epoch 522/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.4058 - acc: 0.8845 - val_loss: 0.3319 - val_acc: 0.9180\n",
      "Epoch 523/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.4156 - acc: 0.8757 - val_loss: 0.3316 - val_acc: 0.9180\n",
      "Epoch 524/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.4141 - acc: 0.8850 - val_loss: 0.3313 - val_acc: 0.9190\n",
      "Epoch 525/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.4029 - acc: 0.8830 - val_loss: 0.3312 - val_acc: 0.9190\n",
      "Epoch 526/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.4030 - acc: 0.8868 - val_loss: 0.3311 - val_acc: 0.9190\n",
      "Epoch 527/1000\n",
      "4000/4000 [==============================] - 0s 33us/step - loss: 0.4088 - acc: 0.8817 - val_loss: 0.3309 - val_acc: 0.9190\n",
      "Epoch 528/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.4020 - acc: 0.8847 - val_loss: 0.3308 - val_acc: 0.9190\n",
      "Epoch 529/1000\n",
      "4000/4000 [==============================] - 0s 35us/step - loss: 0.4106 - acc: 0.8835 - val_loss: 0.3307 - val_acc: 0.9190\n",
      "Epoch 530/1000\n",
      "4000/4000 [==============================] - 0s 32us/step - loss: 0.4159 - acc: 0.8850 - val_loss: 0.3306 - val_acc: 0.9180\n",
      "Epoch 531/1000\n",
      "4000/4000 [==============================] - 0s 34us/step - loss: 0.4023 - acc: 0.8840 - val_loss: 0.3306 - val_acc: 0.9190\n",
      "Epoch 532/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 35us/step - loss: 0.4048 - acc: 0.8882 - val_loss: 0.3303 - val_acc: 0.9190\n",
      "Epoch 533/1000\n",
      "4000/4000 [==============================] - 0s 36us/step - loss: 0.4037 - acc: 0.8817 - val_loss: 0.3300 - val_acc: 0.9190\n",
      "Epoch 534/1000\n",
      "4000/4000 [==============================] - 0s 36us/step - loss: 0.4042 - acc: 0.8858 - val_loss: 0.3295 - val_acc: 0.9190\n",
      "Epoch 535/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.4048 - acc: 0.8862 - val_loss: 0.3295 - val_acc: 0.9190\n",
      "Epoch 536/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.4015 - acc: 0.8840 - val_loss: 0.3292 - val_acc: 0.9190\n",
      "Epoch 537/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4049 - acc: 0.8850 - val_loss: 0.3289 - val_acc: 0.9190\n",
      "Epoch 538/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3998 - acc: 0.8860 - val_loss: 0.3288 - val_acc: 0.9190\n",
      "Epoch 539/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3994 - acc: 0.8862 - val_loss: 0.3285 - val_acc: 0.9200\n",
      "Epoch 540/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.4050 - acc: 0.8842 - val_loss: 0.3283 - val_acc: 0.9190\n",
      "Epoch 541/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.4157 - acc: 0.8812 - val_loss: 0.3281 - val_acc: 0.9190\n",
      "Epoch 542/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.4051 - acc: 0.8882 - val_loss: 0.3278 - val_acc: 0.9190\n",
      "Epoch 543/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.3912 - acc: 0.8890 - val_loss: 0.3276 - val_acc: 0.9190\n",
      "Epoch 544/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3906 - acc: 0.8900 - val_loss: 0.3276 - val_acc: 0.9190\n",
      "Epoch 545/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.4025 - acc: 0.8808 - val_loss: 0.3272 - val_acc: 0.9190\n",
      "Epoch 546/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.4014 - acc: 0.8877 - val_loss: 0.3270 - val_acc: 0.9190\n",
      "Epoch 547/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3985 - acc: 0.8875 - val_loss: 0.3267 - val_acc: 0.9190\n",
      "Epoch 548/1000\n",
      "4000/4000 [==============================] - 0s 38us/step - loss: 0.4074 - acc: 0.8870 - val_loss: 0.3266 - val_acc: 0.9190\n",
      "Epoch 549/1000\n",
      "4000/4000 [==============================] - 0s 33us/step - loss: 0.3958 - acc: 0.8930 - val_loss: 0.3264 - val_acc: 0.9180\n",
      "Epoch 550/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.4006 - acc: 0.8877 - val_loss: 0.3261 - val_acc: 0.9190\n",
      "Epoch 551/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3953 - acc: 0.8860 - val_loss: 0.3260 - val_acc: 0.9190\n",
      "Epoch 552/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3976 - acc: 0.8872 - val_loss: 0.3258 - val_acc: 0.9190\n",
      "Epoch 553/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3974 - acc: 0.8847 - val_loss: 0.3259 - val_acc: 0.9190\n",
      "Epoch 554/1000\n",
      "4000/4000 [==============================] - 0s 34us/step - loss: 0.3991 - acc: 0.8892 - val_loss: 0.3255 - val_acc: 0.9190\n",
      "Epoch 555/1000\n",
      "4000/4000 [==============================] - 0s 33us/step - loss: 0.3971 - acc: 0.8898 - val_loss: 0.3254 - val_acc: 0.9190\n",
      "Epoch 556/1000\n",
      "4000/4000 [==============================] - 0s 32us/step - loss: 0.3999 - acc: 0.8905 - val_loss: 0.3252 - val_acc: 0.9190\n",
      "Epoch 557/1000\n",
      "4000/4000 [==============================] - 0s 37us/step - loss: 0.3909 - acc: 0.8882 - val_loss: 0.3250 - val_acc: 0.9200\n",
      "Epoch 558/1000\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 0.3994 - acc: 0.8765 - val_loss: 0.3248 - val_acc: 0.9200\n",
      "Epoch 559/1000\n",
      "4000/4000 [==============================] - 0s 34us/step - loss: 0.3870 - acc: 0.8872 - val_loss: 0.3246 - val_acc: 0.9200\n",
      "Epoch 560/1000\n",
      "4000/4000 [==============================] - 0s 33us/step - loss: 0.4077 - acc: 0.8817 - val_loss: 0.3245 - val_acc: 0.9180\n",
      "Epoch 561/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.4023 - acc: 0.8868 - val_loss: 0.3244 - val_acc: 0.9190\n",
      "Epoch 562/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3994 - acc: 0.8885 - val_loss: 0.3242 - val_acc: 0.9190\n",
      "Epoch 563/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3962 - acc: 0.8875 - val_loss: 0.3239 - val_acc: 0.9190\n",
      "Epoch 564/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.3953 - acc: 0.8905 - val_loss: 0.3237 - val_acc: 0.9190\n",
      "Epoch 565/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.4068 - acc: 0.8838 - val_loss: 0.3236 - val_acc: 0.9200\n",
      "Epoch 566/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3990 - acc: 0.8850 - val_loss: 0.3234 - val_acc: 0.9200\n",
      "Epoch 567/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3968 - acc: 0.8845 - val_loss: 0.3233 - val_acc: 0.9200\n",
      "Epoch 568/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3832 - acc: 0.8948 - val_loss: 0.3231 - val_acc: 0.9190\n",
      "Epoch 569/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.3945 - acc: 0.8845 - val_loss: 0.3229 - val_acc: 0.9200\n",
      "Epoch 570/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3971 - acc: 0.8835 - val_loss: 0.3226 - val_acc: 0.9200\n",
      "Epoch 571/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3880 - acc: 0.8900 - val_loss: 0.3224 - val_acc: 0.9200\n",
      "Epoch 572/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3914 - acc: 0.8842 - val_loss: 0.3223 - val_acc: 0.9200\n",
      "Epoch 573/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3979 - acc: 0.8802 - val_loss: 0.3221 - val_acc: 0.9200\n",
      "Epoch 574/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3978 - acc: 0.8858 - val_loss: 0.3220 - val_acc: 0.9200\n",
      "Epoch 575/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3977 - acc: 0.8802 - val_loss: 0.3218 - val_acc: 0.9200\n",
      "Epoch 576/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3896 - acc: 0.8825 - val_loss: 0.3216 - val_acc: 0.9200\n",
      "Epoch 577/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3957 - acc: 0.8877 - val_loss: 0.3212 - val_acc: 0.9200\n",
      "Epoch 578/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3845 - acc: 0.8898 - val_loss: 0.3212 - val_acc: 0.9200\n",
      "Epoch 579/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3891 - acc: 0.8890 - val_loss: 0.3212 - val_acc: 0.9200\n",
      "Epoch 580/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3902 - acc: 0.8877 - val_loss: 0.3210 - val_acc: 0.9190\n",
      "Epoch 581/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.3883 - acc: 0.8890 - val_loss: 0.3209 - val_acc: 0.9190\n",
      "Epoch 582/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3964 - acc: 0.8842 - val_loss: 0.3206 - val_acc: 0.9190\n",
      "Epoch 583/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3886 - acc: 0.8905 - val_loss: 0.3203 - val_acc: 0.9200\n",
      "Epoch 584/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3912 - acc: 0.8882 - val_loss: 0.3203 - val_acc: 0.9200\n",
      "Epoch 585/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3943 - acc: 0.8890 - val_loss: 0.3200 - val_acc: 0.9190\n",
      "Epoch 586/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3849 - acc: 0.8920 - val_loss: 0.3198 - val_acc: 0.9180\n",
      "Epoch 587/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3943 - acc: 0.8842 - val_loss: 0.3198 - val_acc: 0.9180\n",
      "Epoch 588/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3851 - acc: 0.8930 - val_loss: 0.3194 - val_acc: 0.9190\n",
      "Epoch 589/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3850 - acc: 0.8882 - val_loss: 0.3193 - val_acc: 0.9190\n",
      "Epoch 590/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3892 - acc: 0.8912 - val_loss: 0.3191 - val_acc: 0.9190\n",
      "Epoch 591/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 35us/step - loss: 0.3892 - acc: 0.8895 - val_loss: 0.3190 - val_acc: 0.9190\n",
      "Epoch 592/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3851 - acc: 0.8898 - val_loss: 0.3190 - val_acc: 0.9190\n",
      "Epoch 593/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3886 - acc: 0.8882 - val_loss: 0.3187 - val_acc: 0.9200\n",
      "Epoch 594/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.3941 - acc: 0.8838 - val_loss: 0.3185 - val_acc: 0.9200\n",
      "Epoch 595/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3805 - acc: 0.8852 - val_loss: 0.3183 - val_acc: 0.9200\n",
      "Epoch 596/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3802 - acc: 0.8922 - val_loss: 0.3182 - val_acc: 0.9200\n",
      "Epoch 597/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3948 - acc: 0.8895 - val_loss: 0.3183 - val_acc: 0.9190\n",
      "Epoch 598/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3923 - acc: 0.8810 - val_loss: 0.3182 - val_acc: 0.9190\n",
      "Epoch 599/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3917 - acc: 0.8870 - val_loss: 0.3180 - val_acc: 0.9190\n",
      "Epoch 600/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3878 - acc: 0.8918 - val_loss: 0.3178 - val_acc: 0.9190\n",
      "Epoch 601/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3850 - acc: 0.8912 - val_loss: 0.3175 - val_acc: 0.9200\n",
      "Epoch 602/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3847 - acc: 0.8882 - val_loss: 0.3175 - val_acc: 0.9190\n",
      "Epoch 603/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3794 - acc: 0.8930 - val_loss: 0.3172 - val_acc: 0.9190\n",
      "Epoch 604/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3753 - acc: 0.8922 - val_loss: 0.3171 - val_acc: 0.9200\n",
      "Epoch 605/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3830 - acc: 0.8892 - val_loss: 0.3168 - val_acc: 0.9200\n",
      "Epoch 606/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.3815 - acc: 0.8888 - val_loss: 0.3166 - val_acc: 0.9200\n",
      "Epoch 607/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3874 - acc: 0.8845 - val_loss: 0.3163 - val_acc: 0.9200\n",
      "Epoch 608/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3822 - acc: 0.8975 - val_loss: 0.3162 - val_acc: 0.9200\n",
      "Epoch 609/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3899 - acc: 0.8885 - val_loss: 0.3159 - val_acc: 0.9200\n",
      "Epoch 610/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3793 - acc: 0.8905 - val_loss: 0.3158 - val_acc: 0.9200\n",
      "Epoch 611/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3888 - acc: 0.8842 - val_loss: 0.3157 - val_acc: 0.9200\n",
      "Epoch 612/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3919 - acc: 0.8825 - val_loss: 0.3155 - val_acc: 0.9200\n",
      "Epoch 613/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3851 - acc: 0.8918 - val_loss: 0.3154 - val_acc: 0.9200\n",
      "Epoch 614/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3855 - acc: 0.8890 - val_loss: 0.3151 - val_acc: 0.9200\n",
      "Epoch 615/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.3768 - acc: 0.8895 - val_loss: 0.3150 - val_acc: 0.9200\n",
      "Epoch 616/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3781 - acc: 0.8920 - val_loss: 0.3148 - val_acc: 0.9200\n",
      "Epoch 617/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3855 - acc: 0.8910 - val_loss: 0.3146 - val_acc: 0.9200\n",
      "Epoch 618/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.3868 - acc: 0.8860 - val_loss: 0.3145 - val_acc: 0.9200\n",
      "Epoch 619/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.3771 - acc: 0.8960 - val_loss: 0.3143 - val_acc: 0.9200\n",
      "Epoch 620/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3796 - acc: 0.8907 - val_loss: 0.3142 - val_acc: 0.9200\n",
      "Epoch 621/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3840 - acc: 0.8850 - val_loss: 0.3139 - val_acc: 0.9200\n",
      "Epoch 622/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3845 - acc: 0.8865 - val_loss: 0.3141 - val_acc: 0.9200\n",
      "Epoch 623/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3765 - acc: 0.8930 - val_loss: 0.3137 - val_acc: 0.9200\n",
      "Epoch 624/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3812 - acc: 0.8865 - val_loss: 0.3135 - val_acc: 0.9200\n",
      "Epoch 625/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3745 - acc: 0.8902 - val_loss: 0.3134 - val_acc: 0.9190\n",
      "Epoch 626/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3823 - acc: 0.8900 - val_loss: 0.3134 - val_acc: 0.9200\n",
      "Epoch 627/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3831 - acc: 0.8920 - val_loss: 0.3133 - val_acc: 0.9200\n",
      "Epoch 628/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3800 - acc: 0.8872 - val_loss: 0.3132 - val_acc: 0.9200\n",
      "Epoch 629/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3758 - acc: 0.8922 - val_loss: 0.3130 - val_acc: 0.9200\n",
      "Epoch 630/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3747 - acc: 0.8922 - val_loss: 0.3128 - val_acc: 0.9200\n",
      "Epoch 631/1000\n",
      "4000/4000 [==============================] - 0s 32us/step - loss: 0.3717 - acc: 0.8907 - val_loss: 0.3127 - val_acc: 0.9200\n",
      "Epoch 632/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.3746 - acc: 0.8952 - val_loss: 0.3126 - val_acc: 0.9200\n",
      "Epoch 633/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3764 - acc: 0.8930 - val_loss: 0.3123 - val_acc: 0.9200\n",
      "Epoch 634/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3719 - acc: 0.8940 - val_loss: 0.3122 - val_acc: 0.9200\n",
      "Epoch 635/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3753 - acc: 0.8930 - val_loss: 0.3121 - val_acc: 0.9200\n",
      "Epoch 636/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3799 - acc: 0.8928 - val_loss: 0.3119 - val_acc: 0.9200\n",
      "Epoch 637/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3736 - acc: 0.8932 - val_loss: 0.3118 - val_acc: 0.9200\n",
      "Epoch 638/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3692 - acc: 0.8952 - val_loss: 0.3116 - val_acc: 0.9200\n",
      "Epoch 639/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3771 - acc: 0.8945 - val_loss: 0.3115 - val_acc: 0.9200\n",
      "Epoch 640/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3786 - acc: 0.8912 - val_loss: 0.3113 - val_acc: 0.9190\n",
      "Epoch 641/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3812 - acc: 0.8918 - val_loss: 0.3113 - val_acc: 0.9200\n",
      "Epoch 642/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3723 - acc: 0.8907 - val_loss: 0.3111 - val_acc: 0.9200\n",
      "Epoch 643/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3742 - acc: 0.8970 - val_loss: 0.3109 - val_acc: 0.9200\n",
      "Epoch 644/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.3781 - acc: 0.8845 - val_loss: 0.3108 - val_acc: 0.9190\n",
      "Epoch 645/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3857 - acc: 0.8868 - val_loss: 0.3107 - val_acc: 0.9190\n",
      "Epoch 646/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3701 - acc: 0.8935 - val_loss: 0.3106 - val_acc: 0.9190\n",
      "Epoch 647/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3639 - acc: 0.8955 - val_loss: 0.3104 - val_acc: 0.9190\n",
      "Epoch 648/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3676 - acc: 0.8925 - val_loss: 0.3102 - val_acc: 0.9190\n",
      "Epoch 649/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3666 - acc: 0.8958 - val_loss: 0.3102 - val_acc: 0.9190\n",
      "Epoch 650/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3813 - acc: 0.8902 - val_loss: 0.3101 - val_acc: 0.9190\n",
      "Epoch 651/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3681 - acc: 0.8935 - val_loss: 0.3098 - val_acc: 0.9190\n",
      "Epoch 652/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3695 - acc: 0.8937 - val_loss: 0.3098 - val_acc: 0.9200\n",
      "Epoch 653/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3732 - acc: 0.8937 - val_loss: 0.3097 - val_acc: 0.9190\n",
      "Epoch 654/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3750 - acc: 0.8858 - val_loss: 0.3097 - val_acc: 0.9180\n",
      "Epoch 655/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3701 - acc: 0.8980 - val_loss: 0.3096 - val_acc: 0.9180\n",
      "Epoch 656/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3786 - acc: 0.8948 - val_loss: 0.3094 - val_acc: 0.9200\n",
      "Epoch 657/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3634 - acc: 0.8975 - val_loss: 0.3092 - val_acc: 0.9190\n",
      "Epoch 658/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.3804 - acc: 0.8890 - val_loss: 0.3093 - val_acc: 0.9190\n",
      "Epoch 659/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3707 - acc: 0.8955 - val_loss: 0.3091 - val_acc: 0.9210\n",
      "Epoch 660/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3625 - acc: 0.8945 - val_loss: 0.3089 - val_acc: 0.9200\n",
      "Epoch 661/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3698 - acc: 0.8960 - val_loss: 0.3087 - val_acc: 0.9200\n",
      "Epoch 662/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3759 - acc: 0.8910 - val_loss: 0.3087 - val_acc: 0.9190\n",
      "Epoch 663/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3612 - acc: 0.8980 - val_loss: 0.3085 - val_acc: 0.9200\n",
      "Epoch 664/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3727 - acc: 0.8925 - val_loss: 0.3084 - val_acc: 0.9200\n",
      "Epoch 665/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3662 - acc: 0.8905 - val_loss: 0.3084 - val_acc: 0.9200\n",
      "Epoch 666/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3612 - acc: 0.8970 - val_loss: 0.3081 - val_acc: 0.9200\n",
      "Epoch 667/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3607 - acc: 0.8978 - val_loss: 0.3079 - val_acc: 0.9200\n",
      "Epoch 668/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3598 - acc: 0.8972 - val_loss: 0.3077 - val_acc: 0.9190\n",
      "Epoch 669/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.3687 - acc: 0.8975 - val_loss: 0.3078 - val_acc: 0.9180\n",
      "Epoch 670/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.3670 - acc: 0.8995 - val_loss: 0.3075 - val_acc: 0.9190\n",
      "Epoch 671/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3569 - acc: 0.9010 - val_loss: 0.3072 - val_acc: 0.9190\n",
      "Epoch 672/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3690 - acc: 0.8922 - val_loss: 0.3072 - val_acc: 0.9200\n",
      "Epoch 673/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3765 - acc: 0.8958 - val_loss: 0.3073 - val_acc: 0.9180\n",
      "Epoch 674/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.3683 - acc: 0.8932 - val_loss: 0.3070 - val_acc: 0.9190\n",
      "Epoch 675/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3668 - acc: 0.8900 - val_loss: 0.3069 - val_acc: 0.9190\n",
      "Epoch 676/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3554 - acc: 0.8985 - val_loss: 0.3069 - val_acc: 0.9190\n",
      "Epoch 677/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3599 - acc: 0.8955 - val_loss: 0.3067 - val_acc: 0.9190\n",
      "Epoch 678/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3600 - acc: 0.8922 - val_loss: 0.3067 - val_acc: 0.9200\n",
      "Epoch 679/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3727 - acc: 0.8925 - val_loss: 0.3064 - val_acc: 0.9200\n",
      "Epoch 680/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3664 - acc: 0.8925 - val_loss: 0.3063 - val_acc: 0.9210\n",
      "Epoch 681/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3656 - acc: 0.8920 - val_loss: 0.3062 - val_acc: 0.9210\n",
      "Epoch 682/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.3633 - acc: 0.8965 - val_loss: 0.3062 - val_acc: 0.9210\n",
      "Epoch 683/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.3626 - acc: 0.8932 - val_loss: 0.3060 - val_acc: 0.9200\n",
      "Epoch 684/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3653 - acc: 0.8945 - val_loss: 0.3058 - val_acc: 0.9210\n",
      "Epoch 685/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3648 - acc: 0.8942 - val_loss: 0.3056 - val_acc: 0.9210\n",
      "Epoch 686/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3640 - acc: 0.8935 - val_loss: 0.3056 - val_acc: 0.9210\n",
      "Epoch 687/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3702 - acc: 0.8850 - val_loss: 0.3056 - val_acc: 0.9200\n",
      "Epoch 688/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3545 - acc: 0.8967 - val_loss: 0.3055 - val_acc: 0.9210\n",
      "Epoch 689/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3635 - acc: 0.8960 - val_loss: 0.3053 - val_acc: 0.9210\n",
      "Epoch 690/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3651 - acc: 0.8952 - val_loss: 0.3052 - val_acc: 0.9210\n",
      "Epoch 691/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3562 - acc: 0.8955 - val_loss: 0.3050 - val_acc: 0.9210\n",
      "Epoch 692/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.3611 - acc: 0.8945 - val_loss: 0.3048 - val_acc: 0.9210\n",
      "Epoch 693/1000\n",
      "4000/4000 [==============================] - 0s 35us/step - loss: 0.3551 - acc: 0.8990 - val_loss: 0.3047 - val_acc: 0.9210\n",
      "Epoch 694/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.3589 - acc: 0.8958 - val_loss: 0.3046 - val_acc: 0.9210\n",
      "Epoch 695/1000\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.3641 - acc: 0.8942 - val_loss: 0.3047 - val_acc: 0.9220\n",
      "Epoch 696/1000\n",
      "4000/4000 [==============================] - 0s 34us/step - loss: 0.3666 - acc: 0.8958 - val_loss: 0.3046 - val_acc: 0.9220\n",
      "Epoch 697/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.3636 - acc: 0.8970 - val_loss: 0.3044 - val_acc: 0.9220\n",
      "Epoch 698/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.3563 - acc: 0.8948 - val_loss: 0.3044 - val_acc: 0.9210\n",
      "Epoch 699/1000\n",
      "4000/4000 [==============================] - 0s 34us/step - loss: 0.3586 - acc: 0.8948 - val_loss: 0.3043 - val_acc: 0.9220\n",
      "Epoch 700/1000\n",
      "4000/4000 [==============================] - 0s 33us/step - loss: 0.3568 - acc: 0.8970 - val_loss: 0.3041 - val_acc: 0.9220\n",
      "Epoch 701/1000\n",
      "4000/4000 [==============================] - 0s 34us/step - loss: 0.3640 - acc: 0.8935 - val_loss: 0.3038 - val_acc: 0.9220\n",
      "Epoch 702/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3603 - acc: 0.8935 - val_loss: 0.3038 - val_acc: 0.9210\n",
      "Epoch 703/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3592 - acc: 0.8950 - val_loss: 0.3036 - val_acc: 0.9220\n",
      "Epoch 704/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3654 - acc: 0.8945 - val_loss: 0.3036 - val_acc: 0.9220\n",
      "Epoch 705/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.3658 - acc: 0.8950 - val_loss: 0.3037 - val_acc: 0.9220\n",
      "Epoch 706/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3627 - acc: 0.8935 - val_loss: 0.3036 - val_acc: 0.9200\n",
      "Epoch 707/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3602 - acc: 0.8950 - val_loss: 0.3037 - val_acc: 0.9200\n",
      "Epoch 708/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3580 - acc: 0.8975 - val_loss: 0.3034 - val_acc: 0.9210\n",
      "Epoch 709/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3556 - acc: 0.8975 - val_loss: 0.3033 - val_acc: 0.9210\n",
      "Epoch 710/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3629 - acc: 0.8930 - val_loss: 0.3034 - val_acc: 0.9210\n",
      "Epoch 711/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3625 - acc: 0.8937 - val_loss: 0.3032 - val_acc: 0.9220\n",
      "Epoch 712/1000\n",
      "4000/4000 [==============================] - 0s 32us/step - loss: 0.3675 - acc: 0.8922 - val_loss: 0.3031 - val_acc: 0.9210\n",
      "Epoch 713/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3585 - acc: 0.9018 - val_loss: 0.3028 - val_acc: 0.9210\n",
      "Epoch 714/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.3552 - acc: 0.8972 - val_loss: 0.3027 - val_acc: 0.9210\n",
      "Epoch 715/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3576 - acc: 0.8937 - val_loss: 0.3025 - val_acc: 0.9220\n",
      "Epoch 716/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3606 - acc: 0.8928 - val_loss: 0.3024 - val_acc: 0.9220\n",
      "Epoch 717/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.3516 - acc: 0.9018 - val_loss: 0.3021 - val_acc: 0.9220\n",
      "Epoch 718/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3520 - acc: 0.8958 - val_loss: 0.3020 - val_acc: 0.9220\n",
      "Epoch 719/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3606 - acc: 0.8975 - val_loss: 0.3020 - val_acc: 0.9220\n",
      "Epoch 720/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3592 - acc: 0.8948 - val_loss: 0.3022 - val_acc: 0.9210\n",
      "Epoch 721/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3595 - acc: 0.8962 - val_loss: 0.3019 - val_acc: 0.9230\n",
      "Epoch 722/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3521 - acc: 0.9015 - val_loss: 0.3018 - val_acc: 0.9230\n",
      "Epoch 723/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3660 - acc: 0.8895 - val_loss: 0.3018 - val_acc: 0.9220\n",
      "Epoch 724/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3524 - acc: 0.8967 - val_loss: 0.3018 - val_acc: 0.9220\n",
      "Epoch 725/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3562 - acc: 0.8932 - val_loss: 0.3017 - val_acc: 0.9210\n",
      "Epoch 726/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3596 - acc: 0.8937 - val_loss: 0.3016 - val_acc: 0.9220\n",
      "Epoch 727/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3534 - acc: 0.8972 - val_loss: 0.3015 - val_acc: 0.9210\n",
      "Epoch 728/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3465 - acc: 0.9005 - val_loss: 0.3013 - val_acc: 0.9220\n",
      "Epoch 729/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.3647 - acc: 0.8962 - val_loss: 0.3011 - val_acc: 0.9210\n",
      "Epoch 730/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.3477 - acc: 0.9040 - val_loss: 0.3009 - val_acc: 0.9230\n",
      "Epoch 731/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.3497 - acc: 0.8995 - val_loss: 0.3008 - val_acc: 0.9220\n",
      "Epoch 732/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3567 - acc: 0.8940 - val_loss: 0.3009 - val_acc: 0.9210\n",
      "Epoch 733/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3533 - acc: 0.8982 - val_loss: 0.3008 - val_acc: 0.9220\n",
      "Epoch 734/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3574 - acc: 0.8970 - val_loss: 0.3006 - val_acc: 0.9210\n",
      "Epoch 735/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3546 - acc: 0.8985 - val_loss: 0.3006 - val_acc: 0.9210\n",
      "Epoch 736/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3432 - acc: 0.8995 - val_loss: 0.3003 - val_acc: 0.9210\n",
      "Epoch 737/1000\n",
      "4000/4000 [==============================] - 0s 35us/step - loss: 0.3585 - acc: 0.8952 - val_loss: 0.3004 - val_acc: 0.9210\n",
      "Epoch 738/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3726 - acc: 0.8885 - val_loss: 0.3006 - val_acc: 0.9200\n",
      "Epoch 739/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3478 - acc: 0.8922 - val_loss: 0.3005 - val_acc: 0.9200\n",
      "Epoch 740/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.3510 - acc: 0.8980 - val_loss: 0.3004 - val_acc: 0.9200\n",
      "Epoch 741/1000\n",
      "4000/4000 [==============================] - 0s 32us/step - loss: 0.3563 - acc: 0.8915 - val_loss: 0.3003 - val_acc: 0.9210\n",
      "Epoch 742/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3504 - acc: 0.8952 - val_loss: 0.3002 - val_acc: 0.9210\n",
      "Epoch 743/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3549 - acc: 0.9012 - val_loss: 0.3002 - val_acc: 0.9230\n",
      "Epoch 744/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3507 - acc: 0.9015 - val_loss: 0.3000 - val_acc: 0.9230\n",
      "Epoch 745/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3444 - acc: 0.9025 - val_loss: 0.3000 - val_acc: 0.9230\n",
      "Epoch 746/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3505 - acc: 0.9002 - val_loss: 0.2999 - val_acc: 0.9230\n",
      "Epoch 747/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3512 - acc: 0.8980 - val_loss: 0.2998 - val_acc: 0.9210\n",
      "Epoch 748/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3521 - acc: 0.8967 - val_loss: 0.2997 - val_acc: 0.9200\n",
      "Epoch 749/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3528 - acc: 0.8937 - val_loss: 0.2996 - val_acc: 0.9220\n",
      "Epoch 750/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3401 - acc: 0.9035 - val_loss: 0.2994 - val_acc: 0.9230\n",
      "Epoch 751/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3503 - acc: 0.9000 - val_loss: 0.2993 - val_acc: 0.9230\n",
      "Epoch 752/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3453 - acc: 0.8997 - val_loss: 0.2991 - val_acc: 0.9220\n",
      "Epoch 753/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3519 - acc: 0.8975 - val_loss: 0.2990 - val_acc: 0.9220\n",
      "Epoch 754/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.3454 - acc: 0.9008 - val_loss: 0.2989 - val_acc: 0.9200\n",
      "Epoch 755/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3530 - acc: 0.8960 - val_loss: 0.2990 - val_acc: 0.9200\n",
      "Epoch 756/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3510 - acc: 0.8982 - val_loss: 0.2990 - val_acc: 0.9190\n",
      "Epoch 757/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3441 - acc: 0.9005 - val_loss: 0.2988 - val_acc: 0.9190\n",
      "Epoch 758/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3508 - acc: 0.9000 - val_loss: 0.2985 - val_acc: 0.9200\n",
      "Epoch 759/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3594 - acc: 0.8940 - val_loss: 0.2984 - val_acc: 0.9220\n",
      "Epoch 760/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3412 - acc: 0.9010 - val_loss: 0.2983 - val_acc: 0.9220\n",
      "Epoch 761/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3448 - acc: 0.9002 - val_loss: 0.2980 - val_acc: 0.9220\n",
      "Epoch 762/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3414 - acc: 0.9032 - val_loss: 0.2981 - val_acc: 0.9210\n",
      "Epoch 763/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3505 - acc: 0.8960 - val_loss: 0.2980 - val_acc: 0.9220\n",
      "Epoch 764/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3493 - acc: 0.8975 - val_loss: 0.2980 - val_acc: 0.9220\n",
      "Epoch 765/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3498 - acc: 0.8995 - val_loss: 0.2978 - val_acc: 0.9210\n",
      "Epoch 766/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3500 - acc: 0.8982 - val_loss: 0.2978 - val_acc: 0.9220\n",
      "Epoch 767/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.3421 - acc: 0.9022 - val_loss: 0.2976 - val_acc: 0.9220\n",
      "Epoch 768/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3533 - acc: 0.8962 - val_loss: 0.2977 - val_acc: 0.9220\n",
      "Epoch 769/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3546 - acc: 0.8995 - val_loss: 0.2975 - val_acc: 0.9220\n",
      "Epoch 770/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3496 - acc: 0.8992 - val_loss: 0.2973 - val_acc: 0.9220\n",
      "Epoch 771/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3501 - acc: 0.8940 - val_loss: 0.2973 - val_acc: 0.9220\n",
      "Epoch 772/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3418 - acc: 0.9018 - val_loss: 0.2972 - val_acc: 0.9230\n",
      "Epoch 773/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3459 - acc: 0.9018 - val_loss: 0.2971 - val_acc: 0.9230\n",
      "Epoch 774/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3493 - acc: 0.8985 - val_loss: 0.2970 - val_acc: 0.9210\n",
      "Epoch 775/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3515 - acc: 0.8935 - val_loss: 0.2969 - val_acc: 0.9210\n",
      "Epoch 776/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3528 - acc: 0.8992 - val_loss: 0.2968 - val_acc: 0.9210\n",
      "Epoch 777/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3384 - acc: 0.9040 - val_loss: 0.2969 - val_acc: 0.9210\n",
      "Epoch 778/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3445 - acc: 0.8965 - val_loss: 0.2969 - val_acc: 0.9210\n",
      "Epoch 779/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.3438 - acc: 0.8962 - val_loss: 0.2968 - val_acc: 0.9210\n",
      "Epoch 780/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3468 - acc: 0.8985 - val_loss: 0.2966 - val_acc: 0.9210\n",
      "Epoch 781/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3496 - acc: 0.8982 - val_loss: 0.2966 - val_acc: 0.9210\n",
      "Epoch 782/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3500 - acc: 0.8990 - val_loss: 0.2965 - val_acc: 0.9220\n",
      "Epoch 783/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3429 - acc: 0.9018 - val_loss: 0.2965 - val_acc: 0.9220\n",
      "Epoch 784/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3399 - acc: 0.8997 - val_loss: 0.2967 - val_acc: 0.9220\n",
      "Epoch 785/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3399 - acc: 0.9000 - val_loss: 0.2964 - val_acc: 0.9220\n",
      "Epoch 786/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3414 - acc: 0.9008 - val_loss: 0.2962 - val_acc: 0.9220\n",
      "Epoch 787/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3423 - acc: 0.9022 - val_loss: 0.2962 - val_acc: 0.9210\n",
      "Epoch 788/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3465 - acc: 0.8960 - val_loss: 0.2960 - val_acc: 0.9210\n",
      "Epoch 789/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3395 - acc: 0.9002 - val_loss: 0.2958 - val_acc: 0.9210\n",
      "Epoch 790/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3517 - acc: 0.8962 - val_loss: 0.2957 - val_acc: 0.9210\n",
      "Epoch 791/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3553 - acc: 0.8948 - val_loss: 0.2957 - val_acc: 0.9220\n",
      "Epoch 792/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.3470 - acc: 0.8980 - val_loss: 0.2958 - val_acc: 0.9210\n",
      "Epoch 793/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3401 - acc: 0.8975 - val_loss: 0.2957 - val_acc: 0.9210\n",
      "Epoch 794/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3431 - acc: 0.9030 - val_loss: 0.2955 - val_acc: 0.9220\n",
      "Epoch 795/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3455 - acc: 0.9025 - val_loss: 0.2955 - val_acc: 0.9210\n",
      "Epoch 796/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3426 - acc: 0.9000 - val_loss: 0.2955 - val_acc: 0.9210\n",
      "Epoch 797/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3503 - acc: 0.8948 - val_loss: 0.2955 - val_acc: 0.9210\n",
      "Epoch 798/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3423 - acc: 0.9048 - val_loss: 0.2955 - val_acc: 0.9220\n",
      "Epoch 799/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3346 - acc: 0.9035 - val_loss: 0.2955 - val_acc: 0.9210\n",
      "Epoch 800/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3442 - acc: 0.9010 - val_loss: 0.2953 - val_acc: 0.9200\n",
      "Epoch 801/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3515 - acc: 0.8985 - val_loss: 0.2951 - val_acc: 0.9220\n",
      "Epoch 802/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3446 - acc: 0.8982 - val_loss: 0.2952 - val_acc: 0.9210\n",
      "Epoch 803/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3403 - acc: 0.8995 - val_loss: 0.2952 - val_acc: 0.9210\n",
      "Epoch 804/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.3483 - acc: 0.9035 - val_loss: 0.2951 - val_acc: 0.9220\n",
      "Epoch 805/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3388 - acc: 0.9018 - val_loss: 0.2949 - val_acc: 0.9210\n",
      "Epoch 806/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3380 - acc: 0.9022 - val_loss: 0.2948 - val_acc: 0.9220\n",
      "Epoch 807/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3397 - acc: 0.8972 - val_loss: 0.2945 - val_acc: 0.9220\n",
      "Epoch 808/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3423 - acc: 0.9045 - val_loss: 0.2944 - val_acc: 0.9200\n",
      "Epoch 809/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3495 - acc: 0.8975 - val_loss: 0.2943 - val_acc: 0.9200\n",
      "Epoch 810/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3376 - acc: 0.9055 - val_loss: 0.2943 - val_acc: 0.9210\n",
      "Epoch 811/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3418 - acc: 0.8967 - val_loss: 0.2943 - val_acc: 0.9240\n",
      "Epoch 812/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3374 - acc: 0.9025 - val_loss: 0.2941 - val_acc: 0.9230\n",
      "Epoch 813/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3413 - acc: 0.9040 - val_loss: 0.2940 - val_acc: 0.9230\n",
      "Epoch 814/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3420 - acc: 0.8988 - val_loss: 0.2939 - val_acc: 0.9220\n",
      "Epoch 815/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3321 - acc: 0.8972 - val_loss: 0.2938 - val_acc: 0.9220\n",
      "Epoch 816/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.3446 - acc: 0.8972 - val_loss: 0.2937 - val_acc: 0.9220\n",
      "Epoch 817/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3358 - acc: 0.9008 - val_loss: 0.2936 - val_acc: 0.9230\n",
      "Epoch 818/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3328 - acc: 0.9032 - val_loss: 0.2933 - val_acc: 0.9230\n",
      "Epoch 819/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3371 - acc: 0.9015 - val_loss: 0.2934 - val_acc: 0.9230\n",
      "Epoch 820/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3382 - acc: 0.9015 - val_loss: 0.2934 - val_acc: 0.9230\n",
      "Epoch 821/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3356 - acc: 0.8992 - val_loss: 0.2934 - val_acc: 0.9210\n",
      "Epoch 822/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3412 - acc: 0.8985 - val_loss: 0.2935 - val_acc: 0.9230\n",
      "Epoch 823/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3391 - acc: 0.8992 - val_loss: 0.2934 - val_acc: 0.9240\n",
      "Epoch 824/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3449 - acc: 0.9062 - val_loss: 0.2931 - val_acc: 0.9220\n",
      "Epoch 825/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3330 - acc: 0.9020 - val_loss: 0.2929 - val_acc: 0.9220\n",
      "Epoch 826/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3420 - acc: 0.8988 - val_loss: 0.2930 - val_acc: 0.9240\n",
      "Epoch 827/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3297 - acc: 0.9030 - val_loss: 0.2928 - val_acc: 0.9240\n",
      "Epoch 828/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.3295 - acc: 0.9002 - val_loss: 0.2926 - val_acc: 0.9230\n",
      "Epoch 829/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.3369 - acc: 0.8978 - val_loss: 0.2925 - val_acc: 0.9220\n",
      "Epoch 830/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3358 - acc: 0.9022 - val_loss: 0.2926 - val_acc: 0.9240\n",
      "Epoch 831/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3323 - acc: 0.9050 - val_loss: 0.2923 - val_acc: 0.9230\n",
      "Epoch 832/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3371 - acc: 0.9000 - val_loss: 0.2923 - val_acc: 0.9240\n",
      "Epoch 833/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3428 - acc: 0.9027 - val_loss: 0.2922 - val_acc: 0.9240\n",
      "Epoch 834/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3431 - acc: 0.8970 - val_loss: 0.2921 - val_acc: 0.9250\n",
      "Epoch 835/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3300 - acc: 0.9073 - val_loss: 0.2919 - val_acc: 0.9230\n",
      "Epoch 836/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3284 - acc: 0.9065 - val_loss: 0.2918 - val_acc: 0.9220\n",
      "Epoch 837/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3343 - acc: 0.9010 - val_loss: 0.2919 - val_acc: 0.9240\n",
      "Epoch 838/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3414 - acc: 0.8982 - val_loss: 0.2919 - val_acc: 0.9210\n",
      "Epoch 839/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3388 - acc: 0.9018 - val_loss: 0.2917 - val_acc: 0.9220\n",
      "Epoch 840/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3311 - acc: 0.9080 - val_loss: 0.2917 - val_acc: 0.9220\n",
      "Epoch 841/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.3400 - acc: 0.8978 - val_loss: 0.2917 - val_acc: 0.9230\n",
      "Epoch 842/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3336 - acc: 0.9025 - val_loss: 0.2917 - val_acc: 0.9240\n",
      "Epoch 843/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3447 - acc: 0.8965 - val_loss: 0.2915 - val_acc: 0.9220\n",
      "Epoch 844/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3334 - acc: 0.9057 - val_loss: 0.2915 - val_acc: 0.9230\n",
      "Epoch 845/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3273 - acc: 0.9008 - val_loss: 0.2912 - val_acc: 0.9220\n",
      "Epoch 846/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3406 - acc: 0.8992 - val_loss: 0.2911 - val_acc: 0.9240\n",
      "Epoch 847/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3395 - acc: 0.8955 - val_loss: 0.2910 - val_acc: 0.9240\n",
      "Epoch 848/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3301 - acc: 0.9022 - val_loss: 0.2911 - val_acc: 0.9230\n",
      "Epoch 849/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3365 - acc: 0.8990 - val_loss: 0.2912 - val_acc: 0.9240\n",
      "Epoch 850/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3415 - acc: 0.8972 - val_loss: 0.2911 - val_acc: 0.9230\n",
      "Epoch 851/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3329 - acc: 0.9040 - val_loss: 0.2911 - val_acc: 0.9240\n",
      "Epoch 852/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3317 - acc: 0.9045 - val_loss: 0.2911 - val_acc: 0.9230\n",
      "Epoch 853/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3379 - acc: 0.8980 - val_loss: 0.2910 - val_acc: 0.9230\n",
      "Epoch 854/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3371 - acc: 0.9045 - val_loss: 0.2911 - val_acc: 0.9230\n",
      "Epoch 855/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.3315 - acc: 0.9005 - val_loss: 0.2910 - val_acc: 0.9230\n",
      "Epoch 856/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3314 - acc: 0.9032 - val_loss: 0.2908 - val_acc: 0.9230\n",
      "Epoch 857/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3279 - acc: 0.9077 - val_loss: 0.2908 - val_acc: 0.9230\n",
      "Epoch 858/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3291 - acc: 0.9002 - val_loss: 0.2907 - val_acc: 0.9220\n",
      "Epoch 859/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3285 - acc: 0.9062 - val_loss: 0.2905 - val_acc: 0.9220\n",
      "Epoch 860/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3273 - acc: 0.9015 - val_loss: 0.2904 - val_acc: 0.9230\n",
      "Epoch 861/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3270 - acc: 0.9018 - val_loss: 0.2903 - val_acc: 0.9230\n",
      "Epoch 862/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3329 - acc: 0.8992 - val_loss: 0.2900 - val_acc: 0.9220\n",
      "Epoch 863/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3372 - acc: 0.8985 - val_loss: 0.2900 - val_acc: 0.9230\n",
      "Epoch 864/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3274 - acc: 0.9022 - val_loss: 0.2900 - val_acc: 0.9230\n",
      "Epoch 865/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3309 - acc: 0.9035 - val_loss: 0.2899 - val_acc: 0.9240\n",
      "Epoch 866/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3329 - acc: 0.9000 - val_loss: 0.2899 - val_acc: 0.9250\n",
      "Epoch 867/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.3272 - acc: 0.9032 - val_loss: 0.2900 - val_acc: 0.9250\n",
      "Epoch 868/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3305 - acc: 0.9027 - val_loss: 0.2899 - val_acc: 0.9250\n",
      "Epoch 869/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3337 - acc: 0.9040 - val_loss: 0.2898 - val_acc: 0.9240\n",
      "Epoch 870/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3335 - acc: 0.9030 - val_loss: 0.2897 - val_acc: 0.9240\n",
      "Epoch 871/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3303 - acc: 0.9050 - val_loss: 0.2896 - val_acc: 0.9250\n",
      "Epoch 872/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3351 - acc: 0.8992 - val_loss: 0.2896 - val_acc: 0.9260\n",
      "Epoch 873/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3341 - acc: 0.9000 - val_loss: 0.2896 - val_acc: 0.9260\n",
      "Epoch 874/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3342 - acc: 0.9035 - val_loss: 0.2895 - val_acc: 0.9250\n",
      "Epoch 875/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3319 - acc: 0.9030 - val_loss: 0.2892 - val_acc: 0.9260\n",
      "Epoch 876/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3306 - acc: 0.9050 - val_loss: 0.2891 - val_acc: 0.9250\n",
      "Epoch 877/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3193 - acc: 0.9075 - val_loss: 0.2892 - val_acc: 0.9250\n",
      "Epoch 878/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3240 - acc: 0.8985 - val_loss: 0.2890 - val_acc: 0.9250\n",
      "Epoch 879/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3321 - acc: 0.9038 - val_loss: 0.2890 - val_acc: 0.9260\n",
      "Epoch 880/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.3298 - acc: 0.9027 - val_loss: 0.2887 - val_acc: 0.9260\n",
      "Epoch 881/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3353 - acc: 0.9030 - val_loss: 0.2888 - val_acc: 0.9260\n",
      "Epoch 882/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3274 - acc: 0.9065 - val_loss: 0.2887 - val_acc: 0.9250\n",
      "Epoch 883/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3317 - acc: 0.9042 - val_loss: 0.2886 - val_acc: 0.9250\n",
      "Epoch 884/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3287 - acc: 0.9045 - val_loss: 0.2886 - val_acc: 0.9240\n",
      "Epoch 885/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3244 - acc: 0.9012 - val_loss: 0.2885 - val_acc: 0.9240\n",
      "Epoch 886/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3265 - acc: 0.9083 - val_loss: 0.2885 - val_acc: 0.9250\n",
      "Epoch 887/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3342 - acc: 0.9020 - val_loss: 0.2885 - val_acc: 0.9250\n",
      "Epoch 888/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3345 - acc: 0.9010 - val_loss: 0.2886 - val_acc: 0.9250\n",
      "Epoch 889/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3219 - acc: 0.9025 - val_loss: 0.2884 - val_acc: 0.9250\n",
      "Epoch 890/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3298 - acc: 0.9000 - val_loss: 0.2884 - val_acc: 0.9260\n",
      "Epoch 891/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3309 - acc: 0.9035 - val_loss: 0.2882 - val_acc: 0.9250\n",
      "Epoch 892/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.3311 - acc: 0.9030 - val_loss: 0.2882 - val_acc: 0.9250\n",
      "Epoch 893/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3278 - acc: 0.9020 - val_loss: 0.2881 - val_acc: 0.9260\n",
      "Epoch 894/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3265 - acc: 0.9035 - val_loss: 0.2882 - val_acc: 0.9250\n",
      "Epoch 895/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3324 - acc: 0.8985 - val_loss: 0.2882 - val_acc: 0.9250\n",
      "Epoch 896/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3306 - acc: 0.9042 - val_loss: 0.2882 - val_acc: 0.9250\n",
      "Epoch 897/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3225 - acc: 0.9048 - val_loss: 0.2881 - val_acc: 0.9250\n",
      "Epoch 898/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3233 - acc: 0.9065 - val_loss: 0.2882 - val_acc: 0.9250\n",
      "Epoch 899/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3261 - acc: 0.9018 - val_loss: 0.2881 - val_acc: 0.9250\n",
      "Epoch 900/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3257 - acc: 0.9075 - val_loss: 0.2878 - val_acc: 0.9250\n",
      "Epoch 901/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3248 - acc: 0.9077 - val_loss: 0.2878 - val_acc: 0.9250\n",
      "Epoch 902/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3272 - acc: 0.9040 - val_loss: 0.2878 - val_acc: 0.9240\n",
      "Epoch 903/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3199 - acc: 0.9065 - val_loss: 0.2875 - val_acc: 0.9240\n",
      "Epoch 904/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3248 - acc: 0.9022 - val_loss: 0.2877 - val_acc: 0.9250\n",
      "Epoch 905/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.3290 - acc: 0.9050 - val_loss: 0.2877 - val_acc: 0.9250\n",
      "Epoch 906/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3235 - acc: 0.9080 - val_loss: 0.2873 - val_acc: 0.9250\n",
      "Epoch 907/1000\n",
      "4000/4000 [==============================] - 0s 23us/step - loss: 0.3324 - acc: 0.9057 - val_loss: 0.2873 - val_acc: 0.9250\n",
      "Epoch 908/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3210 - acc: 0.9087 - val_loss: 0.2873 - val_acc: 0.9250\n",
      "Epoch 909/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3226 - acc: 0.9100 - val_loss: 0.2872 - val_acc: 0.9250\n",
      "Epoch 910/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3342 - acc: 0.8962 - val_loss: 0.2872 - val_acc: 0.9250\n",
      "Epoch 911/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3301 - acc: 0.9025 - val_loss: 0.2871 - val_acc: 0.9260\n",
      "Epoch 912/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3290 - acc: 0.9020 - val_loss: 0.2870 - val_acc: 0.9250\n",
      "Epoch 913/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3239 - acc: 0.9025 - val_loss: 0.2869 - val_acc: 0.9250\n",
      "Epoch 914/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3351 - acc: 0.9002 - val_loss: 0.2870 - val_acc: 0.9260\n",
      "Epoch 915/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3273 - acc: 0.9035 - val_loss: 0.2870 - val_acc: 0.9260\n",
      "Epoch 916/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3229 - acc: 0.9032 - val_loss: 0.2869 - val_acc: 0.9250\n",
      "Epoch 917/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3195 - acc: 0.9052 - val_loss: 0.2867 - val_acc: 0.9240\n",
      "Epoch 918/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3261 - acc: 0.9057 - val_loss: 0.2866 - val_acc: 0.9250\n",
      "Epoch 919/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3197 - acc: 0.9040 - val_loss: 0.2865 - val_acc: 0.9240\n",
      "Epoch 920/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3210 - acc: 0.9055 - val_loss: 0.2865 - val_acc: 0.9250\n",
      "Epoch 921/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3237 - acc: 0.9032 - val_loss: 0.2865 - val_acc: 0.9250\n",
      "Epoch 922/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3294 - acc: 0.9035 - val_loss: 0.2865 - val_acc: 0.9250\n",
      "Epoch 923/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3036 - acc: 0.9165 - val_loss: 0.2864 - val_acc: 0.9250\n",
      "Epoch 924/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3263 - acc: 0.9032 - val_loss: 0.2862 - val_acc: 0.9240\n",
      "Epoch 925/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3202 - acc: 0.9060 - val_loss: 0.2863 - val_acc: 0.9260\n",
      "Epoch 926/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3213 - acc: 0.9035 - val_loss: 0.2863 - val_acc: 0.9260\n",
      "Epoch 927/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3273 - acc: 0.9015 - val_loss: 0.2861 - val_acc: 0.9260\n",
      "Epoch 928/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3315 - acc: 0.9042 - val_loss: 0.2860 - val_acc: 0.9260\n",
      "Epoch 929/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3232 - acc: 0.9022 - val_loss: 0.2860 - val_acc: 0.9260\n",
      "Epoch 930/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.3207 - acc: 0.9020 - val_loss: 0.2859 - val_acc: 0.9260\n",
      "Epoch 931/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3327 - acc: 0.9020 - val_loss: 0.2860 - val_acc: 0.9260\n",
      "Epoch 932/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3158 - acc: 0.9075 - val_loss: 0.2858 - val_acc: 0.9260\n",
      "Epoch 933/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3277 - acc: 0.9012 - val_loss: 0.2858 - val_acc: 0.9260\n",
      "Epoch 934/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3180 - acc: 0.9085 - val_loss: 0.2857 - val_acc: 0.9250\n",
      "Epoch 935/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3297 - acc: 0.9032 - val_loss: 0.2858 - val_acc: 0.9250\n",
      "Epoch 936/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3156 - acc: 0.9075 - val_loss: 0.2859 - val_acc: 0.9260\n",
      "Epoch 937/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3231 - acc: 0.9073 - val_loss: 0.2858 - val_acc: 0.9260\n",
      "Epoch 938/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3236 - acc: 0.9062 - val_loss: 0.2856 - val_acc: 0.9260\n",
      "Epoch 939/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3187 - acc: 0.9048 - val_loss: 0.2854 - val_acc: 0.9260\n",
      "Epoch 940/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3346 - acc: 0.8995 - val_loss: 0.2854 - val_acc: 0.9260\n",
      "Epoch 941/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3141 - acc: 0.9107 - val_loss: 0.2854 - val_acc: 0.9260\n",
      "Epoch 942/1000\n",
      "4000/4000 [==============================] - 0s 30us/step - loss: 0.3210 - acc: 0.9062 - val_loss: 0.2855 - val_acc: 0.9260\n",
      "Epoch 943/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3237 - acc: 0.9068 - val_loss: 0.2855 - val_acc: 0.9260\n",
      "Epoch 944/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3079 - acc: 0.9083 - val_loss: 0.2854 - val_acc: 0.9260\n",
      "Epoch 945/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3205 - acc: 0.9073 - val_loss: 0.2854 - val_acc: 0.9260\n",
      "Epoch 946/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3115 - acc: 0.9075 - val_loss: 0.2853 - val_acc: 0.9260\n",
      "Epoch 947/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3247 - acc: 0.9035 - val_loss: 0.2852 - val_acc: 0.9260\n",
      "Epoch 948/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3176 - acc: 0.9085 - val_loss: 0.2852 - val_acc: 0.9260\n",
      "Epoch 949/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3254 - acc: 0.9060 - val_loss: 0.2852 - val_acc: 0.9260\n",
      "Epoch 950/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3115 - acc: 0.9083 - val_loss: 0.2854 - val_acc: 0.9260\n",
      "Epoch 951/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3189 - acc: 0.9080 - val_loss: 0.2852 - val_acc: 0.9260\n",
      "Epoch 952/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3228 - acc: 0.8997 - val_loss: 0.2853 - val_acc: 0.9260\n",
      "Epoch 953/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3188 - acc: 0.9093 - val_loss: 0.2852 - val_acc: 0.9260\n",
      "Epoch 954/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3116 - acc: 0.9107 - val_loss: 0.2849 - val_acc: 0.9260\n",
      "Epoch 955/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3091 - acc: 0.9107 - val_loss: 0.2847 - val_acc: 0.9260\n",
      "Epoch 956/1000\n",
      "4000/4000 [==============================] - 0s 31us/step - loss: 0.3140 - acc: 0.9075 - val_loss: 0.2846 - val_acc: 0.9260\n",
      "Epoch 957/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.3141 - acc: 0.9107 - val_loss: 0.2847 - val_acc: 0.9260\n",
      "Epoch 958/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3231 - acc: 0.9025 - val_loss: 0.2846 - val_acc: 0.9260\n",
      "Epoch 959/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3119 - acc: 0.9052 - val_loss: 0.2844 - val_acc: 0.9260\n",
      "Epoch 960/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3144 - acc: 0.9110 - val_loss: 0.2842 - val_acc: 0.9260\n",
      "Epoch 961/1000\n",
      "4000/4000 [==============================] - 0s 27us/step - loss: 0.3132 - acc: 0.9117 - val_loss: 0.2841 - val_acc: 0.9270\n",
      "Epoch 962/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3235 - acc: 0.9085 - val_loss: 0.2840 - val_acc: 0.9270\n",
      "Epoch 963/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3136 - acc: 0.9057 - val_loss: 0.2839 - val_acc: 0.9270\n",
      "Epoch 964/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3110 - acc: 0.9100 - val_loss: 0.2839 - val_acc: 0.9270\n",
      "Epoch 965/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3093 - acc: 0.9103 - val_loss: 0.2840 - val_acc: 0.9270\n",
      "Epoch 966/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3179 - acc: 0.9042 - val_loss: 0.2839 - val_acc: 0.9270\n",
      "Epoch 967/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3138 - acc: 0.9062 - val_loss: 0.2837 - val_acc: 0.9270\n",
      "Epoch 968/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.3113 - acc: 0.9070 - val_loss: 0.2838 - val_acc: 0.9270\n",
      "Epoch 969/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3160 - acc: 0.9098 - val_loss: 0.2837 - val_acc: 0.9270\n",
      "Epoch 970/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3235 - acc: 0.9038 - val_loss: 0.2838 - val_acc: 0.9260\n",
      "Epoch 971/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3138 - acc: 0.9062 - val_loss: 0.2838 - val_acc: 0.9260\n",
      "Epoch 972/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3178 - acc: 0.9070 - val_loss: 0.2840 - val_acc: 0.9260\n",
      "Epoch 973/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3191 - acc: 0.9052 - val_loss: 0.2839 - val_acc: 0.9260\n",
      "Epoch 974/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3203 - acc: 0.9027 - val_loss: 0.2841 - val_acc: 0.9260\n",
      "Epoch 975/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3178 - acc: 0.9055 - val_loss: 0.2840 - val_acc: 0.9260\n",
      "Epoch 976/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3108 - acc: 0.9073 - val_loss: 0.2839 - val_acc: 0.9260\n",
      "Epoch 977/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3120 - acc: 0.9095 - val_loss: 0.2838 - val_acc: 0.9260\n",
      "Epoch 978/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3248 - acc: 0.9027 - val_loss: 0.2837 - val_acc: 0.9260\n",
      "Epoch 979/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3152 - acc: 0.9070 - val_loss: 0.2838 - val_acc: 0.9260\n",
      "Epoch 980/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3184 - acc: 0.9083 - val_loss: 0.2837 - val_acc: 0.9260\n",
      "Epoch 981/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.3244 - acc: 0.9005 - val_loss: 0.2837 - val_acc: 0.9260\n",
      "Epoch 982/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.3239 - acc: 0.9038 - val_loss: 0.2836 - val_acc: 0.9260\n",
      "Epoch 983/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3077 - acc: 0.9115 - val_loss: 0.2835 - val_acc: 0.9260\n",
      "Epoch 984/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3076 - acc: 0.9163 - val_loss: 0.2833 - val_acc: 0.9260\n",
      "Epoch 985/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3195 - acc: 0.9020 - val_loss: 0.2833 - val_acc: 0.9260\n",
      "Epoch 986/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3047 - acc: 0.9113 - val_loss: 0.2834 - val_acc: 0.9260\n",
      "Epoch 987/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3237 - acc: 0.9000 - val_loss: 0.2834 - val_acc: 0.9260\n",
      "Epoch 988/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3192 - acc: 0.9057 - val_loss: 0.2832 - val_acc: 0.9260\n",
      "Epoch 989/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3141 - acc: 0.9048 - val_loss: 0.2832 - val_acc: 0.9260\n",
      "Epoch 990/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3174 - acc: 0.9070 - val_loss: 0.2832 - val_acc: 0.9250\n",
      "Epoch 991/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3183 - acc: 0.8980 - val_loss: 0.2831 - val_acc: 0.9250\n",
      "Epoch 992/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3207 - acc: 0.9052 - val_loss: 0.2832 - val_acc: 0.9250\n",
      "Epoch 993/1000\n",
      "4000/4000 [==============================] - 0s 29us/step - loss: 0.3144 - acc: 0.9080 - val_loss: 0.2830 - val_acc: 0.9250\n",
      "Epoch 994/1000\n",
      "4000/4000 [==============================] - 0s 28us/step - loss: 0.3126 - acc: 0.9098 - val_loss: 0.2829 - val_acc: 0.9250\n",
      "Epoch 995/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3108 - acc: 0.9093 - val_loss: 0.2826 - val_acc: 0.9260\n",
      "Epoch 996/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3010 - acc: 0.9103 - val_loss: 0.2826 - val_acc: 0.9260\n",
      "Epoch 997/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3131 - acc: 0.9068 - val_loss: 0.2826 - val_acc: 0.9260\n",
      "Epoch 998/1000\n",
      "4000/4000 [==============================] - 0s 25us/step - loss: 0.3046 - acc: 0.9087 - val_loss: 0.2825 - val_acc: 0.9260\n",
      "Epoch 999/1000\n",
      "4000/4000 [==============================] - 0s 26us/step - loss: 0.3074 - acc: 0.9083 - val_loss: 0.2823 - val_acc: 0.9260\n",
      "Epoch 1000/1000\n",
      "4000/4000 [==============================] - 0s 24us/step - loss: 0.3088 - acc: 0.9065 - val_loss: 0.2822 - val_acc: 0.9260\n"
     ]
    }
   ],
   "source": [
    "model_dropout.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history_dropout = model_dropout.fit(X_train, y_train, batch_size=64, epochs=1000, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 16us/step\n",
      "Evaluation result on Test Data : Cost = 0.2821768097877502, accuracy = 92.60000000000001\n"
     ]
    }
   ],
   "source": [
    "#Model with dropout evaluation\n",
    "[test_cost, test_acc] = model_dropout.evaluate(X_test, y_test)\n",
    "print(\"Evaluation result on Test Data : Cost = {}, accuracy = {}\".format(test_cost, (test_acc*100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Behold the power of dropout!\n",
    "\n",
    "\n",
    "Let's take a look at the dropout model's cost and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Model with Dropout Accuracy Curves')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot cost of model with dropout\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history_dropout.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history_dropout.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training Cost', 'Validation Cost'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Cost',fontsize=16)\n",
    "plt.title('Model with Dropout Cost Curves',fontsize=16)\n",
    " \n",
    "#Plot accuracy of model with dropout\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history_dropout.history['acc'],'r',linewidth=3.0)\n",
    "plt.plot(history_dropout.history['val_acc'],'b',linewidth=3.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Model with Dropout Accuracy Curves',fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks much better! We're actually more accurate on the test set than we are on the training set, but I'm sure that wouldn't be the case for long if we increased the number of Epochs. At any rate, our model is more general now.\n",
    "\n",
    "Thanks for reading, dear reader!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
